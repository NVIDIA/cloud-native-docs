<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GPU Operator with Confidential Containers and Kata &mdash; NVIDIA GPU Operator 24.6.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/api-styles.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/design-tabs.js"></script>
        <script src="_static/version.js"></script>
        <script src="_static/social-media.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Install GPU Operator in Proxy Environments" href="install-gpu-operator-proxy.html" />
    <link rel="prev" title="GPU Operator with Kata Containers" href="gpu-operator-kata.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">About the Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="upgrade.html">Upgrade</a></li>
<li class="toctree-l1"><a class="reference internal" href="uninstall.html">Uninstall</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-vgpu.html">Using NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Operator configurations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-sharing.html">Time-Slicing GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-outdated-kernels.html">Outdated Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom-driver-params.html">Custom GPU Driver Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="precompiled-drivers.html">Precompiled Driver Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-configuration.html">GPU Driver CRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="cdi.html">Container Device Interface Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Sandboxed Workloads</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kubevirt.html">KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kata.html">Kata Containers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Confidential Containers and Kata</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specialized Networks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-proxy.html">HTTP Proxy</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-air-gapped.html">Air-Gapped Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-service-mesh.html">Service Mesh</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CSP configurations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="amazon-eks.html">Amazon EKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="microsoft-aks.html">Azure AKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="google-gke.html">Google GKE</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA GPU Operator</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
<li>
    <a href="https://docs.nvidia.com">NVIDIA Docs Hub</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>
    <a href="https://docs.nvidia.com/datacenter/cloud-native/">NVIDIA Cloud Native Technologies</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>GPU Operator with Confidential Containers and Kata</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="gpu-operator-with-confidential-containers-and-kata">
<h1>GPU Operator with Confidential Containers and Kata<a class="headerlink" href="#gpu-operator-with-confidential-containers-and-kata" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#about-support-for-confidential-containers" id="id2">About Support for Confidential Containers</a></p></li>
<li><p><a class="reference internal" href="#requirements" id="id3">Requirements</a></p></li>
<li><p><a class="reference internal" href="#key-software-components" id="id4">Key Software Components</a></p>
<ul>
<li><p><a class="reference internal" href="#about-nvidia-confidential-computing-manager" id="id5">About NVIDIA Confidential Computing Manager</a></p></li>
<li><p><a class="reference internal" href="#nvidia-confidential-computing-manager-configuration" id="id6">NVIDIA Confidential Computing Manager Configuration</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#limitations-and-restrictions" id="id7">Limitations and Restrictions</a></p></li>
<li><p><a class="reference internal" href="#cluster-topology-considerations" id="id8">Cluster Topology Considerations</a></p></li>
<li><p><a class="reference internal" href="#prerequisites" id="id9">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#overview-of-installation-and-configuration" id="id10">Overview of Installation and Configuration</a></p></li>
<li><p><a class="reference internal" href="#install-the-confidential-containers-operator" id="id11">Install the Confidential Containers Operator</a></p></li>
<li><p><a class="reference internal" href="#install-the-nvidia-gpu-operator" id="id12">Install the NVIDIA GPU Operator</a></p>
<ul>
<li><p><a class="reference internal" href="#procedure" id="id13">Procedure</a></p></li>
<li><p><a class="reference internal" href="#verification" id="id14">Verification</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#managing-the-confidential-computing-mode" id="id15">Managing the Confidential Computing Mode</a></p>
<ul>
<li><p><a class="reference internal" href="#setting-a-cluster-wide-default-mode" id="id16">Setting a Cluster-Wide Default Mode</a></p></li>
<li><p><a class="reference internal" href="#setting-a-node-level-mode" id="id17">Setting a Node-Level Mode</a></p></li>
<li><p><a class="reference internal" href="#verifying-a-mode-change" id="id18">Verifying a Mode Change</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#run-a-sample-workload" id="id19">Run a Sample Workload</a></p>
<ul>
<li><p><a class="reference internal" href="#troubleshooting-workloads" id="id20">Troubleshooting Workloads</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#attestation" id="id21">Attestation</a></p>
<ul>
<li><p><a class="reference internal" href="#about-attestation" id="id22">About Attestation</a></p></li>
<li><p><a class="reference internal" href="#accessing-the-vm-of-a-scheduled-confidential-container" id="id23">Accessing the VM of a Scheduled Confidential Container</a></p></li>
<li><p><a class="reference internal" href="#viewing-the-gpu-ready-state" id="id24">Viewing the GPU Ready State</a></p></li>
<li><p><a class="reference internal" href="#viewing-the-confidential-computing-mode" id="id25">Viewing the Confidential Computing Mode</a></p></li>
<li><p><a class="reference internal" href="#verifying-that-attestation-is-successful" id="id26">Verifying That Attestation Is Successful</a></p></li>
<li><p><a class="reference internal" href="#troubleshooting-attestation" id="id27">Troubleshooting Attestation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#additional-resources" id="id28">Additional Resources</a></p></li>
</ul>
</div>
<section id="about-support-for-confidential-containers">
<h2>About Support for Confidential Containers<a class="headerlink" href="#about-support-for-confidential-containers" title="Permalink to this headline"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Technology Preview features are not supported in production environments
and are not functionally complete.
Technology Preview features provide early access to upcoming product features,
enabling customers to test functionality and provide feedback during the development process.
These releases may not have any documentation, and testing is limited.</p>
</div>
<p>Confidential containers is the cloud-native approach of confidential computing.
Confidential computing extends the practice of securing data in transit and data at rest by
adding the practice of securing data in use.</p>
<p>Confidential computing is a technology that isolates sensitive data in NVIDIA GPUs and a protected CPU enclave during processing.
Confidential computing relies on hardware features such as Intel SGX, Intel TDX, and AMD SEV to provide the <em>trusted execution environment</em> (TEE).
The TEE provides embedded encryption keys and an embedded attestation mechanism to ensure that keys are only accessible by authorized application code.</p>
<p>The following high-level diagram shows some fundamental concepts for confidential containers with the NVIDIA GPU Operator:</p>
<ul class="simple">
<li><p>containerd is configured to run a Kata runtime to start virtual machines.</p></li>
<li><p>Kata starts the virtual machines using an NVIDIA optimized Linux kernel and NVIDIA provided initial RAM disk</p></li>
<li><p>Before the containers run in the virtual machine, a guest pre-start hook runs the local verifier
that is part of the NVIDIA Attestation SDK.</p></li>
</ul>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/gpu-op-confidential-containers.svg"><img alt="_images/gpu-op-confidential-containers.svg" src="_images/gpu-op-confidential-containers.svg" width="920px" /></a>
<figcaption>
<p><span class="caption-text">High-Level Logical Diagram of Software Components and Communication Paths</span><a class="headerlink" href="#id1" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline"></a></h2>
<p>Refer to the <em>Confidential Computing Deployment Guide</em> at the
<a class="reference external" href="https://docs.nvidia.com/confidential-computing">https://docs.nvidia.com/confidential-computing</a> website
for information about supported NVIDIA GPUs, such as the NVIDIA Hopper H100.</p>
<p>The following topics in the deployment guide apply to a cloud-native environment:</p>
<ul class="simple">
<li><p>Hardware selection and initial hardware configuration, such as BIOS settings.</p></li>
<li><p>Host operating system selection, initial configuration, and validation.</p></li>
</ul>
<p>The remaining configuration topics in the deployment guide do not apply to a cloud-native environment.
NVIDIA GPU Operator performs the actions that are described in these topics.</p>
</section>
<section id="key-software-components">
<h2>Key Software Components<a class="headerlink" href="#key-software-components" title="Permalink to this headline"></a></h2>
<p>NVIDIA GPU Operator brings together the following software components to
simplify managing the software required for confidential computing and deploying confidential container workloads:</p>
<dl>
<dt>Confidential Containers Operator</dt><dd><p>The Operator manages installing and deploying a runtime that can run Kata Containers with QEMU.</p>
</dd>
<dt>NVIDIA Kata Manager for Kubernetes</dt><dd><p>GPU Operator deploys NVIDIA Kata Manager for Kubernetes, <code class="docutils literal notranslate"><span class="pre">k8s-kata-manager</span></code>.
The manager performs the following functions:</p>
<ul class="simple">
<li><p>Manages the <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu-snp</span></code> runtime class.</p></li>
<li><p>Configures containerd to use the runtime class.</p></li>
<li><p>Manages the Kata artifacts such as Linux kernel images and initial RAM disks.</p></li>
</ul>
</dd>
<dt>NVIDIA Confidential Computing Manager for Kubernetes</dt><dd><p>GPU Operator deploys the manager, <code class="docutils literal notranslate"><span class="pre">k8s-cc-manager</span></code>, to set the confidential computing mode on the NVIDIA GPUs.</p>
</dd>
<dt>Node Feature Discovery (NFD)</dt><dd><p>When you install NVIDIA GPU Operator for confidential computing, you must specify the <code class="docutils literal notranslate"><span class="pre">nfd.nodefeaturerules=true</span></code> option.
This option directs the Operator to install node feature rules that detect CPU security features and the NVIDIA GPU hardware.
You can confirm the rules are installed by running <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">get</span> <span class="pre">nodefeaturerules</span> <span class="pre">nvidia-nfd-node-featurerules</span></code>.</p>
<p>On nodes that have an NVIDIA Hopper family GPU and either Intel TDX or AMD SEV-SNP, NFD adds labels to the node
such as <code class="docutils literal notranslate"><span class="pre">&quot;feature.node.kubernetes.io/cpu-security.sev.snp.enabled&quot;:</span> <span class="pre">&quot;true&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;nvidia.com/cc.capable&quot;:</span> <span class="pre">&quot;true&quot;</span></code>.
NVIDIA GPU Operator only deploys the operands for confidential containers on nodes that have the
<code class="docutils literal notranslate"><span class="pre">&quot;nvidia.com/cc.capable&quot;:</span> <span class="pre">&quot;true&quot;</span></code> label.</p>
</dd>
</dl>
<section id="about-nvidia-confidential-computing-manager">
<h3>About NVIDIA Confidential Computing Manager<a class="headerlink" href="#about-nvidia-confidential-computing-manager" title="Permalink to this headline"></a></h3>
<p>You can set the default confidential computing mode of the NVIDIA GPUs by setting the
<code class="docutils literal notranslate"><span class="pre">ccManager.defaultMode=&lt;on|off&gt;</span></code> option.
The default value is <code class="docutils literal notranslate"><span class="pre">off</span></code>.
You can set this option when you install NVIDIA GPU Operator or afterward by modifying the
<code class="docutils literal notranslate"><span class="pre">cluster-policy</span></code> instance of the <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code> object.</p>
<p>When you change the mode, the manager performs the following actions:</p>
<ul>
<li><p>Evicts the other GPU Operator operands from the node.</p>
<p>However, the manager does not drain user workloads.
You must make sure ensure that no user workloads running on the node before you change the mode.</p>
</li>
<li><p>Unbinds the GPU from the VFIO PCI device driver.</p></li>
<li><p>Changes the mode and resets the GPU.</p></li>
<li><p>Reschedules the other GPU Operator operands.</p></li>
</ul>
</section>
<section id="nvidia-confidential-computing-manager-configuration">
<h3>NVIDIA Confidential Computing Manager Configuration<a class="headerlink" href="#nvidia-confidential-computing-manager-configuration" title="Permalink to this headline"></a></h3>
<p>The following part of the cluster policy shows the fields related to the manager:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">ccManager</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">  </span><span class="nt">defaultMode</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;off&quot;</span><span class="w"></span>
<span class="w">  </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvcr.io/nvidia/cloud-native</span><span class="w"></span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">k8s-cc-manager</span><span class="w"></span>
<span class="w">  </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v0.1.0</span><span class="w"></span>
<span class="w">  </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">IfNotPresent</span><span class="w"></span>
<span class="w">  </span><span class="nt">imagePullSecrets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span><span class="w"></span>
<span class="w">  </span><span class="nt">env</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CC_CAPABLE_DEVICE_IDS</span><span class="w"></span>
<span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;0x2331,0x2322&quot;</span><span class="w"></span>
<span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>
<section id="limitations-and-restrictions">
<h2>Limitations and Restrictions<a class="headerlink" href="#limitations-and-restrictions" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>GPUs are available to containers as a single GPU in passthrough mode only.
Multi-GPU passthrough and vGPU are not supported.</p></li>
<li><p>Support is limited to initial installation and configuration only.
Upgrade and configuration of existing clusters to configure confidential computing is not supported.</p></li>
<li><p>Support for confidential computing environments is limited to the implementation described on this page.</p></li>
<li><p>NVIDIA supports the Operator and confidential computing with the containerd runtime only.</p></li>
<li><p>The Operator supports performing local attestation only.</p></li>
</ul>
</section>
<section id="cluster-topology-considerations">
<h2>Cluster Topology Considerations<a class="headerlink" href="#cluster-topology-considerations" title="Permalink to this headline"></a></h2>
<p>You can configure all the worker nodes in your cluster for confidential containers or you configure some
nodes for confidential containers and the others for traditional containers.
Consider the following example.</p>
<p>Node A is configured to run traditional containers.</p>
<p>Node B is configured to run confidential containers.</p>
<p>Node A receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Driver</span> <span class="pre">Manager</span> <span class="pre">for</span> <span class="pre">Kubernetes</span></code> – to install the data-center driver.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Container</span> <span class="pre">Toolkit</span></code> – to ensure that containers can access GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Device</span> <span class="pre">Plugin</span> <span class="pre">for</span> <span class="pre">Kubernetes</span></code> – to discover and advertise GPU resources to kubelet.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">DCGM</span> <span class="pre">and</span> <span class="pre">DCGM</span> <span class="pre">Exporter</span></code> – to monitor GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">MIG</span> <span class="pre">Manager</span> <span class="pre">for</span> <span class="pre">Kubernetes</span></code> – to manage MIG-capable GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Node</span> <span class="pre">Feature</span> <span class="pre">Discovery</span></code> – to detect CPU, kernel, and host features and label worker nodes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">GPU</span> <span class="pre">Feature</span> <span class="pre">Discovery</span></code> – to detect NVIDIA GPUs and label worker nodes.</p></li>
</ul>
<p>Node B receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Kata</span> <span class="pre">Manager</span> <span class="pre">for</span> <span class="pre">Kubernetes</span></code> – to manage the NVIDIA artifacts such as the
NVIDIA optimized Linux kernel image and initial RAM disk.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Confidential</span> <span class="pre">Computing</span> <span class="pre">Manager</span> <span class="pre">for</span> <span class="pre">Kubernetes</span></code> – to manage the confidential
computing mode of the NVIDIA GPU on the node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> – to discover and advertise the passthrough GPUs to kubelet.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">VFIO</span> <span class="pre">Manager</span></code> – to load the vfio-pci device driver and bind it to all GPUs on the node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Node</span> <span class="pre">Feature</span> <span class="pre">Discovery</span></code> – to detect CPU security features, NVIDIA GPUs, and label worker nodes.</p></li>
</ul>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h2>
<ul>
<li><p>Refer to the <em>Confidential Computing Deployment Guide</em> for the following prerequisites:</p>
<ul class="simple">
<li><p>You selected and configured your hardware and BIOS to support confidential computing.</p></li>
<li><p>You installed and configured an operating system to support confidential computing.</p></li>
<li><p>You validated that the Linux kernel is SNP-aware.</p></li>
</ul>
</li>
<li><p>Your hosts are configured to enable hardware virtualization and Access Control Services (ACS).
With some AMD CPUs and BIOSes, ACS might be grouped under Advanced Error Reporting (AER).
Enabling these features is typically performed by configuring the host BIOS.</p></li>
<li><p>Your hosts are configured to support IOMMU.</p>
<p>If the output from running <code class="docutils literal notranslate"><span class="pre">ls</span> <span class="pre">/sys/kernel/iommu_groups</span></code> includes <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code>, and so on,
then your host is configured for IOMMU.</p>
<p>If the host is not configured or you are unsure, add the <code class="docutils literal notranslate"><span class="pre">intel_iommu=on</span></code> Linux kernel command-line argument.
For most Linux distributions, you add the argument to the <code class="docutils literal notranslate"><span class="pre">/etc/default/grub</span></code> file:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet intel_iommu=on modprobe.blacklist=nouveau&quot;
...
</pre></div>
</div>
<p>On Ubuntu systems, run <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">update-grub</span></code> after making the change to configure the bootloader.
On other systems, you might need to run <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">dracut</span></code> after making the change.
Refer to the documentation for your operating system.
Reboot the host after configuring the bootloader.</p>
</li>
<li><p>You have a Kubernetes cluster and you have cluster administrator privileges.</p></li>
</ul>
</section>
<section id="overview-of-installation-and-configuration">
<h2>Overview of Installation and Configuration<a class="headerlink" href="#overview-of-installation-and-configuration" title="Permalink to this headline"></a></h2>
<p>Installing and configuring your cluster to support the NVIDIA GPU Operator with confidential containers is as follows:</p>
<ol class="arabic">
<li><p>Label the worker nodes that you want to use with confidential containers.</p>
<p>This step ensures that you can continue to run traditional container workloads with GPU or vGPU workloads on some nodes in your cluster.
Alternatively, you can set the default sandbox workload to <code class="docutils literal notranslate"><span class="pre">vm-passthrough</span></code> to run confidential containers on all worker nodes when you install the GPU Operator.</p>
</li>
<li><p>Install the Confidential Containers Operator.</p>
<p>This step installs the Operator and also the Kata Containers runtime that NVIDIA uses for confidential containers.</p>
</li>
<li><p>Install the NVIDIA GPU Operator.</p>
<p>You install the Operator and specify options to deploy the operands that are required for confidential containers.</p>
</li>
</ol>
<p>After installation, you can change the confidential computing mode and run a sample workload.</p>
</section>
<section id="install-the-confidential-containers-operator">
<h2>Install the Confidential Containers Operator<a class="headerlink" href="#install-the-confidential-containers-operator" title="Permalink to this headline"></a></h2>
<p>Perform the following steps to install and verify the Confidential Containers Operator:</p>
<ol class="arabic">
<li><p>Label the nodes to run virtual machines in containers.
Label only the nodes that you want to run with Confidential Containers.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl label node &lt;node-name&gt; nvidia.com/gpu.workload.config<span class="o">=</span>vm-passthrough
</pre></div>
</div>
</li>
<li><p>Set the Operator version in an environment variable:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">export</span> <span class="nv">VERSION</span><span class="o">=</span>v0.7.0
</pre></div>
</div>
</li>
<li><p>Install the Operator:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -k <span class="s2">&quot;github.com/confidential-containers/operator/config/release?ref=</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">namespace/confidential-containers-system created</span>
<span class="go">customresourcedefinition.apiextensions.k8s.io/ccruntimes.confidentialcontainers.org created</span>
<span class="go">serviceaccount/cc-operator-controller-manager created</span>
<span class="go">role.rbac.authorization.k8s.io/cc-operator-leader-election-role created</span>
<span class="go">clusterrole.rbac.authorization.k8s.io/cc-operator-manager-role created</span>
<span class="go">clusterrole.rbac.authorization.k8s.io/cc-operator-metrics-reader created</span>
<span class="go">clusterrole.rbac.authorization.k8s.io/cc-operator-proxy-role created</span>
<span class="go">rolebinding.rbac.authorization.k8s.io/cc-operator-leader-election-rolebinding created</span>
<span class="go">clusterrolebinding.rbac.authorization.k8s.io/cc-operator-manager-rolebinding created</span>
<span class="go">clusterrolebinding.rbac.authorization.k8s.io/cc-operator-proxy-rolebinding created</span>
<span class="go">configmap/cc-operator-manager-config created</span>
<span class="go">service/cc-operator-controller-manager-metrics-service created</span>
<span class="go">deployment.apps/cc-operator-controller-manager create</span>
</pre></div>
</div>
</li>
<li><p>Optional: View the pods and services in the <code class="docutils literal notranslate"><span class="pre">confidential-containers-system</span></code> namespace:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pod,svc -n confidential-containers-system
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                                                 READY   STATUS    RESTARTS   AGE</span>
<span class="go">pod/cc-operator-controller-manager-c98c4ff74-ksb4q   2/2     Running   0          2m59s</span>

<span class="go">NAME                                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="go">service/cc-operator-controller-manager-metrics-service   ClusterIP   10.98.221.141   &lt;none&gt;        8443/TCP   2m59s</span>
</pre></div>
</div>
</li>
<li><p>Install the sample Confidential Containers runtime by creating the manifests and then editing the node selector so
that the runtime is installed only on the labelled nodes.</p>
<ol class="arabic">
<li><p>Create a local copy of the manifests in a file that is named <code class="docutils literal notranslate"><span class="pre">ccruntime.yaml</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply --dry-run<span class="o">=</span>client -o yaml <span class="se">\</span>
    -k <span class="s2">&quot;github.com/confidential-containers/operator/config/samples/ccruntime/default?ref=</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span><span class="s2">&quot;</span> &gt; ccruntime.yaml
</pre></div>
</div>
</li>
<li><p>Edit the <code class="docutils literal notranslate"><span class="pre">ccruntime.yaml</span></code> file and set the node selector as follows:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">confidentialcontainers.org/v1beta1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CcRuntime</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">ccNodeSelector</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">nvidia.com/gpu.workload.config</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;vm-passthrough&quot;</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Apply the modified manifests:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f ccruntime.yaml
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">ccruntime.confidentialcontainers.org/ccruntime-sample created</span>
</pre></div>
</div>
</li>
</ol>
<p>Wait a few minutes for the Operator to create the base runtime classes.</p>
</li>
<li><p>Optional: View the runtime classes:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get runtimeclass
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME            HANDLER         AGE</span>
<span class="go">kata            kata            13m</span>
<span class="go">kata-clh        kata-clh        13m</span>
<span class="go">kata-clh-tdx    kata-clh-tdx    13m</span>
<span class="go">kata-qemu       kata-qemu       13m</span>
<span class="go">kata-qemu-sev   kata-qemu-sev   13m</span>
<span class="go">kata-qemu-snp   kata-qemu-snp   13m</span>
<span class="go">kata-qemu-tdx   kata-qemu-tdx   13m</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="install-the-nvidia-gpu-operator">
<h2>Install the NVIDIA GPU Operator<a class="headerlink" href="#install-the-nvidia-gpu-operator" title="Permalink to this headline"></a></h2>
<section id="procedure">
<h3>Procedure<a class="headerlink" href="#procedure" title="Permalink to this headline"></a></h3>
<p>Perform the following steps to install the Operator for use with confidential containers:</p>
<ol class="arabic">
<li><p>Add and update the NVIDIA Helm repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia <span class="se">\</span>
   <span class="o">&amp;&amp;</span> helm repo update
</pre></div>
</div>
</li>
<li><p>Specify at least the following options when you install the Operator.
If you want to run Confidential Containers by default on all worker nodes, also specify <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">sandboxWorkloads.defaultWorkload=vm-passthough</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
   -n gpu-operator --create-namespace <span class="se">\</span>
   nvidia/gpu-operator <span class="se">\</span>
   --set sandboxWorkloads.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
   --set kataManager.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
   --set ccManager.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
   --set nfd.nodefeaturerules<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME: gpu-operator</span>
<span class="go">LAST DEPLOYED: Tue Jul 25 19:19:07 2023</span>
<span class="go">NAMESPACE: gpu-operator</span>
<span class="go">STATUS: deployed</span>
<span class="go">REVISION: 1</span>
<span class="go">TEST SUITE: None</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="verification">
<h3>Verification<a class="headerlink" href="#verification" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Verify that the Kata Manager, Confidential Computing Manager, and VFIO Manager operands are running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pods -n gpu-operator
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                                                         READY   STATUS      RESTARTS   AGE</span>
<span class="go">gpu-operator-57bf5d5769-nb98z                                1/1     Running     0          6m21s</span>
<span class="go">gpu-operator-node-feature-discovery-master-b44f595bf-5sjxg   1/1     Running     0          6m21s</span>
<span class="go">gpu-operator-node-feature-discovery-worker-lwhdr             1/1     Running     0          6m21s</span>
<span class="hll"><span class="go">nvidia-cc-manager-yzbw7                                      1/1     Running     0          3m36s</span>
</span><span class="hll"><span class="go">nvidia-kata-manager-bw5mb                                    1/1     Running     0          3m36s</span>
</span><span class="go">nvidia-sandbox-device-plugin-daemonset-cr4s6                 1/1     Running     0          2m37s</span>
<span class="go">nvidia-sandbox-validator-9wjm4                               1/1     Running     0          2m37s</span>
<span class="hll"><span class="go">nvidia-vfio-manager-vg4wp                                    1/1     Running     0          3m36s</span>
</span></pre></div>
</div>
</li>
<li><p>Verify that the <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu</span></code> and <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu-snp</span></code> runtime classes are available:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get runtimeclass
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                       HANDLER                    AGE</span>
<span class="go">kata                       kata                       37m</span>
<span class="go">kata-clh                   kata-clh                   37m</span>
<span class="go">kata-clh-tdx               kata-clh-tdx               37m</span>
<span class="go">kata-qemu                  kata-qemu                  37m</span>
<span class="hll"><span class="go">kata-qemu-nvidia-gpu       kata-qemu-nvidia-gpu       96s</span>
</span><span class="hll"><span class="go">kata-qemu-nvidia-gpu-snp   kata-qemu-nvidia-gpu-snp   96s</span>
</span><span class="go">kata-qemu-sev              kata-qemu-sev              37m</span>
<span class="go">kata-qemu-snp              kata-qemu-snp              37m</span>
<span class="go">kata-qemu-tdx              kata-qemu-tdx              37m</span>
<span class="go">nvidia                     nvidia                     97s</span>
</pre></div>
</div>
</li>
<li><p>Optional: If you have host access to the worker node, you can perform the following steps:</p>
<ol class="arabic">
<li><p>Confirm that the host uses the <code class="docutils literal notranslate"><span class="pre">vfio-pci</span></code> device driver for GPUs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lspci -nnk -d 10de:
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">65:00.0 3D controller [0302]: NVIDIA Corporation xxxxxxx [xxx] [10de:xxxx] (rev xx)</span>
<span class="go">        Subsystem: NVIDIA Corporation xxxxxxx [xxx] [10de:xxxx]</span>
<span class="hll"><span class="go">        Kernel driver in use: vfio-pci</span>
</span><span class="go">        Kernel modules: nvidiafb, nouveau</span>
</pre></div>
</div>
</li>
<li><p>Confirm that NVIDIA Kata Manager installed the <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu-snp</span></code> runtime class files:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ls -1 /opt/nvidia-gpu-operator/artifacts/runtimeclasses/kata-qemu-nvidia-gpu-snp/
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">5.19.2.tar.gz</span>
<span class="go">config-5.19.2-109-nvidia-gpu-sev</span>
<span class="go">configuration-kata-qemu-nvidia-gpu-snp.toml</span>
<span class="go">dpkg.sbom.list</span>
<span class="go">kata-ubuntu-jammy-nvidia-gpu.initrd</span>
<span class="go">vmlinuz-5.19.2-109-nvidia-gpu-sev</span>
<span class="go">...</span>
</pre></div>
</div>
</li>
</ol>
</li>
</ol>
</section>
</section>
<section id="managing-the-confidential-computing-mode">
<h2>Managing the Confidential Computing Mode<a class="headerlink" href="#managing-the-confidential-computing-mode" title="Permalink to this headline"></a></h2>
<p>Three modes are supported:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">on</span></code> – Enable confidential computing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">off</span></code> – Disable confidential computing.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">devtools</span></code> – Development mode for software development and debugging.</p></li>
</ul>
<p>You can set a cluster-wide default mode and you can set the mode on individual nodes.
The mode that you set on a node has higher precedence than the cluster-wide default mode.</p>
<section id="setting-a-cluster-wide-default-mode">
<h3>Setting a Cluster-Wide Default Mode<a class="headerlink" href="#setting-a-cluster-wide-default-mode" title="Permalink to this headline"></a></h3>
<p>To set a cluster-wide mode, specify the <code class="docutils literal notranslate"><span class="pre">ccManager.defaultMode</span></code> field like the following example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl patch clusterpolicies.nvidia.com/cluster-policy <span class="se">\</span>
       -p <span class="s1">&#39;{&quot;spec&quot;: {&quot;ccManager&quot;: {&quot;defaultMode&quot;: &quot;on&quot;}}}&#39;</span>
</pre></div>
</div>
</section>
<section id="setting-a-node-level-mode">
<h3>Setting a Node-Level Mode<a class="headerlink" href="#setting-a-node-level-mode" title="Permalink to this headline"></a></h3>
<p>To set a node-level mode, apply the <code class="docutils literal notranslate"><span class="pre">nvidia.com/cc.mode=&lt;on|off|devtools&gt;</span></code> label like the following example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl label node &lt;node-name&gt; nvidia.com/cc.mode<span class="o">=</span>on --overwrite
</pre></div>
</div>
<p>The mode that you set on a node has higher precedence than the cluster-wide default mode.</p>
</section>
<section id="verifying-a-mode-change">
<h3>Verifying a Mode Change<a class="headerlink" href="#verifying-a-mode-change" title="Permalink to this headline"></a></h3>
<p>To verify that changing the mode was successful, a cluster-wide or node-level change,
view the <code class="docutils literal notranslate"><span class="pre">nvidia.com/cc.mode.state</span></code> node label:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get node &lt;node-name&gt; -o json <span class="p">|</span>  <span class="se">\</span>
    jq <span class="s1">&#39;.items[0].metadata.labels | with_entries(select(.key | startswith(&quot;nvidia.com/cc.mode.state)))&#39;</span>
</pre></div>
</div>
<p>The label is set to either <code class="docutils literal notranslate"><span class="pre">success</span></code> or <code class="docutils literal notranslate"><span class="pre">failed</span></code>.</p>
</section>
</section>
<section id="run-a-sample-workload">
<h2>Run a Sample Workload<a class="headerlink" href="#run-a-sample-workload" title="Permalink to this headline"></a></h2>
<p>A pod specification for a confidential computing requires the following:</p>
<ul class="simple">
<li><p>Specify the <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu-snp</span></code> runtime class.</p></li>
<li><p>Specify a passthrough GPU resource.</p></li>
</ul>
<ol class="arabic">
<li><p>Determine the passthrough GPU resource names:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kubectl get nodes -l nvidia.com/gpu.present -o json | \</span>
<span class="go">  jq &#39;.items[0].status.allocatable |</span>
<span class="go">    with_entries(select(.key | startswith(&quot;nvidia.com/&quot;))) |</span>
<span class="go">    with_entries(select(.value != &quot;0&quot;))&#39;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">   &quot;nvidia.com/GH100_H100_PCIE&quot;: &quot;1&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</li>
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">cuda-vectoradd-coco.yaml</span></code>, like the following example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cuda-vectoradd-coco</span><span class="w"></span>
<span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w"></span>
<span class="hll"><span class="w">    </span><span class="nt">cdi.k8s.io/gpu</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;nvidia.com/pgpu=0&quot;</span><span class="w"></span>
</span><span class="w">    </span><span class="nt">io.katacontainers.config.hypervisor.default_memory</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;16384&quot;</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">runtimeClassName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kata-qemu-nvidia-gpu-snp</span><span class="w"></span>
<span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OnFailure</span><span class="w"></span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cuda-vectoradd</span><span class="w"></span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="hll"><span class="w">      </span><span class="nt">limits</span><span class="p">:</span><span class="w"></span>
</span><span class="w">        </span><span class="s">&quot;nvidia.com/GH100_H100_PCIE&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">io.katacontainers.config.hypervisor.default_memory</span></code> annotation starts the VM with 16 GB of memory.
Modify the value to accommodate your workload.</p>
</li>
<li><p>Create the pod:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f cuda-vectoradd-coco.yaml
</pre></div>
</div>
</li>
<li><p>View the logs from pod:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl logs -n default cuda-vectoradd-coco
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">[Vector addition of 50000 elements]</span>
<span class="go">Copy input data from the host memory to the CUDA device</span>
<span class="go">CUDA kernel launch with 196 blocks of 256 threads</span>
<span class="go">Copy output data from the CUDA device to the host memory</span>
<span class="go">Test PASSED</span>
<span class="go">Done</span>
</pre></div>
</div>
</li>
<li><p>Delete the pod:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl delete -f cuda-vectoradd-coco.yaml
</pre></div>
</div>
</li>
</ol>
<p>Refer to <a class="reference internal" href="gpu-operator-kata.html#about-the-pod-annotation"><span class="std std-ref">About the Pod Annotation</span></a> for information about the pod annotation.</p>
<section id="troubleshooting-workloads">
<h3>Troubleshooting Workloads<a class="headerlink" href="#troubleshooting-workloads" title="Permalink to this headline"></a></h3>
<p>If the sample workload does not run, confirm that you labelled nodes to run virtual machines in containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get nodes -l nvidia.com/gpu.workload.config<span class="o">=</span>vm-passthrough
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME               STATUS   ROLES    AGE   VERSION</span>
<span class="go">kata-worker-1      Ready    &lt;none&gt;   10d   v1.27.3</span>
<span class="go">kata-worker-2      Ready    &lt;none&gt;   10d   v1.27.3</span>
<span class="go">kata-worker-3      Ready    &lt;none&gt;   10d   v1.27.3</span>
</pre></div>
</div>
</section>
</section>
<section id="attestation">
<h2>Attestation<a class="headerlink" href="#attestation" title="Permalink to this headline"></a></h2>
<section id="about-attestation">
<h3>About Attestation<a class="headerlink" href="#about-attestation" title="Permalink to this headline"></a></h3>
<p>With confidential computing, <em>attestation</em> is the assertion that the hardware and
software is trustworthy.</p>
<p>The Kata runtime uses the <code class="docutils literal notranslate"><span class="pre">kata-ubuntu-jammy-nvidia-gpu.initrd</span></code> initial RAM disk file
that NVIDIA Kata Manager for Kubernetes downloaded from NVIDIA Container Registry, nvcr.io.
The initial RAM disk includes an NVIDIA verifier tool that runs as a container guest pre-start hook.
When the attestation is successful, the GPU is set in the <code class="docutils literal notranslate"><span class="pre">Ready</span></code> state.
On failure, containers still start, but CUDA applications fail with a <code class="docutils literal notranslate"><span class="pre">system</span> <span class="pre">not</span> <span class="pre">initialized</span></code> error.</p>
<p>Refer to <em>NVIDIA Hopper Confidential Computing Attestation Verifier</em> at <a class="reference external" href="https://docs.nvidia.com/confidential-computing">https://docs.nvidia.com/confidential-computing</a>
for more information about attestation.</p>
</section>
<section id="accessing-the-vm-of-a-scheduled-confidential-container">
<h3>Accessing the VM of a Scheduled Confidential Container<a class="headerlink" href="#accessing-the-vm-of-a-scheduled-confidential-container" title="Permalink to this headline"></a></h3>
<p>You do not need to access the VM as a routine task.
Accessing the VM is useful for troubleshooting or performing lower-level verification about the confidential computing mode.</p>
<p>This task requires host access to the Kubernetes node that is running the container.</p>
<ol class="arabic">
<li><p>Determine the Kubernetes node and pod sandbox ID:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl describe pod &lt;pod-name&gt;
</pre></div>
</div>
</li>
<li><p>Access the Kubernetes node.
Using secure shell is typical.</p></li>
<li><p>Access the Kata runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kata-runtime <span class="nb">exec</span> &lt;pod-sandbox-ID&gt;
</pre></div>
</div>
</li>
</ol>
</section>
<section id="viewing-the-gpu-ready-state">
<h3>Viewing the GPU Ready State<a class="headerlink" href="#viewing-the-gpu-ready-state" title="Permalink to this headline"></a></h3>
<p>After you access the VM, you can run <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span> <span class="pre">conf-compute</span> <span class="pre">-grs</span></code>:</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Confidential Compute GPUs Ready state: ready</span>
</pre></div>
</div>
</section>
<section id="viewing-the-confidential-computing-mode">
<h3>Viewing the Confidential Computing Mode<a class="headerlink" href="#viewing-the-confidential-computing-mode" title="Permalink to this headline"></a></h3>
<p>After you access the VM, you can run <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span> <span class="pre">conf-compute</span> <span class="pre">-f</span></code> to view the mode:</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">CC status: ON</span>
</pre></div>
</div>
</section>
<section id="verifying-that-attestation-is-successful">
<h3>Verifying That Attestation Is Successful<a class="headerlink" href="#verifying-that-attestation-is-successful" title="Permalink to this headline"></a></h3>
<p>After you access the VM, you can run the following commands to verify that attestation is successful:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp"># </span><span class="nb">source</span> /gpu-attestation/nv-venv/bin/activate
<span class="gp"># </span>python3 /gpu-attestation/nv_attestation_sdk/tests/SmallGPUTest.py
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">[SmallGPUTest] node name : thisNode1</span>
<span class="go">[[&#39;LOCAL_GPU_CLAIMS&#39;, &lt;Devices.GPU: 2&gt;, &lt;Environment.LOCAL: 2&gt;, &#39;&#39;, &#39;&#39;, &#39;&#39;]]</span>
<span class="go">[SmallGPUTest] call attest() - expecting True</span>
<span class="go">Number of GPUs available : 1</span>
<span class="go">-----------------------------------</span>
<span class="go">Fetching GPU 0 information from GPU driver.</span>
<span class="go">VERIFYING GPU : 0</span>
<span class="go">      Driver version fetched : 535.86.05</span>
<span class="go">      VBIOS version fetched : 96.00.5e.00.01</span>
<span class="go">      Validating GPU certificate chains.</span>
<span class="go">             GPU attestation report certificate chain validation successful.</span>
<span class="go">                    The certificate chain revocation status verification successful.</span>
<span class="go">      Authenticating attestation report</span>
<span class="go">             The nonce in the SPDM GET MEASUREMENT request message is matching with the generated nonce.</span>
<span class="go">             Driver version fetched from the attestation report : 535.86.05</span>
<span class="go">             VBIOS version fetched from the attestation report : 96.00.5e.00.01</span>
<span class="go">             Attestation report signature verification successful.</span>
<span class="go">             Attestation report verification successful.</span>
<span class="go">      Authenticating the RIMs.</span>
<span class="go">             Authenticating Driver RIM</span>
<span class="go">                     Schema validation passed.</span>
<span class="go">                     driver RIM certificate chain verification successful.</span>
<span class="go">                     The certificate chain revocation status verification successful.</span>
<span class="go">                     driver RIM signature verification successful.</span>
<span class="go">                     Driver RIM verification successful</span>
<span class="go">            Authenticating VBIOS RIM.</span>
<span class="go">                     RIM Schema validation passed.</span>
<span class="go">                     vbios RIM certificate chain verification successful.</span>
<span class="go">                     The certificate chain revocation status verification successful.</span>
<span class="go">                     vbios RIM signature verification successful.</span>
<span class="go">                     VBIOS RIM verification successful</span>
<span class="go">      Comparing measurements (runtime vs golden)</span>
<span class="go">                     The runtime measurements are matching with the golden measurements.</span>
<span class="go">             GPU is in the expected state.</span>
<span class="go">      GPU 0 verified successfully.</span>
<span class="go">      attestation result: True</span>
<span class="go">      claims list:: {&#39;x-nv-gpu-availability&#39;: True, &#39;x-nv-gpu-attestation-report-available&#39;: ...</span>
<span class="go">      True</span>
<span class="go">      [SmallGPUTest] token : [[&quot;JWT&quot;, &quot;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.e...&quot;],</span>
<span class="go">         {&quot;LOCAL_GPU_CLAIMS&quot;: &quot;eyJhbGciOiJIUzI1NiIsInR5cCI...&quot;}]</span>
<span class="go">      [SmallGPUTest] call validate_token() - expecting True</span>
<span class="go">      True</span>
</pre></div>
</div>
</section>
<section id="troubleshooting-attestation">
<h3>Troubleshooting Attestation<a class="headerlink" href="#troubleshooting-attestation" title="Permalink to this headline"></a></h3>
<p>To troubleshoot attestation failures, access the VM and view the logs in the <code class="docutils literal notranslate"><span class="pre">/var/log/</span></code> directory.</p>
<p>To troubleshoot virtual machine failures, access the Kubernetes node and view logs with the <code class="docutils literal notranslate"><span class="pre">journalctl</span></code> command.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo journalctl -u containerd -f
</pre></div>
</div>
<p>The Kata agent communicates with the virtcontainers library on the host by using the VSOCK port.
The communication is recorded to the system journal on the host.
When you view the logs, refer to logs with a <code class="docutils literal notranslate"><span class="pre">kata</span></code> or <code class="docutils literal notranslate"><span class="pre">virtcontainers</span></code> prefix.</p>
</section>
</section>
<section id="additional-resources">
<h2>Additional Resources<a class="headerlink" href="#additional-resources" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>NVIDIA Confidential Computing documentation is available at <a class="reference external" href="https://docs.nvidia.com/confidential-computing">https://docs.nvidia.com/confidential-computing</a>.</p></li>
<li><p>NVIDIA Verifier Tool is part of the nvTrust project.
Refer to <a class="reference external" href="https://github.com/NVIDIA/nvtrust/tree/main/guest_tools/gpu_verifiers/local_gpu_verifier">https://github.com/NVIDIA/nvtrust/tree/main/guest_tools/gpu_verifiers/local_gpu_verifier</a>
for more information.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="gpu-operator-kata.html" class="btn btn-neutral float-left" title="GPU Operator with Kata Containers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="install-gpu-operator-proxy.html" class="btn btn-neutral float-right" title="Install GPU Operator in Proxy Environments" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
<img src="_static/NVIDIA-LogoBlack.svg" class="only-light"/>
<img src="_static/NVIDIA-LogoWhite.svg" class="only-dark"/>
<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>
<p>
  Copyright &#169; 2020-2024, NVIDIA Corporation.
</p>

    <p>
      <span class="lastupdated">Last updated on Aug 09, 2024.
      </span></p>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>