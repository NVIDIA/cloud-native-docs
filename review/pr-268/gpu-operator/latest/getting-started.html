


<!DOCTYPE html>


<html data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Installing the NVIDIA GPU Operator &#8212; NVIDIA GPU Operator</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/nvidia-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/mermaid-init.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/design-tabs.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'getting-started';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = '../versions1.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '25.10';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>
    <script src="_static/version.js"></script>
    <script src="_static/social-media.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Upgrading the NVIDIA GPU Operator" href="upgrade.html" />
    <link rel="prev" title="About the NVIDIA GPU Operator" href="overview.html" />


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  <meta name="docsearch:version" content="" />
    <meta name="docbuild:last-update" content="Oct 14, 2025"/>



  <script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js" ></script>
  


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA GPU Operator - Home"/>
    <img src="_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="NVIDIA GPU Operator - Home"/>
  
  
    <p class="title logo__title">NVIDIA GPU Operator</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA GPU Operator - Home"/>
    <img src="_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="NVIDIA GPU Operator - Home"/>
  
  
    <p class="title logo__title">NVIDIA GPU Operator</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="overview.html">About the Operator</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="upgrade.html">Upgrade</a></li>
<li class="toctree-l1"><a class="reference internal" href="uninstall.html">Uninstall</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-vgpu.html">Using NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security Considerations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Operator Configuration</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-sharing.html">Time-Slicing GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-outdated-kernels.html">Outdated Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom-driver-params.html">Custom GPU Driver Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="precompiled-drivers.html">Precompiled Driver Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-configuration.html">GPU Driver CRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="cdi.html">Container Device Interface (CDI) Support</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sandboxed Workloads</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kubevirt.html">KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kata.html">Kata Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-confidential-containers.html">Confidential Containers and Kata</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Specialized Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-proxy.html">HTTP Proxy</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-air-gapped.html">Air-Gapped Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-service-mesh.html">Service Mesh</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">CSP configurations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="amazon-eks.html">Amazon EKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="microsoft-aks.html">Azure AKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="google-gke.html">Google GKE</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">NVIDIA DRA Driver for GPUs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="dra-intro-install.html">Introduction &amp; Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="dra-gpus.html">GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="dra-cds.html">ComputeDomains</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    <li class="breadcrumb-item breadcrumb-home">
      <a href="https://docs.nvidia.com">NVIDIA Docs Hub</a>
    </li>
    <li class="breadcrumb-item">
      <a href="https://docs.nvidia.com/datacenter/cloud-native">Cloud Native Technologies</a>
    </li>
    <li class="breadcrumb-item">
      <a href="index.html">NVIDIA GPU Operator</a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">Installing the NVIDIA GPU Operator</li>
  </ul>
</nav></div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="installing-the-nvidia-gpu-operator">
<span id="operator-install-guide"></span><span id="install-gpu-operator"></span><h1>Installing the NVIDIA GPU Operator<a class="headerlink" href="#installing-the-nvidia-gpu-operator" title="Permalink to this headline">#</a></h1>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">#</a></h2>
<ol class="arabic">
<li><p>You have the <code class="docutils literal notranslate"><span class="pre">kubectl</span></code> and <code class="docutils literal notranslate"><span class="pre">helm</span></code> CLIs available on a client machine.</p>
<p>You can run the following commands to install the Helm CLI:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 <span class="se">\</span>
    <span class="o">&amp;&amp;</span> chmod <span class="m">700</span> get_helm.sh <span class="se">\</span>
    <span class="o">&amp;&amp;</span> ./get_helm.sh
</pre></div>
</div>
</li>
<li><p>All worker nodes or node groups to run GPU workloads in the Kubernetes cluster must run the same operating system version to use the NVIDIA GPU Driver container.
Alternatively, if you pre-install the NVIDIA GPU Driver on the nodes, then you can run different operating systems.</p>
<p>For worker nodes or node groups that run CPU workloads only, the nodes can run any operating system because
the GPU Operator does not perform any configuration or management of nodes for CPU-only workloads.</p>
</li>
<li><p>Nodes must be configured with a container engine such CRI-O or containerd.</p></li>
<li><p>If your cluster uses Pod Security Admission (PSA) to restrict the behavior of pods,
label the namespace for the Operator to set the enforcement policy to privileged:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl create ns gpu-operator
<span class="gp">$ </span>kubectl label --overwrite ns gpu-operator pod-security.kubernetes.io/enforce<span class="o">=</span>privileged
</pre></div>
</div>
</li>
<li><p>Node Feature Discovery (NFD) is a dependency for the Operator on each node.
By default, NFD master and worker are automatically deployed by the Operator.
If NFD is already running in the cluster, then you must disable deploying NFD when you install the Operator.</p>
<p>One way to determine if NFD is already running in the cluster is to check for a NFD label on your nodes:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get nodes -o json <span class="p">|</span> jq <span class="s1">&#39;.items[].metadata.labels | keys | any(startswith(&quot;feature.node.kubernetes.io&quot;))&#39;</span>
</pre></div>
</div>
<p>If the command output is <code class="docutils literal notranslate"><span class="pre">true</span></code>, then NFD is already running in the cluster.</p>
</li>
</ol>
</section>
<section id="procedure">
<h2>Procedure<a class="headerlink" href="#procedure" title="Permalink to this headline">#</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>For installation on Red Hat OpenShift Container Platform,
refer to <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/steps-overview.html" title="(in NVIDIA GPU Operator on Red Hat OpenShift Container Platform)"><span>Installation and Upgrade Overview on OpenShift</span></a>.</p>
</div>
<ol class="arabic">
<li><p>Add the NVIDIA Helm repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia <span class="se">\</span>
    <span class="o">&amp;&amp;</span> helm repo update
</pre></div>
</div>
</li>
<li><p>Install the GPU Operator.</p>
<ul>
<li><p>Install the Operator with the default configuration:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
    -n gpu-operator --create-namespace <span class="se">\</span>
    nvidia/gpu-operator <span class="se">\</span>
    --version<span class="o">=</span>v25.10.0
</pre></div>
</div>
</li>
<li><p>Install the Operator and specify configuration options:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
    -n gpu-operator --create-namespace <span class="se">\</span>
    nvidia/gpu-operator <span class="se">\</span>
    --version<span class="o">=</span>v25.10.0 <span class="se">\</span>
    --set &lt;option-name&gt;<span class="o">=</span>&lt;option-value&gt;
</pre></div>
</div>
<p>Refer to the <a class="reference internal" href="#gpu-operator-helm-chart-options"><span class="std std-ref">Common Chart Customization Options</span></a>
and <a class="reference internal" href="#common-deployment-scenarios"><span class="std std-ref">Common Deployment Scenarios</span></a> for more information.</p>
</li>
</ul>
</li>
</ol>
</section>
<section id="common-chart-customization-options">
<span id="id1"></span><span id="chart-customization-options"></span><span id="gpu-operator-helm-chart-options"></span><h2>Common Chart Customization Options<a class="headerlink" href="#common-chart-customization-options" title="Permalink to this headline">#</a></h2>
<p>The following options are available when using the Helm chart.
These options can be used with <code class="docutils literal notranslate"><span class="pre">--set</span></code> when installing with Helm.</p>
<p>The following table identifies the most frequently used options.
To view all the options, run <code class="docutils literal notranslate"><span class="pre">helm</span> <span class="pre">show</span> <span class="pre">values</span> <span class="pre">nvidia/gpu-operator</span></code>.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-given table">
<colgroup>
<col style="width: 20%" />
<col style="width: 50%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ccManager.enabled</span></code></p></td>
<td><p>When set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the Operator deploys NVIDIA Confidential Computing Manager for Kubernetes.
Refer to <a class="reference internal" href="gpu-operator-confidential-containers.html"><span class="doc">GPU Operator with Confidential Containers and Kata</span></a> for more information.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cdi.enabled</span></code></p></td>
<td><p>When set to <code class="docutils literal notranslate"><span class="pre">true</span></code> (default), the Operator installs two additional runtime classes,
nvidia-cdi and nvidia-legacy, and enables the use of the Container Device Interface (CDI)
for making GPUs accessible to containers.
Using CDI aligns the Operator with the recent efforts to standardize how complex devices like GPUs
are exposed to containerized environments.</p>
<p>Pods can specify <code class="docutils literal notranslate"><span class="pre">spec.runtimeClassName</span></code> as <code class="docutils literal notranslate"><span class="pre">nvidia-cdi</span></code> to use the functionality or
specify <code class="docutils literal notranslate"><span class="pre">nvidia-legacy</span></code> to prevent using CDI to perform device injection.</p>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">cdi.default</span></code>  Deprecated.</p></td>
<td><p>This field is deprecated as of v25.10.0 and will be ignored.
The <code class="docutils literal notranslate"><span class="pre">cdi.enabled</span></code> field is set to <code class="docutils literal notranslate"><span class="pre">true</span></code> by default in versions 25.10.0 and later.
When set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the container runtime uses CDI to perform device injection by default.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">daemonsets.annotations</span></code></p></td>
<td><p>Map of custom annotations to add to all GPU Operator managed pods.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">{}</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">daemonsets.labels</span></code></p></td>
<td><p>Map of custom labels to add to all GPU Operator managed pods.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">{}</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">dcgmExporter.enabled</span></code></p></td>
<td><p>By default, the Operator gathers GPU telemetry in Kubernetes via <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-telemetry/latest/dcgm-exporter.html">DCGM Exporter</a>.
Set this value to <code class="docutils literal notranslate"><span class="pre">false</span></code> to disable it.
Available values are <code class="docutils literal notranslate"><span class="pre">true</span></code> (default) or <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">dcgmExporter.service.internalTrafficPolicy</span></code></p></td>
<td><p>Specifies the <a class="reference external" href="https://kubernetes.io/docs/concepts/services-networking/service/#traffic-policies">internalTrafficPolicy</a> for the DCGM Exporter service.
Available values are <code class="docutils literal notranslate"><span class="pre">Cluster</span></code> (default) or <code class="docutils literal notranslate"><span class="pre">Local</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Cluster</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">devicePlugin.config</span></code></p></td>
<td><p>Specifies the configuration for the NVIDIA Device Plugin as a config map.</p>
<p>In most cases, this field is configured after installing the Operator, such as
to configure <a class="reference internal" href="gpu-sharing.html"><span class="doc">Time-Slicing GPUs in Kubernetes</span></a>.</p>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">{}</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">driver.enabled</span></code></p></td>
<td><p>By default, the Operator deploys NVIDIA drivers as a container on the system.
Set this value to <code class="docutils literal notranslate"><span class="pre">false</span></code> when using the Operator on systems with pre-installed drivers.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">driver.kernelModuleType</span></code></p></td>
<td><p>Specifies the type of the NVIDIA GPU Kernel modules to use.
Valid values are <code class="docutils literal notranslate"><span class="pre">auto</span></code> (default), <code class="docutils literal notranslate"><span class="pre">proprietary</span></code>, and <code class="docutils literal notranslate"><span class="pre">open</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">Auto</span></code> means that the recommended kernel module type (open or proprietary) is chosen based on the GPU devices on the host and the driver branch used.
Note, <code class="docutils literal notranslate"><span class="pre">auto</span></code> is only supported with the 570.86.15 and 570.124.06 or later driver containers.
550 and 535 branch drivers do not yet support this mode.
<code class="docutils literal notranslate"><span class="pre">Open</span></code> means the open kernel module is used.
<code class="docutils literal notranslate"><span class="pre">Proprietary</span></code> means the proprietary module is used.</p>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">auto</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">driver.repository</span></code></p></td>
<td><p>The images are downloaded from NGC. Specify another image repository when using
custom driver images.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nvcr.io/nvidia</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">driver.rdma.enabled</span></code></p></td>
<td><p>Controls whether the driver daemon set builds and loads the legacy <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module.</p>
<p>You might be able to use GPUDirect RDMA without enabling this option.
Refer to <a class="reference internal" href="gpu-operator-rdma.html"><span class="doc">GPUDirect RDMA and GPUDirect Storage</span></a> for information about whether you can use DMA-BUF or
you need to use legacy <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code>.</p>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">driver.rdma.useHostMofed</span></code></p></td>
<td><p>Indicate if MLNX_OFED (MOFED) drivers are pre-installed on the host.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">driver.startupProbe</span></code></p></td>
<td><p>By default, the driver container has an initial delay of <code class="docutils literal notranslate"><span class="pre">60s</span></code> before starting liveness probes.
The probe runs the <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> command with a timeout duration of <code class="docutils literal notranslate"><span class="pre">60s</span></code>.
You can increase the <code class="docutils literal notranslate"><span class="pre">timeoutSeconds</span></code> duration if the <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> command
runs slowly in your cluster.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">60s</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">driver.useOpenKernelModules</span></code> Deprecated.</p></td>
<td><p>This field is deprecated as of v25.3.0 and will be ignored. Use <code class="docutils literal notranslate"><span class="pre">kernelModuleType</span></code> instead.
When set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the driver containers install the NVIDIA Open GPU Kernel module driver.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">driver.usePrecompiled</span></code></p></td>
<td><p>When set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the Operator attempts to deploy driver containers that have
precompiled kernel drivers.
Refer to the <a class="reference internal" href="precompiled-drivers.html"><span class="doc">precompiled driver containers</span></a> page for the supported operating systems.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">driver.version</span></code></p></td>
<td><p>Version of the NVIDIA datacenter driver supported by the Operator.</p>
<p>If you set <code class="docutils literal notranslate"><span class="pre">driver.usePrecompiled</span></code> to <code class="docutils literal notranslate"><span class="pre">true</span></code>, then set this field to
a driver branch, such as <code class="docutils literal notranslate"><span class="pre">525</span></code>.</p>
</td>
<td><p>Depends on the version of the Operator. See the Component Matrix
for more information on supported drivers.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">gdrcopy.enabled</span></code></p></td>
<td><p>Enables support for GDRCopy.
When set to <code class="docutils literal notranslate"><span class="pre">true</span></code>, the GDRCopy Driver runs as a sidecar container in the GPU driver pod.
For information about GDRCopy, refer to the <a class="reference external" href="https://developer.nvidia.com/gdrcopy">gdrcopy</a> page.</p>
<p>You can enable GDRCopy if you use the <a class="reference internal" href="gpu-driver-configuration.html"><span class="doc">NVIDIA GPU Driver Custom Resource Definition</span></a>.</p>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">kataManager.enabled</span></code></p></td>
<td><p>The GPU Operator deploys NVIDIA Kata Manager when this field is <code class="docutils literal notranslate"><span class="pre">true</span></code>.
Refer to <a class="reference internal" href="gpu-operator-kata.html"><span class="doc">GPU Operator with Kata Containers</span></a> for more information.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">mig.strategy</span></code></p></td>
<td><p>Controls the strategy to be used with MIG on supported NVIDIA GPUs. Options
are either <code class="docutils literal notranslate"><span class="pre">mixed</span></code> or <code class="docutils literal notranslate"><span class="pre">single</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">single</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">migManager.enabled</span></code></p></td>
<td><p>The MIG manager watches for changes to the MIG geometry and applies reconfiguration as needed. By
default, the MIG manager only runs on nodes with GPUs that support MIG (for e.g. A100).</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">nfd.enabled</span></code></p></td>
<td><p>Deploys Node Feature Discovery plugin as a daemonset.
Set this variable to <code class="docutils literal notranslate"><span class="pre">false</span></code> if NFD is already running in the cluster.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">nfd.nodefeaturerules</span></code></p></td>
<td><p>Installs node feature rules that are related to confidential computing.
NFD uses the rules to detect security features in CPUs and NVIDIA GPUs.
Set this variable to <code class="docutils literal notranslate"><span class="pre">true</span></code> when you configure the Operator for Confidential Containers.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">operator.labels</span></code></p></td>
<td><p>Map of custom labels that will be added to all GPU Operator managed pods.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">{}</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code></p></td>
<td><p>The GPU Operator deploys <code class="docutils literal notranslate"><span class="pre">PodSecurityPolicies</span></code> if enabled.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">false</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.defaultWorkload</span></code></p></td>
<td><p>Specifies the default type of workload for the cluster, one of <code class="docutils literal notranslate"><span class="pre">container</span></code>, <code class="docutils literal notranslate"><span class="pre">vm-passthrough</span></code>, or <code class="docutils literal notranslate"><span class="pre">vm-vgpu</span></code>.</p>
<p>Setting <code class="docutils literal notranslate"><span class="pre">vm-passthrough</span></code> or <code class="docutils literal notranslate"><span class="pre">vm-vgpu</span></code> can be helpful if you plan to run all or mostly virtual machines in your cluster.
Refer to <a class="reference internal" href="gpu-operator-kubevirt.html"><span class="doc">KubeVirt</span></a>, <a class="reference internal" href="gpu-operator-kata.html"><span class="doc">Kata Containers</span></a>, or <a class="reference internal" href="gpu-operator-confidential-containers.html"><span class="doc">Confidential Containers</span></a>.</p>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">container</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">toolkit.enabled</span></code></p></td>
<td><p>By default, the Operator deploys the NVIDIA Container Toolkit (<code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code> stack)
as a container on the system. Set this value to <code class="docutils literal notranslate"><span class="pre">false</span></code> when using the Operator on systems
with pre-installed NVIDIA runtimes.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="common-deployment-scenarios">
<h2>Common Deployment Scenarios<a class="headerlink" href="#common-deployment-scenarios" title="Permalink to this headline">#</a></h2>
<p>The following common deployment scenarios and sample commands apply best to
bare metal hosts or virtual machines with GPU passthrough.</p>
<section id="specifying-the-operator-namespace">
<h3>Specifying the Operator Namespace<a class="headerlink" href="#specifying-the-operator-namespace" title="Permalink to this headline">#</a></h3>
<p>Both the Operator and operands are installed in the same namespace.
The namespace is configurable and is specified during installation.
For example, to install the GPU Operator in the <code class="docutils literal notranslate"><span class="pre">nvidia-gpu-operator</span></code> namespace:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
     -n nvidia-gpu-operator --create-namespace <span class="se">\</span>
     nvidia/gpu-operator <span class="se">\</span>
     --version<span class="o">=</span>v25.10.0 <span class="se">\</span>
</pre></div>
</div>
<p>If you do not specify a namespace during installation, all GPU Operator components are installed in the <code class="docutils literal notranslate"><span class="pre">default</span></code> namespace.</p>
</section>
<section id="preventing-installation-of-operands-on-some-nodes">
<h3>Preventing Installation of Operands on Some Nodes<a class="headerlink" href="#preventing-installation-of-operands-on-some-nodes" title="Permalink to this headline">#</a></h3>
<p>By default, the GPU Operator operands are deployed on all GPU worker nodes in the cluster.
GPU worker nodes are identified by the presence of the label <code class="docutils literal notranslate"><span class="pre">feature.node.kubernetes.io/pci-10de.present=true</span></code>.
The value <code class="docutils literal notranslate"><span class="pre">0x10de</span></code> is the PCI vendor ID that is assigned to NVIDIA.</p>
<p>To disable operands from getting deployed on a GPU worker node, label the node with <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.deploy.operands=false</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl label nodes <span class="nv">$NODE</span> nvidia.com/gpu.deploy.operands<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</section>
<section id="preventing-installation-of-nvidia-gpu-driver-on-some-nodes">
<h3>Preventing Installation of NVIDIA GPU Driver on Some Nodes<a class="headerlink" href="#preventing-installation-of-nvidia-gpu-driver-on-some-nodes" title="Permalink to this headline">#</a></h3>
<p>By default, the GPU Operator deploys the driver on all GPU worker nodes in the cluster.
To prevent installing the driver on a GPU worker node, label the node like the following sample command.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl label nodes <span class="nv">$NODE</span> nvidia.com/gpu.deploy.driver<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</section>
<section id="installation-on-red-hat-enterprise-linux">
<h3>Installation on Red Hat Enterprise Linux<a class="headerlink" href="#installation-on-red-hat-enterprise-linux" title="Permalink to this headline">#</a></h3>
<p>In this scenario, use the NVIDIA Container Toolkit image that is built on UBI 8:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
     -n gpu-operator --create-namespace <span class="se">\</span>
     nvidia/gpu-operator <span class="se">\</span>
     --version<span class="o">=</span>v25.10.0 <span class="se">\</span>
     --set toolkit.version<span class="o">=</span>v1.16.1-ubi8
</pre></div>
</div>
<p>Replace the <code class="docutils literal notranslate"><span class="pre">v1.16.1</span></code> value in the preceding command with the version that is supported
with the NVIDIA GPU Operator.
Refer to the <a class="reference internal" href="platform-support.html#gpu-operator-component-matrix"><span class="std std-ref">GPU Operator Component Matrix</span></a> on the platform support page.</p>
<p>When using RHEL8 with Kubernetes, SELinux must be enabled either in permissive or enforcing mode for use with the GPU Operator.
Additionally, when using RHEL8 with containerd as the runtime and SELinux is enabled (either in permissive or enforcing mode) at the host level, containerd must also be configured for SELinux, by setting the <code class="docutils literal notranslate"><span class="pre">enable_selinux=true</span></code> configuration option.
Note, network restricted environments are not supported.</p>
</section>
<section id="pre-installed-nvidia-gpu-drivers">
<h3>Pre-Installed NVIDIA GPU Drivers<a class="headerlink" href="#pre-installed-nvidia-gpu-drivers" title="Permalink to this headline">#</a></h3>
<p>In this scenario, the NVIDIA GPU driver is already installed on the worker nodes that have GPUs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
     -n gpu-operator --create-namespace <span class="se">\</span>
     nvidia/gpu-operator <span class="se">\</span>
     --version<span class="o">=</span>v25.10.0 <span class="se">\</span>
     --set driver.enabled<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
<p>The preceding command prevents the Operator from installing the GPU driver on any nodes in the cluster.</p>
<p>If you do not specify the <code class="docutils literal notranslate"><span class="pre">driver.enabled=false</span></code> argument and nodes in the cluster have a pre-installed GPU driver, the init container in the driver pod detects that the driver is preinstalled and labels the node so that the driver pod is terminated and does not get re-scheduled on to the node.
The Operator proceeds to start other pods, such as the container toolkit pod.</p>
</section>
<section id="pre-installed-nvidia-gpu-drivers-and-nvidia-container-toolkit">
<span id="preinstalled-drivers-and-toolkit"></span><h3>Pre-Installed NVIDIA GPU Drivers and NVIDIA Container Toolkit<a class="headerlink" href="#pre-installed-nvidia-gpu-drivers-and-nvidia-container-toolkit" title="Permalink to this headline">#</a></h3>
<p>In this scenario, the NVIDIA GPU driver and the NVIDIA Container Toolkit are already installed on
the worker nodes that have GPUs.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>This scenario applies to NVIDIA DGX Systems that run NVIDIA Base OS.</p>
</div>
<p>Before installing the Operator, ensure that the default runtime is set to <code class="docutils literal notranslate"><span class="pre">nvidia</span></code>.
Refer to <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration" title="(in NVIDIA Container Toolkit)"><span>Configuration</span></a> in the NVIDIA Container Toolkit documentation for more information.</p>
<p>Install the Operator with the following options:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
     -n gpu-operator --create-namespace <span class="se">\</span>
     nvidia/gpu-operator <span class="se">\</span>
     --version<span class="o">=</span>v25.10.0 <span class="se">\</span>
     --set driver.enabled<span class="o">=</span><span class="nb">false</span> <span class="se">\</span>
     --set toolkit.enabled<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</section>
<section id="pre-installed-nvidia-container-toolkit-but-no-drivers">
<h3>Pre-Installed NVIDIA Container Toolkit (but no drivers)<a class="headerlink" href="#pre-installed-nvidia-container-toolkit-but-no-drivers" title="Permalink to this headline">#</a></h3>
<p>In this scenario, the NVIDIA Container Toolkit is already installed on the worker nodes that have GPUs.</p>
<ol class="arabic">
<li><p>Configure toolkit to use the <code class="docutils literal notranslate"><span class="pre">root</span></code> directory of the driver installation as <code class="docutils literal notranslate"><span class="pre">/run/nvidia/driver</span></code>, because this is the path mounted by driver container.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo sed -i <span class="s1">&#39;s/^#root/root/&#39;</span> /etc/nvidia-container-runtime/config.toml
</pre></div>
</div>
</li>
</ol>
<ol class="arabic">
<li><p>Install the Operator with the following options (which will provision a driver):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
    -n gpu-operator --create-namespace <span class="se">\</span>
    nvidia/gpu-operator <span class="se">\</span>
    --version<span class="o">=</span>v25.10.0 <span class="se">\</span>
    --set toolkit.enabled<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="running-a-custom-driver-image">
<h3>Running a Custom Driver Image<a class="headerlink" href="#running-a-custom-driver-image" title="Permalink to this headline">#</a></h3>
<p>If you want to use custom driver container images, such as version 465.27, then
you can build a custom driver container image. Follow these steps:</p>
<ul>
<li><p>Rebuild the driver container by specifying the <code class="docutils literal notranslate"><span class="pre">$DRIVER_VERSION</span></code> argument when building the Docker image. For
reference, the driver container Dockerfiles are available on the Git repository at <a class="gitlab reference external" href="https://gitlab.com/nvidia/container-images/driver">nvidia/container-images/driver</a>.</p></li>
<li><p>Build the container using the appropriate Dockerfile. For example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker build --pull -t <span class="se">\</span>
    --build-arg <span class="nv">DRIVER_VERSION</span><span class="o">=</span><span class="m">455</span>.28 <span class="se">\</span>
    nvidia/driver:455.28-ubuntu20.04 <span class="se">\</span>
    --file Dockerfile .
</pre></div>
</div>
<p>Ensure that the driver container is tagged as shown in the example by using the <code class="docutils literal notranslate"><span class="pre">driver:&lt;version&gt;-&lt;os&gt;</span></code> schema.</p>
</li>
<li><p>Specify the new driver image and repository by overriding the defaults in
the Helm install command. For example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
     -n gpu-operator --create-namespace <span class="se">\</span>
     nvidia/gpu-operator <span class="se">\</span>
     --version<span class="o">=</span>v25.10.0 <span class="se">\</span>
     --set driver.repository<span class="o">=</span>docker.io/nvidia <span class="se">\</span>
     --set driver.version<span class="o">=</span><span class="s2">&quot;465.27&quot;</span>
</pre></div>
</div>
</li>
</ul>
<p>These instructions are provided for reference and evaluation purposes.
Not using the standard releases of the GPU Operator from NVIDIA would mean limited
support for such custom configurations.</p>
</section>
</section>
<section id="specifying-configuration-options-for-containerd">
<span id="custom-runtime-options"></span><h2>Specifying Configuration Options for containerd<a class="headerlink" href="#specifying-configuration-options-for-containerd" title="Permalink to this headline">#</a></h2>
<p>When you use containerd as the container runtime, the following configuration
options are used with the container-toolkit deployed with GPU Operator:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">toolkit</span><span class="p">:</span><span class="w"></span>
<span class="w">   </span><span class="nt">env</span><span class="p">:</span><span class="w"></span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_CONFIG</span><span class="w"></span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/var/snap/microk8s/current/args/containerd-template.toml</span><span class="w"></span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_SOCKET</span><span class="w"></span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/var/snap/microk8s/common/run/containerd.sock</span><span class="w"></span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">RUNTIME_CONFIG_SOURCE</span><span class="w"></span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">file=/var/snap/microk8s/current/args/containerd.toml</span><span class="w"></span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NVIDIA_CONTAINER_RUNTIME_MODES_CDI_ANNOTATION_PREFIXES</span><span class="w"></span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;cdi.k8s.io/&quot;</span><span class="w"></span>
</pre></div>
</div>
<p>If you need to specify custom values, refer to the following sample command for the syntax:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">helm install gpu-operator -n gpu-operator --create-namespace \</span>
<span class="go">  nvidia/gpu-operator $HELM_OPTIONS \</span>
<span class="go">    --version=v25.10.0 \</span>
<span class="go">    --set toolkit.env[0].name=CONTAINERD_CONFIG \</span>
<span class="go">    --set toolkit.env[0].value=/var/snap/microk8s/current/args/containerd-template.toml \</span>
<span class="go">    --set toolkit.env[1].name=CONTAINERD_SOCKET \</span>
<span class="go">    --set toolkit.env[1].value=/var/snap/microk8s/common/run/containerd.sock \</span>
<span class="go">    --set toolkit.env[2].name=RUNTIME_CONFIG_SOURCE \</span>
<span class="go">    --set toolkit.env[2].value=file=/var/snap/microk8s/current/args/containerd.toml \</span>
<span class="go">    --set toolkit.env[2].name=NVIDIA_CONTAINER_RUNTIME_MODES_CDI_ANNOTATION_PREFIXES \</span>
<span class="go">    --set toolkit.env[2].value=&quot;cdi.k8s.io/&quot;</span>
</pre></div>
</div>
<p>These options are defined as follows:</p>
<dl class="simple">
<dt>CONTAINERD_CONFIG</dt><dd><p>The path on the host to the <code class="docutils literal notranslate"><span class="pre">containerd</span></code> config
you would like to have updated with support for the <code class="docutils literal notranslate"><span class="pre">nvidia-container-runtime</span></code>.
By default this will point to <code class="docutils literal notranslate"><span class="pre">/var/snap/microk8s/current/args/containerd-template.toml</span></code>
(the default location for <code class="docutils literal notranslate"><span class="pre">containerd</span></code>). It should be customized if your <code class="docutils literal notranslate"><span class="pre">containerd</span></code>
installation is not in the default location.</p>
</dd>
<dt>CONTAINERD_SOCKET</dt><dd><p>The path on the host to the socket file used to
communicate with <code class="docutils literal notranslate"><span class="pre">containerd</span></code>. The operator will use this to send a
<code class="docutils literal notranslate"><span class="pre">SIGHUP</span></code> signal to the <code class="docutils literal notranslate"><span class="pre">containerd</span></code> daemon to reload its config. By
default this will point to <code class="docutils literal notranslate"><span class="pre">/var/snap/microk8s/common/run/containerd.sock</span></code>
(the default location for <code class="docutils literal notranslate"><span class="pre">containerd</span></code>). It should be customized if
your <code class="docutils literal notranslate"><span class="pre">containerd</span></code> installation is not in the default location.</p>
</dd>
<dt>RUNTIME_CONFIG_SOURCE</dt><dd><p>The path on the host to the runtime config source file used to
load the nvidia-container-runtime into the containerd runtime.
By default this will point to <code class="docutils literal notranslate"><span class="pre">/var/snap/microk8s/current/args/containerd.toml</span></code>
(the default location for <code class="docutils literal notranslate"><span class="pre">containerd</span></code>). It should be customized if
your <code class="docutils literal notranslate"><span class="pre">containerd</span></code> installation is not in the default location.</p>
</dd>
<dt>NVIDIA_CONTAINER_RUNTIME_MODES_CDI_ANNOTATION_PREFIXES</dt><dd><p>The annotation prefix for the CDI modes.
By default this will point to <code class="docutils literal notranslate"><span class="pre">cdi.k8s.io/</span></code>
(the default prefix for CDI modes). It should be customized if
your CDI installation is not in the default location.</p>
</dd>
</dl>
<section id="rancher-kubernetes-engine-2">
<h3>Rancher Kubernetes Engine 2<a class="headerlink" href="#rancher-kubernetes-engine-2" title="Permalink to this headline">#</a></h3>
<p>For Rancher Kubernetes Engine 2 (RKE2), refer to
<a class="reference external" href="https://docs.rke2.io/advanced#deploy-nvidia-operator">Deploy NVIDIA Operator</a>
in the RKE2 documentation.</p>
<p>Refer to the <a class="reference internal" href="release-notes.html#v24-9-0-known-limitations"><span class="std std-ref">Known Limitations</span></a>.</p>
</section>
<section id="microk8s">
<h3>MicroK8s<a class="headerlink" href="#microk8s" title="Permalink to this headline">#</a></h3>
<p>For MicroK8s, set the following in the <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">toolkit</span><span class="p">:</span><span class="w"></span>
<span class="w">   </span><span class="nt">env</span><span class="p">:</span><span class="w"></span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_CONFIG</span><span class="w"></span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/var/snap/microk8s/current/args/containerd.toml</span><span class="w"></span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_SOCKET</span><span class="w"></span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/var/snap/microk8s/common/run/containerd.sock</span><span class="w"></span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_RUNTIME_CLASS</span><span class="w"></span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia</span><span class="w"></span>
<span class="w">   </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CONTAINERD_SET_AS_DEFAULT</span><span class="w"></span>
<span class="w">     </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="w"></span>
</pre></div>
</div>
<p>These options can be passed to GPU Operator during install time as below.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">helm install gpu-operator -n gpu-operator --create-namespace \</span>
<span class="go">  nvidia/gpu-operator $HELM_OPTIONS \</span>
<span class="go">    --version=v25.10.0 \</span>
<span class="go">    --set toolkit.env[0].name=CONTAINERD_CONFIG \</span>
<span class="go">    --set toolkit.env[0].value=/var/snap/microk8s/current/args/containerd.toml \</span>
<span class="go">    --set toolkit.env[1].name=CONTAINERD_SOCKET \</span>
<span class="go">    --set toolkit.env[1].value=/var/snap/microk8s/common/run/containerd.sock \</span>
<span class="go">    --set toolkit.env[2].name=CONTAINERD_RUNTIME_CLASS \</span>
<span class="go">    --set toolkit.env[2].value=nvidia \</span>
<span class="go">    --set toolkit.env[3].name=CONTAINERD_SET_AS_DEFAULT \</span>
<span class="go">    --set-string toolkit.env[3].value=true</span>
</pre></div>
</div>
</section>
</section>
<section id="verification-running-sample-gpu-applications">
<span id="verify-gpu-operator-install"></span><span id="running-sample-gpu-applications"></span><h2>Verification: Running Sample GPU Applications<a class="headerlink" href="#verification-running-sample-gpu-applications" title="Permalink to this headline">#</a></h2>
<section id="cuda-vectoradd">
<h3>CUDA VectorAdd<a class="headerlink" href="#cuda-vectoradd" title="Permalink to this headline">#</a></h3>
<p>In the first example, lets run a simple CUDA sample, which adds two vectors together:</p>
<ol class="arabic">
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">cuda-vectoradd.yaml</span></code>, with contents like the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cuda-vectoradd</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OnFailure</span><span class="w"></span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cuda-vectoradd</span><span class="w"></span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">limits</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Run the pod:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f cuda-vectoradd.yaml
</pre></div>
</div>
<p>The pod starts, runs the <code class="docutils literal notranslate"><span class="pre">vectorAdd</span></code> command, and then exits.</p>
</li>
<li><p>View the logs from the container:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl logs pod/cuda-vectoradd
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">[Vector addition of 50000 elements]</span>
<span class="go">Copy input data from the host memory to the CUDA device</span>
<span class="go">CUDA kernel launch with 196 blocks of 256 threads</span>
<span class="go">Copy output data from the CUDA device to the host memory</span>
<span class="go">Test PASSED</span>
<span class="go">Done</span>
</pre></div>
</div>
</li>
<li><p>Removed the stopped pod:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl delete -f cuda-vectoradd.yaml
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">pod &quot;cuda-vectoradd&quot; deleted</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="jupyter-notebook">
<h3>Jupyter Notebook<a class="headerlink" href="#jupyter-notebook" title="Permalink to this headline">#</a></h3>
<p>You can perform the following steps to deploy Jupyter Notebook in your cluster:</p>
<ol class="arabic">
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">tf-notebook.yaml</span></code>, with contents like the following example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span><span class="w"></span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Service</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tf-notebook</span><span class="w"></span>
<span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tf-notebook</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NodePort</span><span class="w"></span>
<span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">80</span><span class="w"></span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">http</span><span class="w"></span>
<span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8888</span><span class="w"></span>
<span class="w">    </span><span class="nt">nodePort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30001</span><span class="w"></span>
<span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tf-notebook</span><span class="w"></span>
<span class="nn">---</span><span class="w"></span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tf-notebook</span><span class="w"></span>
<span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tf-notebook</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">securityContext</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">fsGroup</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0</span><span class="w"></span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tf-notebook</span><span class="w"></span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">tensorflow/tensorflow:latest-gpu-jupyter</span><span class="w"></span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">limits</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">    </span><span class="nt">ports</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8888</span><span class="w"></span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">notebook</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Apply the manifest to deploy the pod and start the service:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f tf-notebook.yaml
</pre></div>
</div>
</li>
<li><p>Check the pod status:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pod tf-notebook
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAMESPACE   NAME          READY   STATUS      RESTARTS   AGE</span>
<span class="go">default     tf-notebook   1/1     Running     0          3m45s</span>
</pre></div>
</div>
</li>
<li><p>Because the manifest includes a service, get the external port for the notebook:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get svc tf-notebook
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)       AGE</span>
<span class="go">tf-notebook   NodePort    10.106.229.20   &lt;none&gt;        80:30001/TCP  4m41s</span>
</pre></div>
</div>
</li>
<li><p>Get the token for the Jupyter notebook:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl logs tf-notebook
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">[I 21:50:23.188 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret</span>
<span class="go">[I 21:50:23.390 NotebookApp] Serving notebooks from local directory: /tf</span>
<span class="go">[I 21:50:23.391 NotebookApp] The Jupyter Notebook is running at:</span>
<span class="go">[I 21:50:23.391 NotebookApp] http://tf-notebook:8888/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9</span>
<span class="go">[I 21:50:23.391 NotebookApp]  or http://127.0.0.1:8888/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9</span>
<span class="go">[I 21:50:23.391 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span>
<span class="go">[C 21:50:23.394 NotebookApp]</span>

<span class="go">To access the notebook, open this file in a browser:</span>
<span class="go">   file:///root/.local/share/jupyter/runtime/nbserver-1-open.html</span>
<span class="go">Or copy and paste one of these URLs:</span>
<span class="go">   http://tf-notebook:8888/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9</span>
<span class="go">or http://127.0.0.1:8888/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9</span>
</pre></div>
</div>
</li>
</ol>
<p>The notebook should now be accessible from your browser at this URL:
<a class="reference external" href="http://your-machine-ip:30001/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9">http://your-machine-ip:30001/?token=3660c9ee9b225458faaf853200bc512ff2206f635ab2b1d9</a>.</p>
</section>
</section>
<section id="installation-on-commercially-supported-kubernetes-platforms">
<h2>Installation on Commercially Supported Kubernetes Platforms<a class="headerlink" href="#installation-on-commercially-supported-kubernetes-platforms" title="Permalink to this headline">#</a></h2>
<div class="pst-scrollable-table-container"><table class="colwidths-given table">
<colgroup>
<col style="width: 30%" />
<col style="width: 70%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Product</p></th>
<th class="head"><p>Documentation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><div class="line-block">
<div class="line">Red Hat OpenShift 4</div>
<div class="line">using RHCOS worker nodes</div>
</div>
</td>
<td><p><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html" title="(in NVIDIA GPU Operator on Red Hat OpenShift Container Platform)"><span>NVIDIA GPU Operator on Red Hat OpenShift Container Platform</span></a></p></td>
</tr>
<tr class="row-odd"><td><div class="line-block">
<div class="line">VMware vSphere with Tanzu</div>
<div class="line">and NVIDIA AI Enterprise</div>
</div>
</td>
<td><p><a class="reference external" href="https://docs.nvidia.com/ai-enterprise/deployment-guide-vmware/0.1.0/index.html"><em>NVIDIA AI Enterprise VMware vSphere Deployment Guide</em></a></p></td>
</tr>
<tr class="row-even"><td><p>Google Cloud Anthos</p></td>
<td><p><a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/edge/latest/anthos-guide.html" title="(in NVIDIA Cloud Native Reference Architectures)"><span>NVIDIA GPUs with Google Anthos</span></a></p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="overview.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">About the NVIDIA GPU Operator</p>
      </div>
    </a>
    <a class="right-next"
       href="upgrade.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Upgrading the NVIDIA GPU Operator</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#procedure">Procedure</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-chart-customization-options">Common Chart Customization Options</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-deployment-scenarios">Common Deployment Scenarios</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specifying-the-operator-namespace">Specifying the Operator Namespace</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preventing-installation-of-operands-on-some-nodes">Preventing Installation of Operands on Some Nodes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#preventing-installation-of-nvidia-gpu-driver-on-some-nodes">Preventing Installation of NVIDIA GPU Driver on Some Nodes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#installation-on-red-hat-enterprise-linux">Installation on Red Hat Enterprise Linux</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-installed-nvidia-gpu-drivers">Pre-Installed NVIDIA GPU Drivers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-installed-nvidia-gpu-drivers-and-nvidia-container-toolkit">Pre-Installed NVIDIA GPU Drivers and NVIDIA Container Toolkit</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pre-installed-nvidia-container-toolkit-but-no-drivers">Pre-Installed NVIDIA Container Toolkit (but no drivers)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#running-a-custom-driver-image">Running a Custom Driver Image</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specifying-configuration-options-for-containerd">Specifying Configuration Options for containerd</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rancher-kubernetes-engine-2">Rancher Kubernetes Engine 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#microk8s">MicroK8s</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#verification-running-sample-gpu-applications">Verification: Running Sample GPU Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cuda-vectoradd">CUDA VectorAdd</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jupyter-notebook">Jupyter Notebook</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installation-on-commercially-supported-kubernetes-platforms">Installation on Commercially Supported Kubernetes Platforms</a></li>
</ul>
  </nav></div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">

<div class="footer-links">
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Manage My Privacy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/preferences/start/">Do Not Sell or Share My Data</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/">Terms of Service</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/">Accessibility</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/">Corporate Policies</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/product-security/">Product Security</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/contact/">Contact</a>
  
  
  
</div>
</div>
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright  2020-2025, NVIDIA Corporation.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
</div>

  </footer>

  <script type="text/javascript">if (typeof _satellite !== "undefined") {_satellite.pageBottom();}</script>
  


  </body>
</html>