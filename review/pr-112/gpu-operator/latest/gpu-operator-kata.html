<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GPU Operator with Kata Containers &mdash; NVIDIA GPU Operator 24.6.2 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/api-styles.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/design-tabs.js"></script>
        <script crossorigin="anonymous" integrity="sha256-QdTG1YTLLTwD3b95jLqFxpQX9uYuJMNAtVZgwKX4oYU=" src="https://cdn.jsdelivr.net/npm/mermaid@9.3.0/dist/mermaid.min.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/version.js"></script>
        <script src="_static/social-media.js"></script>
        <script>initMermaid();</script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="GPU Operator with Confidential Containers and Kata" href="gpu-operator-confidential-containers.html" />
    <link rel="prev" title="GPU Operator with KubeVirt" href="gpu-operator-kubevirt.html" />
 
<script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
          </a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">About the Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="upgrade.html">Upgrade</a></li>
<li class="toctree-l1"><a class="reference internal" href="uninstall.html">Uninstall</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-vgpu.html">Using NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Operator Configuration</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-sharing.html">Time-Slicing GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-outdated-kernels.html">Outdated Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom-driver-params.html">Custom GPU Driver Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="precompiled-drivers.html">Precompiled Driver Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-configuration.html">GPU Driver CRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="cdi.html">Container Device Interface Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Sandboxed Workloads</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kubevirt.html">KubeVirt</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Kata Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-confidential-containers.html">Confidential Containers and Kata</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specialized Networks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-proxy.html">HTTP Proxy</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-air-gapped.html">Air-Gapped Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-service-mesh.html">Service Mesh</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CSP configurations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="amazon-eks.html">Amazon EKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="microsoft-aks.html">Azure AKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="google-gke.html">Google GKE</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA GPU Operator</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
<div> <!-- class="omni-version-warning" -->
  <p class="omni-version-warning-content"> Upgrade to NVIDIA Container Toolkit v1.16.2 or GPU Operator v24.6.2 to install a critical security update.<br/>
  Refer to <a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5582">Security Bulletin: NVIDIA Container Toolkit - September 2024</a> for more information.</p>
</div>

<li>
    <a href="https://docs.nvidia.com">NVIDIA Docs Hub</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>
    <a href="https://docs.nvidia.com/datacenter/cloud-native/">NVIDIA Cloud Native Technologies</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>GPU Operator with Kata Containers</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="gpu-operator-with-kata-containers">
<h1>GPU Operator with Kata Containers<a class="headerlink" href="#gpu-operator-with-kata-containers" title="Permalink to this headline">ÔÉÅ</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#about-the-operator-with-kata-containers" id="id2">About the Operator with Kata Containers</a></p>
<ul>
<li><p><a class="reference internal" href="#about-nvidia-kata-manager" id="id3">About NVIDIA Kata Manager</a></p></li>
<li><p><a class="reference internal" href="#nvidia-kata-manager-configuration" id="id4">NVIDIA Kata Manager Configuration</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#benefits-of-using-kata-containers" id="id5">Benefits of Using Kata Containers</a></p></li>
<li><p><a class="reference internal" href="#limitations-and-restrictions" id="id6">Limitations and Restrictions</a></p></li>
<li><p><a class="reference internal" href="#cluster-topology-considerations" id="id7">Cluster Topology Considerations</a></p></li>
<li><p><a class="reference internal" href="#prerequisites" id="id8">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#overview-of-installation-and-configuration" id="id9">Overview of Installation and Configuration</a></p></li>
<li><p><a class="reference internal" href="#install-the-confidential-containers-operator" id="id10">Install the Confidential Containers Operator</a></p></li>
<li><p><a class="reference internal" href="#install-the-nvidia-gpu-operator" id="id11">Install the NVIDIA GPU Operator</a></p>
<ul>
<li><p><a class="reference internal" href="#procedure" id="id12">Procedure</a></p></li>
<li><p><a class="reference internal" href="#verification" id="id13">Verification</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#run-a-sample-workload" id="id14">Run a Sample Workload</a></p>
<ul>
<li><p><a class="reference internal" href="#troubleshooting-workloads" id="id15">Troubleshooting Workloads</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#about-the-pod-annotation" id="id16">About the Pod Annotation</a></p></li>
</ul>
</div>
<section id="about-the-operator-with-kata-containers">
<h2>About the Operator with Kata Containers<a class="headerlink" href="#about-the-operator-with-kata-containers" title="Permalink to this headline">ÔÉÅ</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Technology Preview features are not supported in production environments
and are not functionally complete.
Technology Preview features provide early access to upcoming product features,
enabling customers to test functionality and provide feedback during the development process.
These releases may not have any documentation, and testing is limited.</p>
</div>
<p>Kata Containers are similar, but subtly different from traditional containers such as a Docker container.</p>
<p>A traditional container packages software for user-space isolation from the host,
but the container runs on the host and shares the operating system kernel with the host.
Sharing the operating system kernel is a potential vulnerability.</p>
<p>A Kata container runs in a virtual machine on the host.
The virtual machine has a separate operating system and operating system kernel.
Hardware virtualization and a separate kernel provide improved workload isolation
in comparison with traditional containers.</p>
<p>The NVIDIA GPU Operator works with the Kata container runtime.
Kata uses a hypervisor, like QEMU, to provide a lightweight virtual machine with a single purpose‚Äìto run a Kubernetes pod.</p>
<p>The following diagram shows the software components that Kubernetes uses to run a Kata container.</p>
<figure class="align-default" id="id1">
<div data-mermaid="flowchart LR
  a[Kubelet] --&gt; b[CRI] --&gt; c[Kata\nRuntime] --&gt; d[Lightweight\nQEMU VM] --&gt; e[Lightweight\nGuest OS] --&gt; f[Pod] --&gt; g[Container]"><div class="mermaid">
            flowchart LR
  a[Kubelet] --&gt; b[CRI] --&gt; c[Kata\nRuntime] --&gt; d[Lightweight\nQEMU VM] --&gt; e[Lightweight\nGuest OS] --&gt; f[Pod] --&gt; g[Container]
        </div></div><figcaption>
<p><span class="caption-text">Software Components with Kata Container Runtime</span><a class="headerlink" href="#id1" title="Permalink to this image">ÔÉÅ</a></p>
</figcaption>
</figure>
<p>NVIDIA supports Kata Containers by using the Confidential Containers Operator to install the Kata runtime and QEMU.
Even though the Operator isn‚Äôt used for confidential computing in this configuration, the Operator
simplifies the installation of the Kata runtime.</p>
<section id="about-nvidia-kata-manager">
<h3>About NVIDIA Kata Manager<a class="headerlink" href="#about-nvidia-kata-manager" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>When you configure the GPU Operator for Kata Containers, the Operator
deploys NVIDIA Kata Manager as an operand.</p>
<p>The manager downloads an NVIDIA optimized Linux kernel image and initial RAM disk that
provides the lightweight operating system for the virtual machines that run in QEMU.
These artifacts are downloaded from the NVIDIA container registry, nvcr.io, on each worker node.</p>
<p>The manager also configures each worker node with a runtime class, <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu</span></code>,
and configures containerd for the runtime class.</p>
</section>
<section id="nvidia-kata-manager-configuration">
<h3>NVIDIA Kata Manager Configuration<a class="headerlink" href="#nvidia-kata-manager-configuration" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The following part of the cluster policy shows the fields related to the manager:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">kataManager</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">  </span><span class="nt">config</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">artifactsDir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/opt/nvidia-gpu-operator/artifacts/runtimeclasses</span><span class="w"></span>
<span class="w">    </span><span class="nt">runtimeClasses</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">artifacts</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">pullSecret</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"></span>
<span class="w">        </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvcr.io/nvidia/cloud-native/kata-gpu-artifacts:ubuntu22.04-525</span><span class="w"></span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kata-qemu-nvidia-gpu</span><span class="w"></span>
<span class="w">      </span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">artifacts</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">pullSecret</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"></span>
<span class="w">        </span><span class="nt">url</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvcr.io/nvidia/cloud-native/kata-gpu-artifacts:ubuntu22.04-535-snp</span><span class="w"></span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kata-qemu-nvidia-gpu-snp</span><span class="w"></span>
<span class="w">      </span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span><span class="w"></span>
<span class="w">  </span><span class="nt">repository</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvcr.io/nvidia/cloud-native</span><span class="w"></span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">k8s-kata-manager</span><span class="w"></span>
<span class="w">  </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v0.1.0</span><span class="w"></span>
<span class="w">  </span><span class="nt">imagePullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">IfNotPresent</span><span class="w"></span>
<span class="w">  </span><span class="nt">imagePullSecrets</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span><span class="w"></span>
<span class="w">  </span><span class="nt">env</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span><span class="w"></span>
<span class="w">  </span><span class="nt">resources</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span><span class="w"></span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu</span></code> runtime class is used with Kata Containers.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu-snp</span></code> runtime class is used with Confidential Containers
and is installed by default even though it is not used with this configuration.</p>
</section>
</section>
<section id="benefits-of-using-kata-containers">
<h2>Benefits of Using Kata Containers<a class="headerlink" href="#benefits-of-using-kata-containers" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>The primary benefits of Kata Containers are as follows:</p>
<ul class="simple">
<li><p>Running untrusted workloads in a container.
The virtual machine provides a layer of defense against the untrusted code.</p></li>
<li><p>Limiting access to hardware devices such as NVIDIA GPUs.
The virtual machine is provided access to specific devices.
This approach ensures that the workload cannot access additional devices.</p></li>
<li><p>Transparent deployment of unmodified containers.</p></li>
</ul>
</section>
<section id="limitations-and-restrictions">
<h2>Limitations and Restrictions<a class="headerlink" href="#limitations-and-restrictions" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>GPUs are available to containers as a single GPU in passthrough mode only.
Multi-GPU passthrough and vGPU are not supported.</p></li>
<li><p>Support is limited to initial installation and configuration only.
Upgrade and configuration of existing clusters for Kata Containers is not supported.</p></li>
<li><p>Support for Kata Containers is limited to the implementation described on this page.
The Operator does not support Red Hat OpenShift sandbox containers.</p></li>
<li><p>Uninstalling the GPU Operator or the NVIDIA Kata Manager does not remove the files
that the manager downloads and installs in the <code class="docutils literal notranslate"><span class="pre">/opt/nvidia-gpu-operator/artifacts/runtimeclasses/kata-qemu-nvidia-gpu/</span></code>
directory on the worker nodes.</p></li>
<li><p>NVIDIA supports the Operator and Kata Containers with the containerd runtime only.</p></li>
</ul>
</section>
<section id="cluster-topology-considerations">
<h2>Cluster Topology Considerations<a class="headerlink" href="#cluster-topology-considerations" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>You can configure all the worker nodes in your cluster for Kata Containers or you configure some
nodes for Kata Containers and the others for traditional containers.
Consider the following example.</p>
<p>Node A is configured to run traditional containers.</p>
<p>Node B is configured to run Kata Containers.</p>
<p>Node A receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Driver</span> <span class="pre">Manager</span> <span class="pre">for</span> <span class="pre">Kubernetes</span></code> ‚Äì to install the data-center driver.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Container</span> <span class="pre">Toolkit</span></code> ‚Äì to ensure that containers can access GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Device</span> <span class="pre">Plugin</span> <span class="pre">for</span> <span class="pre">Kubernetes</span></code> ‚Äì to discover and advertise GPU resources to kubelet.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">DCGM</span> <span class="pre">and</span> <span class="pre">DCGM</span> <span class="pre">Exporter</span></code> ‚Äì to monitor GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">MIG</span> <span class="pre">Manager</span> <span class="pre">for</span> <span class="pre">Kubernetes</span></code> ‚Äì to manage MIG-capable GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Node</span> <span class="pre">Feature</span> <span class="pre">Discovery</span></code> ‚Äì to detect CPU, kernel, and host features and label worker nodes.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">GPU</span> <span class="pre">Feature</span> <span class="pre">Discovery</span></code> ‚Äì to detect NVIDIA GPUs and label worker nodes.</p></li>
</ul>
<p>Node B receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Kata</span> <span class="pre">Manager</span> <span class="pre">for</span> <span class="pre">Kubernetes</span></code> ‚Äì to manage the NVIDIA artifacts such as the
NVIDIA optimized Linux kernel image and initial RAM disk.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> ‚Äì to discover and advertise the passthrough GPUs to kubelet.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">VFIO</span> <span class="pre">Manager</span></code> ‚Äì to load the vfio-pci device driver and bind it to all GPUs on the node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Node</span> <span class="pre">Feature</span> <span class="pre">Discovery</span></code> ‚Äì to detect CPU security features, NVIDIA GPUs, and label worker nodes.</p></li>
</ul>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">ÔÉÅ</a></h2>
<ul>
<li><p>Your hosts are configured to enable hardware virtualization and Access Control Services (ACS).
With some AMD CPUs and BIOSes, ACS might be grouped under Advanced Error Reporting (AER).
Enabling these features is typically performed by configuring the host BIOS.</p></li>
<li><p>Your hosts are configured to support IOMMU.</p>
<p>If the output from running <code class="docutils literal notranslate"><span class="pre">ls</span> <span class="pre">/sys/kernel/iommu_groups</span></code> includes <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">1</span></code>, and so on,
then your host is configured for IOMMU.</p>
<p>If a host is not configured or you are unsure, add the <code class="docutils literal notranslate"><span class="pre">intel_iommu=on</span></code> Linux kernel command-line argument.
For most Linux distributions, you add the argument to the <code class="docutils literal notranslate"><span class="pre">/etc/default/grub</span></code> file:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
GRUB_CMDLINE_LINUX_DEFAULT=&quot;quiet intel_iommu=on modprobe.blacklist=nouveau&quot;
...
</pre></div>
</div>
<p>On Ubuntu systems, run <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">update-grub</span></code> after making the change to configure the bootloader.
On other systems, you might need to run <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">dracut</span></code> after making the change.
Refer to the documentation for your operating system.
Reboot the host after configuring the bootloader.</p>
</li>
<li><p>You have a Kubernetes cluster and you have cluster administrator privileges.</p></li>
</ul>
</section>
<section id="overview-of-installation-and-configuration">
<h2>Overview of Installation and Configuration<a class="headerlink" href="#overview-of-installation-and-configuration" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Installing and configuring your cluster to support the NVIDIA GPU Operator with Kata Containers is as follows:</p>
<ol class="arabic">
<li><p>Label the worker nodes that you want to use with Kata Containers.</p>
<p>This step ensures that you can continue to run traditional container workloads with GPU or vGPU workloads on some nodes in your cluster.
Alternatively, you can set the default sandbox workload to <code class="docutils literal notranslate"><span class="pre">vm-passthrough</span></code> to run confidential containers on all worker nodes.</p>
</li>
<li><p>Install the Confidential Containers Operator.</p>
<p>This step installs the Operator and also the Kata Containers runtime that NVIDIA uses for Kata Containers.</p>
</li>
<li><p>Install the NVIDIA GPU Operator.</p>
<p>You install the Operator and specify options to deploy the operands that are required for Kata Containers.</p>
</li>
</ol>
<p>After installation, you can run a sample workload.</p>
</section>
<section id="install-the-confidential-containers-operator">
<h2>Install the Confidential Containers Operator<a class="headerlink" href="#install-the-confidential-containers-operator" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Perform the following steps to install and verify the Confidential Containers Operator:</p>
<ol class="arabic">
<li><p>Label the nodes to run virtual machines in containers.
Label only the nodes that you want to run with Kata Containers.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl label node &lt;node-name&gt; nvidia.com/gpu.workload.config<span class="o">=</span>vm-passthrough
</pre></div>
</div>
</li>
<li><p>Set the Operator version in an environment variable:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">export</span> <span class="nv">VERSION</span><span class="o">=</span>v0.7.0
</pre></div>
</div>
</li>
<li><p>Install the Operator:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -k <span class="s2">&quot;github.com/confidential-containers/operator/config/release?ref=</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">namespace/confidential-containers-system created</span>
<span class="go">customresourcedefinition.apiextensions.k8s.io/ccruntimes.confidentialcontainers.org created</span>
<span class="go">serviceaccount/cc-operator-controller-manager created</span>
<span class="go">role.rbac.authorization.k8s.io/cc-operator-leader-election-role created</span>
<span class="go">clusterrole.rbac.authorization.k8s.io/cc-operator-manager-role created</span>
<span class="go">clusterrole.rbac.authorization.k8s.io/cc-operator-metrics-reader created</span>
<span class="go">clusterrole.rbac.authorization.k8s.io/cc-operator-proxy-role created</span>
<span class="go">rolebinding.rbac.authorization.k8s.io/cc-operator-leader-election-rolebinding created</span>
<span class="go">clusterrolebinding.rbac.authorization.k8s.io/cc-operator-manager-rolebinding created</span>
<span class="go">clusterrolebinding.rbac.authorization.k8s.io/cc-operator-proxy-rolebinding created</span>
<span class="go">configmap/cc-operator-manager-config created</span>
<span class="go">service/cc-operator-controller-manager-metrics-service created</span>
<span class="go">deployment.apps/cc-operator-controller-manager create</span>
</pre></div>
</div>
</li>
<li><p>Optional: View the pods and services in the <code class="docutils literal notranslate"><span class="pre">confidential-containers-system</span></code> namespace:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pod,svc -n confidential-containers-system
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                                                 READY   STATUS    RESTARTS   AGE</span>
<span class="go">pod/cc-operator-controller-manager-c98c4ff74-ksb4q   2/2     Running   0          2m59s</span>

<span class="go">NAME                                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE</span>
<span class="go">service/cc-operator-controller-manager-metrics-service   ClusterIP   10.98.221.141   &lt;none&gt;        8443/TCP   2m59s</span>
</pre></div>
</div>
</li>
<li><p>Install the sample Confidential Containers runtime by creating the manifests and then editing the node selector so
that the runtime is installed only on the labelled nodes.</p>
<ol class="arabic">
<li><p>Create a local copy of the manifests in a file that is named <code class="docutils literal notranslate"><span class="pre">ccruntime.yaml</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply --dry-run<span class="o">=</span>client -o yaml <span class="se">\</span>
    -k <span class="s2">&quot;github.com/confidential-containers/operator/config/samples/ccruntime/default?ref=</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span><span class="s2">&quot;</span> &gt; ccruntime.yaml
</pre></div>
</div>
</li>
<li><p>Edit the <code class="docutils literal notranslate"><span class="pre">ccruntime.yaml</span></code> file and set the node selector as follows:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">confidentialcontainers.org/v1beta1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">CcRuntime</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">ccNodeSelector</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">nvidia.com/gpu.workload.config</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;vm-passthrough&quot;</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Apply the modified manifests:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f ccruntime.yaml
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">ccruntime.confidentialcontainers.org/ccruntime-sample created</span>
</pre></div>
</div>
</li>
</ol>
<p>Wait a few minutes for the Operator to create the base runtime classes.</p>
</li>
<li><p>Optional: View the runtime classes:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get runtimeclass
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME            HANDLER         AGE</span>
<span class="go">kata            kata            13m</span>
<span class="go">kata-clh        kata-clh        13m</span>
<span class="go">kata-clh-tdx    kata-clh-tdx    13m</span>
<span class="go">kata-qemu       kata-qemu       13m</span>
<span class="go">kata-qemu-sev   kata-qemu-sev   13m</span>
<span class="go">kata-qemu-snp   kata-qemu-snp   13m</span>
<span class="go">kata-qemu-tdx   kata-qemu-tdx   13m</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="install-the-nvidia-gpu-operator">
<h2>Install the NVIDIA GPU Operator<a class="headerlink" href="#install-the-nvidia-gpu-operator" title="Permalink to this headline">ÔÉÅ</a></h2>
<section id="procedure">
<h3>Procedure<a class="headerlink" href="#procedure" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>Perform the following steps to install the Operator for use with Kata Containers:</p>
<ol class="arabic">
<li><p>Add and update the NVIDIA Helm repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia <span class="se">\</span>
   <span class="o">&amp;&amp;</span> helm repo update
</pre></div>
</div>
</li>
<li><p>Specify at least the following options when you install the Operator.
If you want to run Kata Containers by default on all worker nodes, also specify <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">sandboxWorkloads.defaultWorkload=vm-passthough</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
   -n gpu-operator --create-namespace <span class="se">\</span>
   nvidia/gpu-operator <span class="se">\</span>
   --set sandboxWorkloads.enabled<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
   --set kataManager.enabled<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME: gpu-operator</span>
<span class="go">LAST DEPLOYED: Tue Jul 25 19:19:07 2023</span>
<span class="go">NAMESPACE: gpu-operator</span>
<span class="go">STATUS: deployed</span>
<span class="go">REVISION: 1</span>
<span class="go">TEST SUITE: None</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="verification">
<h3>Verification<a class="headerlink" href="#verification" title="Permalink to this headline">ÔÉÅ</a></h3>
<ol class="arabic">
<li><p>Verify that the Kata Manager and VFIO Manager operands are running:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pods -n gpu-operator
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                                                         READY   STATUS      RESTARTS   AGE</span>
<span class="go">gpu-operator-57bf5d5769-nb98z                                1/1     Running     0          6m21s</span>
<span class="go">gpu-operator-node-feature-discovery-master-b44f595bf-5sjxg   1/1     Running     0          6m21s</span>
<span class="go">gpu-operator-node-feature-discovery-worker-lwhdr             1/1     Running     0          6m21s</span>
<span class="hll"><span class="go">nvidia-kata-manager-bw5mb                                    1/1     Running     0          3m36s</span>
</span><span class="go">nvidia-sandbox-device-plugin-daemonset-cr4s6                 1/1     Running     0          2m37s</span>
<span class="go">nvidia-sandbox-validator-9wjm4                               1/1     Running     0          2m37s</span>
<span class="hll"><span class="go">nvidia-vfio-manager-vg4wp                                    1/1     Running     0          3m36s</span>
</span></pre></div>
</div>
</li>
<li><p>Verify that the <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu</span></code> and <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu-snp</span></code> runtime classes are available:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get runtimeclass
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                       HANDLER                    AGE</span>
<span class="go">kata                       kata                       37m</span>
<span class="go">kata-clh                   kata-clh                   37m</span>
<span class="go">kata-clh-tdx               kata-clh-tdx               37m</span>
<span class="go">kata-qemu                  kata-qemu                  37m</span>
<span class="hll"><span class="go">kata-qemu-nvidia-gpu       kata-qemu-nvidia-gpu       96s</span>
</span><span class="hll"><span class="go">kata-qemu-nvidia-gpu-snp   kata-qemu-nvidia-gpu-snp   96s</span>
</span><span class="go">kata-qemu-sev              kata-qemu-sev              37m</span>
<span class="go">kata-qemu-snp              kata-qemu-snp              37m</span>
<span class="go">kata-qemu-tdx              kata-qemu-tdx              37m</span>
<span class="go">nvidia                     nvidia                     97s</span>
</pre></div>
</div>
</li>
<li><p>Optional: If you have host access to the worker node, you can perform the following steps:</p>
<ol class="arabic">
<li><p>Confirm that the host uses the <code class="docutils literal notranslate"><span class="pre">vfio-pci</span></code> device driver for GPUs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lspci -nnk -d 10de:
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">65:00.0 3D controller [0302]: NVIDIA Corporation GA102GL [A10] [10de:2236] (rev a1)</span>
<span class="go">        Subsystem: NVIDIA Corporation GA102GL [A10] [10de:1482]</span>
<span class="hll"><span class="go">        Kernel driver in use: vfio-pci</span>
</span><span class="go">        Kernel modules: nvidiafb, nouveau</span>
</pre></div>
</div>
</li>
<li><p>Confirm that NVIDIA Kata Manager installed the <code class="docutils literal notranslate"><span class="pre">kata-qemu-nvidia-gpu</span></code> runtime class files:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ls -1 /opt/nvidia-gpu-operator/artifacts/runtimeclasses/kata-qemu-nvidia-gpu/
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">configuration-nvidia-gpu-qemu.toml</span>
<span class="go">kata-ubuntu-jammy-nvidia-gpu.initrd</span>
<span class="go">vmlinuz-5.xx.x-xxx-nvidia-gpu</span>
<span class="go">...</span>
</pre></div>
</div>
</li>
</ol>
</li>
</ol>
</section>
</section>
<section id="run-a-sample-workload">
<h2>Run a Sample Workload<a class="headerlink" href="#run-a-sample-workload" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>A pod specification for a Kata container requires the following:</p>
<ul class="simple">
<li><p>Specify a Kata runtime class.</p></li>
<li><p>Specify a passthrough GPU resource.</p></li>
</ul>
<ol class="arabic">
<li><p>Determine the passthrough GPU resource names:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">kubectl get nodes -l nvidia.com/gpu.present -o json | \</span>
<span class="go">  jq &#39;.items[0].status.allocatable |</span>
<span class="go">    with_entries(select(.key | startswith(&quot;nvidia.com/&quot;))) |</span>
<span class="go">    with_entries(select(.value != &quot;0&quot;))&#39;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">   &quot;nvidia.com/GA102GL_A10&quot;: &quot;1&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</li>
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">cuda-vectoradd-kata.yaml</span></code>, like the following example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cuda-vectoradd-kata</span><span class="w"></span>
<span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w"></span>
<span class="hll"><span class="w">    </span><span class="nt">cdi.k8s.io/gpu</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;nvidia.com/pgpu=0&quot;</span><span class="w"></span>
</span><span class="w">    </span><span class="nt">io.katacontainers.config.hypervisor.default_memory</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;16384&quot;</span><span class="w"></span>
<span class="hll"><span class="nt">spec</span><span class="p">:</span><span class="w"></span>
</span><span class="w">  </span><span class="nt">runtimeClassName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kata-qemu-nvidia-gpu</span><span class="w"></span>
<span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OnFailure</span><span class="w"></span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cuda-vectoradd</span><span class="w"></span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="hll"><span class="w">      </span><span class="nt">limits</span><span class="p">:</span><span class="w"></span>
</span><span class="w">        </span><span class="s">&quot;nvidia.com/GA102GL_A10&quot;</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">io.katacontainers.config.hypervisor.default_memory</span></code> annotation starts the VM with 16 GB of memory.
Modify the value to accommodate your workload.</p>
</li>
<li><p>Create the pod:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f cuda-vectoradd-kata.yaml
</pre></div>
</div>
</li>
<li><p>View the logs from pod:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl logs -n default cuda-vectoradd-kata
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">[Vector addition of 50000 elements]</span>
<span class="go">Copy input data from the host memory to the CUDA device</span>
<span class="go">CUDA kernel launch with 196 blocks of 256 threads</span>
<span class="go">Copy output data from the CUDA device to the host memory</span>
<span class="go">Test PASSED</span>
<span class="go">Done</span>
</pre></div>
</div>
</li>
<li><p>Delete the pod:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl delete -f cuda-vectoradd-kata.yaml
</pre></div>
</div>
</li>
</ol>
<section id="troubleshooting-workloads">
<h3>Troubleshooting Workloads<a class="headerlink" href="#troubleshooting-workloads" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>If the sample workload does not run, confirm that you labelled nodes to run virtual machines in containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get nodes -l nvidia.com/gpu.workload.config<span class="o">=</span>vm-passthrough
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME               STATUS   ROLES    AGE   VERSION</span>
<span class="go">kata-worker-1      Ready    &lt;none&gt;   10d   v1.27.3</span>
<span class="go">kata-worker-2      Ready    &lt;none&gt;   10d   v1.27.3</span>
<span class="go">kata-worker-3      Ready    &lt;none&gt;   10d   v1.27.3</span>
</pre></div>
</div>
</section>
</section>
<section id="about-the-pod-annotation">
<h2>About the Pod Annotation<a class="headerlink" href="#about-the-pod-annotation" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">cdi.k8s.io/gpu:</span> <span class="pre">&quot;nvidia.com/pgpu=0&quot;</span></code> annotation is used when the pod sandbox is created.
The annotation ensures that the virtual machine created by the Kata runtime is created with
the correct PCIe topology so that GPU passthrough succeeds.</p>
<p>The annotation refers to a Container Device Interface (CDI) device, <code class="docutils literal notranslate"><span class="pre">nvidia.com/pgpu=0</span></code>.
The <code class="docutils literal notranslate"><span class="pre">pgpu</span></code> indicates passthrough GPU and the <code class="docutils literal notranslate"><span class="pre">0</span></code> indicates the device index.
The index is defined by the order that the GPUs are enumerated on the PCI bus.
The index does not correlate to a CUDA index.</p>
<p>The NVIDIA Kata Manager creates a CDI specification on the GPU nodes.
The file includes a device entry for each passthrough device.</p>
<p>In the following sample <code class="docutils literal notranslate"><span class="pre">/var/run/cdi/nvidia.com-pgpu.yaml</span></code> file shows one GPU that
is bound to the VFIO PCI driver:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">cdiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.5.0</span><span class="w"></span>
<span class="nt">containerEdits</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">{}</span><span class="w"></span>
<span class="nt">devices</span><span class="p">:</span><span class="w"></span>
<span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerEdits</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">deviceNodes</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/dev/vfio/10</span><span class="w"></span>
<span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;0&quot;</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/pgpu</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="gpu-operator-kubevirt.html" class="btn btn-neutral float-left" title="GPU Operator with KubeVirt" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="gpu-operator-confidential-containers.html" class="btn btn-neutral float-right" title="GPU Operator with Confidential Containers and Kata" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
<img src="_static/NVIDIA-LogoBlack.svg" class="only-light"/>
<img src="_static/NVIDIA-LogoWhite.svg" class="only-dark"/>
<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>
<p>
  Copyright &#169; 2020-2024, NVIDIA Corporation.
</p>

    <p>
      <span class="lastupdated">Last updated on Oct 18, 2024.
      </span></p>
<script type="text/javascript">if (typeof _satellite !== "undefined"){ _satellite.pageBottom();}</script>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>