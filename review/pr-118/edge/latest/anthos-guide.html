<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NVIDIA GPUs with Google Anthos &mdash; NVIDIA Cloud Native Reference Architectures 1.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/api-styles.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/design-tabs.js"></script>
        <script src="_static/version.js"></script>
        <script src="_static/social-media.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Accelerating workloads with NVIDIA GPUs with Red Hat Device Edge" href="nvidia-gpu-with-device-edge.html" />
 
<script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
          </a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="nvidia-gpu-with-device-edge.html">Accelerating workloads with NVIDIA GPUs with Red Hat Device Edge</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">NVIDIA GPUs with Google Anthos</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Cloud Native Reference Architectures</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
<div> <!-- class="omni-version-warning" -->
  <p class="omni-version-warning-content"> Upgrade to NVIDIA Container Toolkit v1.16.2 or GPU Operator v24.6.2 to install a critical security update.<br/>
  Refer to <a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5582">Security Bulletin: NVIDIA Container Toolkit - September 2024</a> for more information.</p>
</div>

<li>
    <a href="https://docs.nvidia.com">NVIDIA Docs Hub</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>
    <a href="https://docs.nvidia.com/datacenter/cloud-native/">NVIDIA Cloud Native Technologies</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>NVIDIA GPUs with Google Anthos</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="nvidia-gpus-with-google-anthos">
<span id="anthos-guide"></span><h1>NVIDIA GPUs with Google Anthos<a class="headerlink" href="#nvidia-gpus-with-google-anthos" title="Permalink to this headline"></a></h1>
<section id="changelog">
<h2>Changelog<a class="headerlink" href="#changelog" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><dl class="simple">
<dt>3/22/2020 (author: PR):</dt><dd><ul>
<li><p>Fixed URLs</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>11/30/2020 (author: PR/DF):</dt><dd><ul>
<li><p>Added information on Anthos on bare metal</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>11/25/2020 (author: PR):</dt><dd><ul>
<li><p>Migrated docs to new format</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>8/14/2020 (author: PR):</dt><dd><ul>
<li><p>Initial Version</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>Google Cloud’s Anthos is a modern application management platform that lets users
build, deploy, and manage applications anywhere in a secure, consistent manner.
The platform provides a consistent development and operations experience across
deployments while reducing operational overhead and improving developer productivity.
Anthos runs in hybrid and multi-cloud environments that spans <a class="reference external" href="https://cloud.google.com/kubernetes-engine">Google Cloud</a>,
<a class="reference external" href="https://cloud.google.com/anthos/docs/setup/on-premises">on-premise</a>, and is generally
available on <a class="reference external" href="https://cloud.google.com/anthos/docs/setup/public-cloud">Amazon Web Services (AWS)</a>.
Support for Anthos on Microsoft Azure is in preview. For more information on Anthos,
see the <a class="reference external" href="https://cloud.google.com/anthos">product overview</a>.</p>
<p>Systems with NVIDIA GPUs can be deployed in various configurations for use with Google Cloud’s Anthos.
The purpose of this document is to provide users with steps on getting started with using
NVIDIA GPUs with Anthos in these various configurations.</p>
</section>
<section id="deployment-configurations">
<h2>Deployment Configurations<a class="headerlink" href="#deployment-configurations" title="Permalink to this headline"></a></h2>
<p>Anthos can be deployed in different configurations. Depending on your deployment, choose one of the sections below
to get started with NVIDIA GPUs in Google Cloud’s Anthos:</p>
<ol class="arabic simple">
<li><p><a class="reference internal" href="#anthos-dgx-bm"><span class="std std-ref">Anthos Clusters on Bare Metal with NVIDIA DGX Systems and GPU-Accelerated Servers</span></a></p></li>
<li><p><a class="reference internal" href="#anthos-virt"><span class="std std-ref">Anthos Clusters with VMware and NVIDIA GPU-Accelerated Servers</span></a></p></li>
</ol>
</section>
<section id="supported-platforms">
<h2>Supported Platforms<a class="headerlink" href="#supported-platforms" title="Permalink to this headline"></a></h2>
<section id="gpus">
<h3>GPUs<a class="headerlink" href="#gpus" title="Permalink to this headline"></a></h3>
<p>The following GPUs are supported:</p>
<ul class="simple">
<li><p>NVIDIA A100, T4 and V100</p></li>
</ul>
</section>
<section id="dgx-systems">
<h3>DGX Systems<a class="headerlink" href="#dgx-systems" title="Permalink to this headline"></a></h3>
<p>The following NVIDIA DGX systems are supported:</p>
<ul class="simple">
<li><p>NVIDIA DGX A100</p></li>
<li><p>NVIDIA DGX-2 and DGX-1 (Volta)</p></li>
</ul>
</section>
<section id="linux-distributions">
<h3>Linux Distributions<a class="headerlink" href="#linux-distributions" title="Permalink to this headline"></a></h3>
<p>The following Linux distributions are supported:</p>
<ul class="simple">
<li><p>Ubuntu 18.04.z, 20.04.z LTS</p></li>
</ul>
<p>For more information on the Anthos Ready platforms, visit this <a class="reference external" href="https://cloud.google.com/anthos/docs/resources/partner-platforms#nvidia">page</a>.</p>
</section>
</section>
<section id="getting-support">
<h2>Getting Support<a class="headerlink" href="#getting-support" title="Permalink to this headline"></a></h2>
<p>For support issues related to using GPUs with Anthos, please <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/new">open a ticket</a>
on the NVIDIA GPU Operator GitHub project. Your feedback is appreciated.</p>
<p>DGX customers can visit the <a class="reference external" href="https://www.nvidia.com/en-us/data-center/dgx-systems/support/">NVIDIA DGX Systems Support Portal</a>.</p>
</section>
<section id="anthos-clusters-on-bare-metal-with-nvidia-dgx-systems-and-gpu-accelerated-servers">
<span id="anthos-dgx-bm"></span><h2>Anthos Clusters on Bare Metal with NVIDIA DGX Systems and GPU-Accelerated Servers<a class="headerlink" href="#anthos-clusters-on-bare-metal-with-nvidia-dgx-systems-and-gpu-accelerated-servers" title="Permalink to this headline"></a></h2>
<p>Anthos on bare metal with DGX A100 or NVIDIA GPU-accelerated servers systems enables a consistent development and operational experience across deployments,
while reducing expensive overhead and improving developer productivity. Refer to the Anthos <a class="reference external" href="https://cloud.google.com/anthos/gke/docs">documentation</a> for
more information on Anthos cluster environments.</p>
<section id="installation-flow">
<h3>Installation Flow<a class="headerlink" href="#installation-flow" title="Permalink to this headline"></a></h3>
<p>The basic steps described in this document follows this workflow:</p>
<ol class="arabic simple">
<li><p>Configure nodes</p>
<ul class="simple">
<li><p>Ensure each node (including the control plane) meets the pre-requisites, including time synchronization, correct versions of Docker and other conditions.</p></li>
</ul>
</li>
<li><p>Configure networking (Optional)</p>
<ul class="simple">
<li><p>Ensure network connectivity between control plane and nodes - ideally the VIPs, control plane and the nodes in the cluster are in the same network subnet.</p></li>
</ul>
</li>
<li><p>Configure an admin workstation and set up Anthos to create the cluster</p>
<ul class="simple">
<li><p>Set up the cluster using Anthos on bare-metal</p></li>
</ul>
</li>
<li><p>Setup NVIDIA software on GPU nodes</p>
<ul class="simple">
<li><p>Set up the NVIDIA software components on the GPU nodes to ensure that your cluster can run CUDA applications.</p></li>
</ul>
</li>
</ol>
<p>At the end of the installation flow, you should have a user cluster with GPU-enabled nodes that you can use to deploy applications.</p>
</section>
<section id="configure-nodes">
<h3>Configure Nodes<a class="headerlink" href="#configure-nodes" title="Permalink to this headline"></a></h3>
<p>These steps are required on each node in the cluster (including the control plane).</p>
<section id="time-synchronization">
<h4>Time Synchronization<a class="headerlink" href="#time-synchronization" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Ensure <code class="docutils literal notranslate"><span class="pre">apparmor</span></code> is stopped:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get install -y apparmor-utils policycoreutils
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl --now <span class="nb">enable</span> apparmor <span class="se">\</span>
   <span class="o">&amp;&amp;</span> systemctl stop apparmor
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Synchronize the time on each node:</p>
<blockquote>
<div><ul>
<li><p>Check the current time</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>timedatectl
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">               Local time: Fri 2020-11-20 10:38:06 PST</span>
<span class="go">           Universal time: Fri 2020-11-20 18:38:06 UTC</span>
<span class="go">                 RTC time: Fri 2020-11-20 18:38:08</span>
<span class="go">                Time zone: US/Pacific (PST, -0800)</span>
<span class="go">System clock synchronized: no</span>
<span class="go">              NTP service: active</span>
<span class="go">          RTC in local TZ: no</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Configure the NTP server in <code class="docutils literal notranslate"><span class="pre">/etc/systemd/timesyncd.conf</span></code>:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">NTP=time.google.com</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Adjust the system clock:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>timedatectl set-local-rtc <span class="m">0</span> --adjust-system-clock
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Restart the service</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl restart systemd-timesyncd.service
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Verify the synchronization with the time server</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>timedatectl
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">               Local time: Fri 2020-11-20 11:03:22 PST</span>
<span class="go">           Universal time: Fri 2020-11-20 19:03:22 UTC</span>
<span class="go">                 RTC time: Fri 2020-11-20 19:03:22</span>
<span class="go">                Time zone: US/Pacific (PST, -0800)</span>
<span class="go">System clock synchronized: yes</span>
<span class="go">              NTP service: active</span>
<span class="go">          RTC in local TZ: no</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
</section>
<section id="test-network-connectivity">
<h4>Test Network Connectivity<a class="headerlink" href="#test-network-connectivity" title="Permalink to this headline"></a></h4>
<ul>
<li><p>Ensure you can <code class="docutils literal notranslate"><span class="pre">nslookup</span></code> on <em>hostname</em></p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl restart systemd-resolved <span class="se">\</span>
   <span class="o">&amp;&amp;</span> ping us.archive.ubuntu.com
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ping: us.archive.ubuntu.com: Temporary failure in name resolution</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Check the nameserver in <code class="docutils literal notranslate"><span class="pre">resolve.conf</span></code></p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat &lt;&lt;EOF &gt; /etc/resolv.conf
<span class="go">nameserver 8.8.8.8</span>
<span class="go">EOF</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>And re-test <code class="docutils literal notranslate"><span class="pre">ping</span></code></p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ping us.archive.ubuntu.com

<span class="go">PING us.archive.ubuntu.com (91.189.91.38) 56(84) bytes of data.</span>
<span class="go">64 bytes from banjo.canonical.com (91.189.91.38): icmp_seq=1 ttl=49 time=73.4 ms</span>
<span class="go">64 bytes from banjo.canonical.com (91.189.91.38): icmp_seq=2 ttl=49 time=73.3 ms</span>
<span class="go">64 bytes from banjo.canonical.com (91.189.91.38): icmp_seq=3 ttl=49 time=73.4 ms</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
</section>
<section id="install-docker">
<h4>Install Docker<a class="headerlink" href="#install-docker" title="Permalink to this headline"></a></h4>
<p>Follow these steps to install Docker. On DGX systems, Docker may already be installed using the <code class="docutils literal notranslate"><span class="pre">docker-ce</span></code> package.
In this case, use <code class="docutils literal notranslate"><span class="pre">docker.io</span></code> as the base installation package for Docker to ensure a successful cluster setup with
Anthos.</p>
<ul>
<li><p>Stop services using docker:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl stop kubelet <span class="se">\</span>
   <span class="o">&amp;&amp;</span> systemctl stop docker <span class="se">\</span>
   <span class="o">&amp;&amp;</span> systemctl stop containerd <span class="se">\</span>
   <span class="o">&amp;&amp;</span> systemctl stop containerd.io
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Purge the existing packages of Docker and <code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code> if any:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl stop run-docker-netns-default.mount <span class="se">\</span>
   <span class="o">&amp;&amp;</span> systemctl stop docker.haproxy
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>dpkg -r nv-docker-options <span class="se">\</span>
   <span class="o">&amp;&amp;</span> dpkg --purge nv-docker-options <span class="se">\</span>
   <span class="o">&amp;&amp;</span> dpkg -r nvidia-docker2 <span class="se">\</span>
   <span class="o">&amp;&amp;</span> dpkg --purge nvidia-docker2 <span class="se">\</span>
   <span class="o">&amp;&amp;</span> dpkg -r docker-ce <span class="se">\</span>
   <span class="o">&amp;&amp;</span> dpkg --purge docker-ce <span class="se">\</span>
   <span class="o">&amp;&amp;</span> dpkg -r docker-ce-cli <span class="se">\</span>
   <span class="o">&amp;&amp;</span> dpkg -r containerd <span class="se">\</span>
   <span class="o">&amp;&amp;</span> dpkg --purge containerd <span class="se">\</span>
   <span class="o">&amp;&amp;</span> dpkg -r containerd.io <span class="se">\</span>
   <span class="o">&amp;&amp;</span> dpkg --purge
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>Re-install Docker</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get update <span class="se">\</span>
   <span class="o">&amp;&amp;</span> apt-get install -y apt-transport-https <span class="se">\</span>
      ca-certificates <span class="se">\</span>
      curl <span class="se">\</span>
      software-properties-common <span class="se">\</span>
      inetutils-traceroute <span class="se">\</span>
      conntrack
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl -fsSL https://download.docker.com/linux/ubuntu/gpg <span class="p">|</span> apt-key add -
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>add-apt-repository <span class="se">\</span>
   <span class="s2">&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \</span>
<span class="gp">   $</span><span class="s2">(lsb_release -cs) stable&quot;</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get update <span class="se">\</span>
   <span class="o">&amp;&amp;</span> apt-get install -y docker.io
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>systemctl --now <span class="nb">enable</span> docker
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<section id="install-nvidia-docker-on-gpu-nodes">
<h5>Install nvidia-docker on GPU Nodes<a class="headerlink" href="#install-nvidia-docker-on-gpu-nodes" title="Permalink to this headline"></a></h5>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This step should be performed on the GPU nodes only</p>
</div>
<p>For DGX systems, re-install <code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code> from the DGX repositories:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>apt-get install -y nvidia-docker2
</pre></div>
</div>
<p>Since Kubernetes does not support the <code class="docutils literal notranslate"><span class="pre">--gpus</span></code> option with Docker yet, the <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime should
be setup as the default container runtime for Docker on the GPU node. This can be done by adding the
<code class="docutils literal notranslate"><span class="pre">default-runtime</span></code> line into the Docker daemon config file, which is usually located on the system
at <code class="docutils literal notranslate"><span class="pre">/etc/docker/daemon.json</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">   &quot;default-runtime&quot;: &quot;nvidia&quot;,</span>
<span class="go">   &quot;runtimes&quot;: {</span>
<span class="go">        &quot;nvidia&quot;: {</span>
<span class="go">            &quot;path&quot;: &quot;/usr/bin/nvidia-container-runtime&quot;,</span>
<span class="go">            &quot;runtimeArgs&quot;: []</span>
<span class="go">      }</span>
<span class="go">   }</span>
<span class="go">}</span>
</pre></div>
</div>
<p>Restart the Docker daemon to complete the installation after setting the default runtime:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo systemctl restart docker
</pre></div>
</div>
<p>For non-DGX systems, refer to the NVIDIA Container Toolkit <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker">installation guide</a>
to setup <code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code>.</p>
</section>
</section>
</section>
<section id="configure-networking-optional">
<h3>Configure Networking (Optional)<a class="headerlink" href="#configure-networking-optional" title="Permalink to this headline"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following steps are provided as a reference for configuring the network so that the control plane and the
nodes are on the same subnet by using tunnels and DNAT. If the nodes in your cluster are on the same subnet,
then you may skip this step.</p>
<p>In the example below:</p>
<ul class="simple">
<li><p>The control plane is at <code class="docutils literal notranslate"><span class="pre">10.117.29.41</span></code></p></li>
<li><p>The GPU node or admin workstation is at <code class="docutils literal notranslate"><span class="pre">10.110.20.149</span></code></p></li>
<li><p>The control plane VIP is <code class="docutils literal notranslate"><span class="pre">10.0.0.8</span></code></p></li>
</ul>
<p>If the machines are on a different subnet than each other or the control plane VIP then tunnel routes
can be used to establish connectivity.</p>
<p>There are two scenarios to consider:</p>
<ol class="arabic simple">
<li><p>If the machines are on the same subnet, but the VIP is on a different subnet, then add the correct
IP route (using <code class="docutils literal notranslate"><span class="pre">ip</span> <span class="pre">route</span> <span class="pre">add</span> <span class="pre">10.0.0.8</span> <span class="pre">via</span> <span class="pre">&lt;contro-plane-ip&gt;</span></code> from the GPU node or admin-workstation</p></li>
<li><p>If the machines and VIP are on different subnets, then a tunnel is also needed to enable the above
route command to succeed where <code class="docutils literal notranslate"><span class="pre">&lt;control-plane-ip&gt;</span></code> is the control plane tunnel <code class="docutils literal notranslate"><span class="pre">192.168.210.1</span></code>.</p></li>
</ol>
</div>
<section id="control-plane">
<h4>Control Plane<a class="headerlink" href="#control-plane" title="Permalink to this headline"></a></h4>
<p>Setup tunneling:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip tunnel add tun0 mode ipip <span class="nb">local</span> <span class="m">10</span>.117.29.41 remote <span class="m">10</span>.110.20.149
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip addr add <span class="m">192</span>.168.200.1/24 dev tun0
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip link <span class="nb">set</span> tun0 up
</pre></div>
</div>
<p>Update DNAT to support the control plane VIP over the tunnel:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>iptables -t nat -I PREROUTING  -p udp -d <span class="m">192</span>.168.210.1  --dport <span class="m">6081</span> -j DNAT --to-destination <span class="m">10</span>.117.29.41
</pre></div>
</div>
</section>
<section id="gpu-node-or-admin-workstation">
<h4>GPU Node or Admin Workstation<a class="headerlink" href="#gpu-node-or-admin-workstation" title="Permalink to this headline"></a></h4>
<p>Establish connectivity with the control plane:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip tunnel add tun1 mode ipip <span class="nb">local</span> <span class="m">10</span>.110.20.149  remote <span class="m">10</span>.117.29.41
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip addr add <span class="m">192</span>.168.210.2/24 dev tun1
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip link <span class="nb">set</span> tun1 up
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>ip route add <span class="m">10</span>.0.0.8/32 via <span class="m">192</span>.168.210.1
</pre></div>
</div>
<p>Setup DNAT:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>iptables -t nat -I OUTPUT -p udp -d <span class="m">10</span>.117.29.41  --dport <span class="m">6081</span> -j DNAT --to-destination <span class="m">192</span>.168.210.1
</pre></div>
</div>
</section>
</section>
<section id="configure-admin-workstation">
<h3>Configure Admin Workstation<a class="headerlink" href="#configure-admin-workstation" title="Permalink to this headline"></a></h3>
<p>Configure the admin workstation prior to setting up the cluster.</p>
<p>Download the Google Cloud SDK:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>wget https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-314.0.0-linux-x86_64.tar.gz <span class="se">\</span>
   <span class="o">&amp;&amp;</span> tar -xf google-cloud-sdk-314.0.0-linux-x86_64.tar.gz
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>google-cloud-sdk/install.sh
</pre></div>
</div>
<p>Install the Anthos authentication components:</p>
<blockquote>
<div><p>$ gcloud components install anthos-auth</p>
</div></blockquote>
<p>See the <a class="reference external" href="https://cloud.google.com/anthos/gke/docs/on-prem/how-to/install-overview-basic">Anthos installtion overview</a>
for detailed instructions for installing Anthos in an on-premise environment and setup your cluster.</p>
</section>
<section id="setup-nvidia-software-on-gpu-nodes">
<h3>Setup NVIDIA Software on GPU Nodes<a class="headerlink" href="#setup-nvidia-software-on-gpu-nodes" title="Permalink to this headline"></a></h3>
<p>Once the Anthos cluster has been set up, you can proceed to deploy the NVIDIA software components on the GPU nodes.</p>
<section id="nvidia-drivers">
<h4>NVIDIA Drivers<a class="headerlink" href="#nvidia-drivers" title="Permalink to this headline"></a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DGX systems include the NVIDIA drivers. This step can be skipped.</p>
</div>
<p>For complete instructions on setting up NVIDIA drivers, visit the quickstart
guide at <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html">https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html</a>.
The guide covers a number of pre-installation requirements and steps on supported Linux
distributions for a successful install of the driver.</p>
</section>
<section id="nvidia-device-plugin">
<h4>NVIDIA Device Plugin<a class="headerlink" href="#nvidia-device-plugin" title="Permalink to this headline"></a></h4>
<p>To use GPUs in Kubernetes, the <a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin/">NVIDIA Device Plugin</a> is required.
The NVIDIA Device Plugin is a daemonset that automatically enumerates the number of GPUs on each node of the cluster
and allows pods to be run on GPUs.</p>
<p>The preferred method to deploy the device plugin is as a daemonset using <code class="docutils literal notranslate"><span class="pre">helm</span></code>.</p>
<p>Add the <code class="docutils literal notranslate"><span class="pre">nvidia-device-plugin</span></code> <code class="docutils literal notranslate"><span class="pre">helm</span></code> repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm repo add nvdp https://nvidia.github.io/k8s-device-plugin <span class="se">\</span>
   <span class="o">&amp;&amp;</span> helm repo update
</pre></div>
</div>
<p>Deploy the device plugin:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --generate-name nvdp/nvidia-device-plugin
</pre></div>
</div>
<p>For more user configurable options while deploying the daemonset, refer to the device plugin
<a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin/#deployment-via-helm">README</a></p>
</section>
<section id="node-feature-discovery">
<h4>Node Feature Discovery<a class="headerlink" href="#node-feature-discovery" title="Permalink to this headline"></a></h4>
<p>For detecting the hardware configuration and system configuration, we will deploy the <a class="reference external" href="https://github.com/kubernetes-sigs/node-feature-discovery">Node Feature Discovery</a>
add-on:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/node-feature-discovery/v0.6.0/nfd-master.yaml.template
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/node-feature-discovery/v0.6.0/nfd-worker-daemonset.yaml.template
</pre></div>
</div>
<p>See the <a class="reference external" href="https://kubernetes-sigs.github.io/node-feature-discovery">NFD documentation</a> for more information on NFD.</p>
</section>
</section>
</section>
<section id="anthos-clusters-with-vmware-and-nvidia-gpu-accelerated-servers">
<span id="anthos-virt"></span><h2>Anthos Clusters with VMware and NVIDIA GPU Accelerated Servers<a class="headerlink" href="#anthos-clusters-with-vmware-and-nvidia-gpu-accelerated-servers" title="Permalink to this headline"></a></h2>
<p>Anthos running on-premise has requirements for which vSphere versions are supported along with network and storage requirements.
Please see the Anthos version compatibility matrix for more information:
<a class="reference external" href="https://cloud.google.com/anthos/gke/docs/on-prem/versioning-and-upgrades#version_compatibility_matrix.">https://cloud.google.com/anthos/gke/docs/on-prem/versioning-and-upgrades#version_compatibility_matrix</a>.</p>
<p>This guide assumes that the user already has an installed Anthos on-premise cluster in a vSphere environment. Please see
<a class="reference external" href="https://cloud.google.com/anthos/gke/docs/on-prem/how-to/install-overview-basic">https://cloud.google.com/anthos/gke/docs/on-prem/how-to/install-overview-basic</a>
for detailed instructions for installing Anthos in an on-premise environment.</p>
<p>Kubernetes provides access to special hardware resources such as NVIDIA GPUs, NICs,
Infiniband adapters and other devices through the <a class="reference external" href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">device plugin framework</a>.
However, configuring and managing nodes with these hardware resources requires
configuration of multiple software components such as drivers, container runtimes
or other libraries which are difficult and prone to errors. The <a class="reference external" href="https://github.com/NVIDIA/gpu-operator">NVIDIA GPU Operator</a>
uses the operator framework within Kubernetes to automate the management of all NVIDIA
software components needed to provision GPUs.</p>
<p>In the VMware vSphere configuration, Anthos uses the NVIDIA GPU Operator to configure GPU nodes in the Kubernetes cluster
so that the nodes can be used to schedule CUDA applications. The GPU Operator itself is
deployed using Helm. The rest of this section provides users with steps on getting
started.</p>
<section id="configuring-pcie-passthrough">
<h3>Configuring PCIe Passthrough<a class="headerlink" href="#configuring-pcie-passthrough" title="Permalink to this headline"></a></h3>
<p>For the GPU to be accessible to the VM, first you must enable <a class="reference external" href="https://kb.vmware.com/s/article/1010789">PCI Passthrough</a>
on the ESXi host. This can be done from the vSphere client. This will require a reboot
of the ESXi host to complete the process and therefore the host should be put into
maintenance mode and any VMs running on the ESXi host evacuated to another.
If you only have a single ESXi host, then the VMs will need to be restarted after the reboot.</p>
<p>From the vSphere client, select an ESXi host from the Inventory of VMware vSphere Client.
In the Configure tab, click Hardware &gt; PCI Devices. This will show you the
passthrough-enabled devices (you will most likely find none at this time).</p>
<a class="reference internal image-reference" href="_images/image01.png"><img alt="_images/image01.png" src="_images/image01.png" style="width: 800px;" /></a>
<p>Click CONFIGURE PASSTHROUGH to launch the Edit PCI Device Availability window. Look for the GPU device and
select the checkbox next to it (the GPU device will be recognizable as having NVIDIA Corporation in the Vendor Name view).
Select the GPU devices (you may have more than one) and click OK.</p>
<a class="reference internal image-reference" href="_images/image02.png"><img alt="_images/image02.png" src="_images/image02.png" style="width: 800px;" /></a>
<p>At this point, the GPU(s) will appear as Available (pending). You will need to select Reboot This Host and complete the reboot before proceeding to the next step.</p>
<a class="reference internal image-reference" href="_images/image03.png"><img alt="_images/image03.png" src="_images/image03.png" style="width: 800px;" /></a>
<p>It is a VMware best practice to reboot an ESXi host only when it is in maintenance mode and after all the VMs have been migrated to other hosts.
If you have only 1 ESXi host, then you can reboot without migrating the VMs, though shutting them down gracefully first is always a good idea.</p>
<a class="reference internal image-reference" href="_images/image04.png"><img alt="_images/image04.png" src="_images/image04.png" style="width: 400px;" /></a>
<p>Once the server has rebooted. Make sure to remove maintenance mode (if it was used) or restart the VMs that needed to be stopped (when only a single ESXi host is used).</p>
</section>
<section id="adding-gpus-to-a-node">
<h3>Adding GPUs to a Node<a class="headerlink" href="#adding-gpus-to-a-node" title="Permalink to this headline"></a></h3>
<section id="creating-a-node-pool-for-the-gpu-node">
<h4>Creating a Node Pool for the GPU Node<a class="headerlink" href="#creating-a-node-pool-for-the-gpu-node" title="Permalink to this headline"></a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is an optional step.</p>
</div>
<p>Node Pools are a good way to specify pools of Kubernetes worker nodes which may have different or unique attributes. In this case, we have the opportunity to
create a node pool which contains workers that manually have a GPU assigned to it. See <a class="reference external" href="https://cloud.google.com/anthos/gke/docs/on-prem/how-to/managing-node-pools?hl=en">managing node pools</a>
in the Google GKE documentation for more information regarding node pools with Anthos on-premise.</p>
<p>First, edit your user cluster config.yaml file on the admin workstation and add an additional node pool:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">- name: user-cluster1-gpu</span>
<span class="go">  cpus: 4</span>
<span class="go">  memoryMB: 8192</span>
<span class="go">  replicas: 1</span>
<span class="go">  labels:</span>
<span class="go">    hardware: gpu</span>
</pre></div>
</div>
<p>After adding the node pool to your configuration, use the <code class="docutils literal notranslate"><span class="pre">gkectl</span></code> update command push the change:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>gkectl update cluster --kubeconfig <span class="o">[</span>ADMIN_CLUSTER_KUBECONFIG<span class="o">]</span> <span class="se">\</span>
   --config <span class="o">[</span>USER_CLUSTER_KUBECONFIG<span class="o">]</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Reading config with version &quot;v1&quot;</span>
<span class="go">Update summary for cluster user-cluster1-bundledlb:</span>
<span class="go">   Node pool(s) to be created: [user-cluster1-gpu]</span>
<span class="go">Do you want to continue? [Y/n]: Y</span>
<span class="go">Updating cluster &quot;user-cluster1-bundledlb&quot;...</span>
<span class="go">Creating node MachineDeployment(s) in user cluster...  DONE</span>
<span class="go">Done updating the user cluster</span>
</pre></div>
</div>
</section>
<section id="add-gpus-to-nodes-in-vsphere">
<h4>Add GPUs to Nodes in vSphere<a class="headerlink" href="#add-gpus-to-nodes-in-vsphere" title="Permalink to this headline"></a></h4>
<p>Select an existing user-cluster node to add a GPU to (if you created a node pool
with the previous step then you would choose a node from that pool). Make sure that
this VM is on the host with the GPU (if you have vMotion enabled this could be as
simple as right clicking on the VM and selecting <strong>Migrate</strong>).</p>
<p>To configure a PCI device on a virtual machine, from the Inventory in vSphere Client,
right-click the virtual machine and select <strong>Power-&gt;Power Off</strong>.</p>
<a class="reference internal image-reference" href="_images/image05.png"><img alt="_images/image05.png" src="_images/image05.png" style="width: 800px;" /></a>
<p>After the VM is powered off, right-click the virtual machine and click <strong>Edit Settings</strong>.</p>
<a class="reference internal image-reference" href="_images/image06.png"><img alt="_images/image06.png" src="_images/image06.png" style="width: 400px;" /></a>
<p>Within the Edit Settings window, click <strong>ADD NEW DEVICE</strong>.</p>
<a class="reference internal image-reference" href="_images/image07.png"><img alt="_images/image07.png" src="_images/image07.png" style="width: 800px;" /></a>
<p>Choose PCI Device from the dropdown.</p>
<a class="reference internal image-reference" href="_images/image08.png"><img alt="_images/image08.png" src="_images/image08.png" style="width: 400px;" /></a>
<p>You may need to select the GPU or if it’s the only device available it may be automatically
selected for you. If you don’t see the GPU, it’s possible your VM is not currently on the
ESXi host with the passthrough device configured.</p>
<a class="reference internal image-reference" href="_images/image09.png"><img alt="_images/image09.png" src="_images/image09.png" style="width: 800px;" /></a>
<p>Expand the <strong>Memory</strong> section and make sure to select the option for Reserve all <strong>Guest Memory (All locked)</strong>.</p>
<a class="reference internal image-reference" href="_images/image10.png"><img alt="_images/image10.png" src="_images/image10.png" style="width: 800px;" /></a>
<p>Click <strong>OK</strong>.</p>
<p>Before the VM can be started, the VM/Host Rule for VM anti-affinity must be deleted.
(Note that this step may not be necessary if your cluster’s <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> contains <code class="docutils literal notranslate"><span class="pre">antiAffinityGroups.enabled:</span> <span class="pre">False</span></code>).
From the vSphere Inventory list, click on the cluster then the <strong>Configure</strong> tab and then
under <strong>Configuration</strong> select <strong>VM/Host Rules</strong>. Select the rule containing your node and delete it.</p>
<a class="reference internal image-reference" href="_images/image11.png"><img alt="_images/image11.png" src="_images/image11.png" style="width: 800px;" /></a>
<p>Now you can power on the VM, right click on the VM and select <strong>Power&gt;Power On</strong>.</p>
<a class="reference internal image-reference" href="_images/image12.png"><img alt="_images/image12.png" src="_images/image12.png" style="width: 800px;" /></a>
<p>If vSphere presents you with <strong>Power On Recommendations</strong> then select <strong>OK</strong>.</p>
<a class="reference internal image-reference" href="_images/image13.png"><img alt="_images/image13.png" src="_images/image13.png" style="width: 800px;" /></a>
<p>The following steps should be performed from your Admin Workstation or other Linux system which has the ability to use <code class="docutils literal notranslate"><span class="pre">kubectl</span></code> to work with the cluster.</p>
<p>Install the NVIDIA GPU Operator:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
  -n gpu-operator --create-namespace <span class="se">\</span>
  nvidia/gpu-operator
</pre></div>
</div>
<p>Refer to <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html" title="(in NVIDIA GPU Operator)"><span>Installing the NVIDIA GPU Operator</span></a>
in the NVIDIA GPU Operator documentation for installation options.</p>
</section>
</section>
<section id="running-gpu-applications">
<h3>Running GPU Applications<a class="headerlink" href="#running-gpu-applications" title="Permalink to this headline"></a></h3>
<section id="jupyter-notebooks">
<h4>Jupyter Notebooks<a class="headerlink" href="#jupyter-notebooks" title="Permalink to this headline"></a></h4>
<p>This section of the guide walks through how to run a sample Jupyter notebook on the Kubernetes cluster.</p>
<ol class="arabic">
<li><p>Create a yaml file for the pod and service for the notebook:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nv">LOADBALANCERIP</span><span class="o">=</span>&lt;ip address to be used to expose the service&gt;
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat &lt;&lt; EOF <span class="p">|</span> kubectl create -f -
<span class="go">apiVersion: v1</span>
<span class="go">kind: Service</span>
<span class="go">metadata:</span>
<span class="go">  name: tf-notebook</span>
<span class="go">  labels:</span>
<span class="go">    app: tf-notebook</span>
<span class="go">spec:</span>
<span class="go">  type: LoadBalancer</span>
<span class="go">  loadBalancerIP: $LOADBALANCERIP</span>
<span class="go">  ports:</span>
<span class="go">  - port: 80</span>
<span class="go">    name: http</span>
<span class="go">    targetPort: 8888</span>
<span class="go">    nodePort: 30001</span>
<span class="go">  selector:</span>
<span class="go">    app: tf-notebook</span>
<span class="go">---</span>
<span class="go">apiVersion: v1</span>
<span class="go">kind: Pod</span>
<span class="go">metadata:</span>
<span class="go">  name: tf-notebook</span>
<span class="go">  labels:</span>
<span class="go">    app: tf-notebook</span>
<span class="go">spec:</span>
<span class="go">  securityContext:</span>
<span class="go">    fsGroup: 0</span>
<span class="go">  containers:</span>
<span class="go">  - name: tf-notebook</span>
<span class="go">    image: tensorflow/tensorflow:latest-gpu-jupyter</span>
<span class="go">    resources:</span>
<span class="go">      limits:</span>
<span class="go">        nvidia.com/gpu: 1</span>
<span class="go">    ports:</span>
<span class="go">    - containerPort: 8888</span>
<span class="go">      name: notebook</span>
<span class="go">EOF</span>
</pre></div>
</div>
</li>
<li><p>View the logs of the tf-notebook pod to obtain the token:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl logs tf-notebook
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[I 19:07:43.061 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret</span>
<span class="go">[I 19:07:43.423 NotebookApp] Serving notebooks from local directory: /tf</span>
<span class="go">[I 19:07:43.423 NotebookApp] The Jupyter Notebook is running at:</span>
<span class="go">[I 19:07:43.423 NotebookApp] http://tf-notebook:8888/?token=fc5d8b9d6f29d5ddad62e8c731f83fc8e90a2d817588d772</span>
<span class="go">[I 19:07:43.423 NotebookApp]  or http://127.0.0.1:8888/?token=fc5d8b9d6f29d5ddad62e8c731f83fc8e90a2d817588d772</span>
<span class="go">[I 19:07:43.423 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span>
<span class="go">[C 19:07:43.429 NotebookApp]</span>

<span class="go">   To access the notebook, open this file in a browser:</span>
<span class="go">      file:///root/.local/share/jupyter/runtime/nbserver-1-open.html</span>
<span class="go">   Or copy and paste one of these URLs:</span>
<span class="go">      http://tf-notebook:8888/?token=fc5d8b9d6f29d5ddad62e8c731f83fc8e90a2d817588d772</span>
<span class="go">   or http://127.0.0.1:8888/?token=fc5d8b9d6f29d5ddad62e8c731f83fc8e90a2d817588d772</span>
<span class="go">[I 19:08:24.180 NotebookApp] 302 GET / (172.16.20.30) 0.61ms</span>
<span class="go">[I 19:08:24.182 NotebookApp] 302 GET /tree? (172.16.20.30) 0.57ms</span>
</pre></div>
</div>
</li>
<li><p>From a web browser, navigate to <code class="docutils literal notranslate"><span class="pre">http://&lt;LOADBALANCERIP&gt;</span></code> and enter the token where prompted to login:
Depending on your environment you may not have web browser access to the exposed service. You may be able to use
<a class="reference external" href="https://www.ssh.com/ssh/tunneling/example">SSH Port Forwarding/Tunneling</a> to achieve this.</p>
<a class="reference internal image-reference" href="_images/image14.png"><img alt="_images/image14.png" src="_images/image14.png" style="width: 800px;" /></a>
</li>
<li><p>Once logged in, navigate click on the tenserflow-tutorials folder and then on the first file, <strong>classification.ipynb</strong>:</p>
<a class="reference internal image-reference" href="_images/image15.png"><img alt="_images/image15.png" src="_images/image15.png" style="width: 800px;" /></a>
</li>
<li><p>This will launch a new tab with the Notebook loaded. You can now run through the Notebook by clicking on the <strong>Run</strong>
button. The notebook will step through each section and execute the code as you go. Continue pressing <strong>Run</strong> until you
reach the end of the notebook and observe the execution of the classification program.</p>
<a class="reference internal image-reference" href="_images/image16.png"><img alt="_images/image16.png" src="_images/image16.png" style="width: 800px;" /></a>
</li>
<li><p>Once the notebook is complete you can check the logs of the <code class="docutils literal notranslate"><span class="pre">tf-notebook</span></code> pod to confirm it was using the GPU:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">=========snip===============</span>
<span class="go">[I 19:17:58.116 NotebookApp] Saving file at /tensorflow-tutorials/classification.ipynb</span>
<span class="go">2020-05-21 19:21:01.422482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1</span>
<span class="go">2020-05-21 19:21:01.436767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.437469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:</span>
<span class="go">pciBusID: 0000:13:00.0 name: Tesla P4 computeCapability: 6.1</span>
<span class="go">coreClock: 1.1135GHz coreCount: 20 deviceMemorySize: 7.43GiB deviceMemoryBandwidth: 178.99GiB/s</span>
<span class="go">2020-05-21 19:21:01.438477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1</span>
<span class="go">2020-05-21 19:21:01.462370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10</span>
<span class="go">2020-05-21 19:21:01.475269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10</span>
<span class="go">2020-05-21 19:21:01.478104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10</span>
<span class="go">2020-05-21 19:21:01.501057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10</span>
<span class="go">2020-05-21 19:21:01.503901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10</span>
<span class="go">2020-05-21 19:21:01.544763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7</span>
<span class="go">2020-05-21 19:21:01.545022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.545746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.546356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0</span>
<span class="go">2020-05-21 19:21:01.546705: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA</span>
<span class="go">2020-05-21 19:21:01.558283: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2194840000 Hz</span>
<span class="go">2020-05-21 19:21:01.558919: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6f2c000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:</span>
<span class="go">2020-05-21 19:21:01.558982: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version</span>
<span class="go">2020-05-21 19:21:01.645786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.646387: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x53ab350 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:</span>
<span class="go">2020-05-21 19:21:01.646430: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P4, Compute Capability 6.1</span>
<span class="go">2020-05-21 19:21:01.647005: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.647444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:</span>
<span class="go">pciBusID: 0000:13:00.0 name: Tesla P4 computeCapability: 6.1</span>
<span class="go">coreClock: 1.1135GHz coreCount: 20 deviceMemorySize: 7.43GiB deviceMemoryBandwidth: 178.99GiB/s</span>
<span class="go">2020-05-21 19:21:01.647523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1</span>
<span class="go">2020-05-21 19:21:01.647570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10</span>
<span class="go">2020-05-21 19:21:01.647611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10</span>
<span class="go">2020-05-21 19:21:01.647647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10</span>
<span class="go">2020-05-21 19:21:01.647683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10</span>
<span class="go">2020-05-21 19:21:01.647722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10</span>
<span class="go">2020-05-21 19:21:01.647758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7</span>
<span class="go">2020-05-21 19:21:01.647847: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.648311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.648720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0</span>
<span class="go">2020-05-21 19:21:01.649158: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1</span>
<span class="go">2020-05-21 19:21:01.650302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:</span>
<span class="go">2020-05-21 19:21:01.650362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0</span>
<span class="go">2020-05-21 19:21:01.650392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N</span>
<span class="go">2020-05-21 19:21:01.650860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.651341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero</span>
<span class="go">2020-05-21 19:21:01.651773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7048 MB memory) -&gt; physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:13:00.0, compute capability: 6.1)</span>
<span class="go">2020-05-21 19:21:03.601093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10</span>
<span class="go">[I 19:21:58.132 NotebookApp] Saving file at /tensorflow-tutorials/classification.ipynb</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="uninstall-and-cleanup">
<h3>Uninstall and Cleanup<a class="headerlink" href="#uninstall-and-cleanup" title="Permalink to this headline"></a></h3>
<p>You can remove the <code class="docutils literal notranslate"><span class="pre">tf-notebook</span></code> and service with the following commands:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl delete pod tf-notebook
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl delete svc tf-notebook
</pre></div>
</div>
<p>You can remove the GPU operator with the command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm uninstall <span class="k">$(</span>helm list <span class="p">|</span> grep gpu-operator <span class="p">|</span> awk <span class="s1">&#39;{print $1}&#39;</span><span class="k">)</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">release &quot;gpu-operator-1590086955&quot; uninstalled</span>
</pre></div>
</div>
<p>You can now stop the VM, remove the PCI device, remove the memory reservation, and restart the VM.</p>
<p>You do not need to remove the PCI passthrough device from the host.</p>
</section>
<section id="known-issues">
<h3>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this headline"></a></h3>
<p>This section outlines some known issues with using Google Cloud’s Anthos with NVIDIA GPUs.</p>
<ol class="arabic">
<li><p>Attaching a GPU to a Anthos on-prem worker node requires manually editing the VM from vSphere.
These changes will not survive an Anthos on-prem upgrade process. When the node with the GPU is
deleted as part of the update process, the new VM replacing it will not have the GPU added.
The GPU must be added back to a new VM manually again. While the NVIDIA GPU seems to be able to
handle that event gracefully, the workload backed by the GPU may need to be initiated again manually.</p></li>
<li><p>Attaching a VM to the GPU means that the VM can no longer be migrated to another ESXi host. The VM
will essentially be pinned to the ESXi host which hosts the GPU. vMotion and VMware HA features cannot be used.</p></li>
<li><p>VMs that use a PCI Passthrough device require that their full memory allocation be locked. This will cause a
<strong>Virtual machine memory usage</strong> alarm on the VM which can safely be ignored.</p>
<a class="reference internal image-reference" href="_images/image17.png"><img alt="_images/image17.png" src="_images/image17.png" style="width: 800px;" /></a>
</li>
</ol>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="nvidia-gpu-with-device-edge.html" class="btn btn-neutral float-left" title="Accelerating workloads with NVIDIA GPUs with Red Hat Device Edge" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
<img src="_static/NVIDIA-LogoBlack.svg" class="only-light"/>
<img src="_static/NVIDIA-LogoWhite.svg" class="only-dark"/>
<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>
<p>
  Copyright &#169; 2020-2024, NVIDIA Corporation.
</p>

    <p>
      <span class="lastupdated">Last updated on Oct 31, 2024.
      </span></p>
<script type="text/javascript">if (typeof _satellite !== "undefined"){ _satellite.pageBottom();}</script>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>