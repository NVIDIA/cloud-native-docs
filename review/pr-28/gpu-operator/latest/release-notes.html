<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Release Notes &mdash; NVIDIA GPU Operator 23.9.2 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/api-styles.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/design-tabs.js"></script>
        <script src="_static/version.js"></script>
        <script src="_static/social-media.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Troubleshooting the NVIDIA GPU Operator" href="troubleshooting.html" />
    <link rel="prev" title="Platform Support" href="platform-support.html" />
 


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >


<a href="index.html">
  <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
</a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">About the Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="upgrade.html">Upgrade</a></li>
<li class="toctree-l1"><a class="reference internal" href="uninstall.html">Uninstall</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform-support.html">Platform Support</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-vgpu.html">Using NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Operator configurations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-sharing.html">Time-Slicing GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-outdated-kernels.html">Outdated Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom-driver-params.html">Custom GPU Driver Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="precompiled-drivers.html">Precompiled Driver Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-configuration.html">GPU Driver CRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="cdi.html">Container Device Interface Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Sandboxed Workloads</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kubevirt.html">KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kata.html">Kata Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-confidential-containers.html">Confidential Containers and Kata</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specialized Networks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-proxy.html">HTTP Proxy</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-air-gapped.html">Air-Gapped Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-service-mesh.html">Service Mesh</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CSP configurations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="amazon-eks.html">Amazon EKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="microsoft-aks.html">Azure AKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="google-gke.html">Google GKE</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA GPU Operator</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
<li>
    <a href="https://docs.nvidia.com">NVIDIA Docs Hub</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>
    <a href="https://docs.nvidia.com/datacenter/cloud-native/">NVIDIA Cloud Native Technologies</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>Release Notes</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="release-notes">
<span id="operator-release-notes"></span><h1>Release Notes<a class="headerlink" href="#release-notes" title="Permalink to this headline"></a></h1>
<p>This document describes the new features, improvements, fixed and known issues for the NVIDIA GPU Operator.</p>
<p>See the <a class="reference internal" href="platform-support.html#gpu-operator-component-matrix"><span class="std std-ref">GPU Operator Component Matrix</span></a> for a list of components included in each release.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>GPU Operator beta releases are documented on <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/releases">GitHub</a>. NVIDIA AI Enterprise builds are not posted on GitHub.</p>
</div>
<hr class="docutils" />
<section id="v23-9-2">
<span id="id1"></span><h2>23.9.2<a class="headerlink" href="#v23-9-2" title="Permalink to this headline"></a></h2>
<section id="new-features">
<span id="v23-9-2-new-features"></span><h3>New Features<a class="headerlink" href="#new-features" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Added support for the NVIDIA Data Center GPU Driver version 550.54.14.
Refer to the <a class="reference internal" href="platform-support.html#gpu-operator-component-matrix"><span class="std std-ref">GPU Operator Component Matrix</span></a>
on the platform support page.</p></li>
<li><p>Added support for Kubernetes v1.29.
Refer to <a class="reference internal" href="platform-support.html#supported-operating-systems-and-kubernetes-platforms"><span class="std std-ref">Supported Operating Systems and Kubernetes Platforms</span></a>
on the platform support page.</p></li>
<li><p>Added support for Red Hat OpenShift Container Platform 4.15.
Refer to <a class="reference internal" href="platform-support.html#supported-operating-systems-and-kubernetes-platforms"><span class="std std-ref">Supported Operating Systems and Kubernetes Platforms</span></a>
on the platform support page.</p></li>
<li><p>Added support for the following software component versions:</p>
<blockquote>
<div><ul class="simple">
<li><p>NVIDIA Data Center GPU Driver version 550.54.14</p></li>
<li><p>NVIDIA Container Toolkit version v1.14.6</p></li>
<li><p>NVIDIA Kubernetes Device Plugin version v1.14.5</p></li>
<li><p>NVIDIA MIG Manager version v0.6.0</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</section>
<section id="fixed-issues">
<span id="v23-9-2-fixed-issues"></span><h3>Fixed issues<a class="headerlink" href="#fixed-issues" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Previously, duplicate image pull secrets were added to some daemon sets and caused an error
like the following when a node is deleted and the controller manager deleted the pods.</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">I1031 00:09:44.553742       1 gc_controller.go:329] &quot;PodGC is force deleting Pod&quot; pod=&quot;gpu-operator/nvidia-driver-daemonset-k69f2&quot;</span>
<span class="go">E1031 00:09:44.556500       1 gc_controller.go:255] failed to create manager for existing fields: failed to convert new object (gpu-operator/nvidia-driver-daemonset-k69f2; /v1, Kind=Pod) to smd typed: .spec.imagePullSecrets: duplicate entries for key [name=&quot;ngc-secret&quot;]</span>
</pre></div>
</div>
</li>
<li><p>Previously, common daemon set labels, annotations, and tolerations configured in ClusterPolicy were not
also applied to the default NVIDIADriver CR instance.
Refer to Github <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/665">issue #665</a> for more details.</p></li>
<li><p>Previously, the technical preview NVIDIA driver custom resource was failing to render the <code class="docutils literal notranslate"><span class="pre">licensing-config</span></code>
volume mount that is required for licensing a vGPU guest driver.
Refer to Github <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/672">issue #672</a> for more details.</p></li>
<li><p>Previously, the technical preview NVIDIA driver custom resource was broken when GDS was enabled.
An OS suffix was not appended to the image path of the GDS driver container image.
Refer to GitHub <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/608">issue #608</a> for more details.</p></li>
<li><p>Previously, the technical preview NVIDIA driver custom resource failed to render daemon sets
when <code class="docutils literal notranslate"><span class="pre">additionalConfig</span></code> volumes were configured that were host path volumes. This issue
prevented users from mounting entitlements on RHEL systems.</p></li>
<li><p>Previously, it was not possible to disable the CUDA workload validation pod that the <code class="docutils literal notranslate"><span class="pre">operator-validator</span></code> pod
deploys. You can now disable this pod by setting the following environment variable in ClusterPolicy:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">validator</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">cuda</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">env</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;WITH_WORKLOAD&quot;</span><span class="w"></span>
<span class="w">      </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;false&quot;</span><span class="w"></span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="known-limitations">
<span id="v23-9-2-known-limitations"></span><h3>Known Limitations<a class="headerlink" href="#known-limitations" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">1g.12gb</span></code> MIG profile does not operate as expected on the NVIDIA GH200 GPU when the MIG configuration is set to <code class="docutils literal notranslate"><span class="pre">all-balanced</span></code>.</p></li>
<li><p>The GPU Driver container does not run on hosts that have a custom kernel with the SEV-SNP CPU feature
because of the missing <code class="docutils literal notranslate"><span class="pre">kernel-headers</span></code> package within the container.
With a custom kernel, NVIDIA recommends pre-installing the NVIDIA drivers on the host if you want to
run traditional container workloads with NVIDIA GPUs.</p></li>
<li><p>If you cordon a node while the GPU driver upgrade process is already in progress,
the Operator uncordons the node and upgrades the driver on the node.
You can determine if an upgrade is in progress by checking the node label
<code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu-driver-upgrade-state</span> <span class="pre">!=</span> <span class="pre">upgrade-done</span></code>.</p></li>
<li><p>NVIDIA vGPU is incompatible with KubeVirt v0.58.0, v0.58.1, and v0.59.0, as well
as OpenShift Virtualization 4.12.0—4.12.2.</p></li>
<li><p>Using NVIDIA vGPU on bare metal nodes and NVSwitch is not supported.</p></li>
<li><p>When installing the Operator on Amazon EKS and using Kubernetes versions lower than
<code class="docutils literal notranslate"><span class="pre">1.25</span></code>, specify the <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">psp.enabled=true</span></code> Helm argument because EKS enables
pod security policy (PSP).
If you use Kubernetes version <code class="docutils literal notranslate"><span class="pre">1.25</span></code> or higher, do not specify the <code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code>
argument so that the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>, is used.</p></li>
<li><p>All worker nodes in the Kubernetes cluster must run the same operating system version to use the NVIDIA GPU Driver container.
Alternatively, if you pre-install the NVIDIA GPU Driver on the nodes, then you can run different operating systems.
The technical preview feature that provides <a class="reference internal" href="gpu-driver-configuration.html"><span class="doc">NVIDIA GPU Driver Custom Resource Definition</span></a> is also an alternative.</p></li>
<li><p>NVIDIA GPUDirect Storage (GDS) is not supported with secure boot enabled systems.</p></li>
<li><p>Driver Toolkit images are broken with Red Hat OpenShift version <code class="docutils literal notranslate"><span class="pre">4.11.12</span></code> and require cluster-level entitlements to be enabled
in this case for the driver installation to succeed.</p></li>
<li><p>The NVIDIA GPU Operator can only be used to deploy a single NVIDIA GPU Driver type and version.
The NVIDIA vGPU and Data Center GPU Driver cannot be used within the same cluster.
The technical preview feature that provides <a class="reference internal" href="gpu-driver-configuration.html"><span class="doc">NVIDIA GPU Driver Custom Resource Definition</span></a> is an alternative.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver must be blacklisted when using NVIDIA vGPU.
Otherwise the driver fails to initialize the GPU with the error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">enable</span> <span class="pre">MSI-X</span></code> in the system journal logs.
Additionally, all GPU operator pods become stuck in the <code class="docutils literal notranslate"><span class="pre">Init</span></code> state.</p></li>
<li><p>When using RHEL 8 with containerd as the runtime and SELinux is enabled (either in permissive or enforcing mode)
at the host level, containerd must also be configured for SELinux, such as setting the <code class="docutils literal notranslate"><span class="pre">enable_selinux=true</span></code>
configuration option.
Additionally, network-restricted environments are not supported.</p></li>
</ul>
</section>
</section>
<section id="v23-9-1">
<span id="id2"></span><h2>23.9.1<a class="headerlink" href="#v23-9-1" title="Permalink to this headline"></a></h2>
<section id="v23-9-1-new-features">
<span id="id3"></span><h3>New Features<a class="headerlink" href="#v23-9-1-new-features" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Added support for NVIDIA GH200 Grace Hopper Superchip as a technology preview feature.
Refer to <a class="reference internal" href="platform-support.html#supported-nvidia-gpus-and-systems"><span class="std std-ref">Supported NVIDIA Data Center GPUs and Systems</span></a>.</p>
<p>The following prerequisites are required for using the Operator with GH200:</p>
<ul class="simple">
<li><p>Run Ubuntu 22.04 and an NVIDIA Linux kernel, such as one provided with a <code class="docutils literal notranslate"><span class="pre">linux-nvidia-&lt;x.x&gt;</span></code> package.</p></li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">init_on_alloc=0</span></code> and <code class="docutils literal notranslate"><span class="pre">memhp_default_state=online_movable</span></code> as Linux kernel boot parameters.</p></li>
<li><p>Run the NVIDIA Open GPU Kernel module driver.</p></li>
</ul>
</li>
<li><p>Added support for configuring the driver container to use the NVIDIA Open GPU Kernel module driver.
Support is limited to installation using the runfile installer.
Support for precompiled driver containers with open kernel modules is not available.</p>
<p>For clusters that use GPUDirect Storage (GDS), beginning with CUDA toolkit 12.2.2 and
the NVIDIA GPUDirect Storage kernel driver version v2.17.5, are only supported
with the open kernel modules.</p>
<p>NVIDIA GH200 Grace Hopper Superchip systems are only supported with the open kernel modules.</p>
<ul class="simple">
<li><p>Refer to <a class="reference internal" href="getting-started.html#gpu-operator-helm-chart-options"><span class="std std-ref">Chart Customization Options</span></a> for information about setting
<code class="docutils literal notranslate"><span class="pre">useOpenKernelModules</span></code> if you manage the driver containers with the NVIDIA cluster policy custom resource definition.</p></li>
<li><p>Refer to <a class="reference internal" href="gpu-driver-configuration.html"><span class="doc">NVIDIA GPU Driver Custom Resource Definition</span></a> for information about setting <code class="docutils literal notranslate"><span class="pre">spec.useOpenKernelModules</span></code>
if you manage the driver containers with the technology preview NVIDIA driver custom resource.</p></li>
</ul>
</li>
<li><p>Added support for the following software component versions:</p>
<ul>
<li><p>NVIDIA Data Center GPU Driver version 535.129.03</p></li>
<li><p>NVIDIA Driver Manager for Kubernetes v0.6.5</p></li>
<li><p>NVIDIA Kubernetes Device Plugin v1.14.3</p></li>
<li><p>NVIDIA DCGM Exporter 3.3.0-3.2.0</p></li>
<li><p>NVIDIA Data Center GPU Manager (DCGM) v3.3.0-1</p></li>
<li><p>NVIDIA KubeVirt GPU Device Plugin v1.2.4</p></li>
<li><p>NVIDIA GPUDirect Storage (GDS) Driver v2.17.5</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>This version, and newer versions of the NVIDIA GDS kernel driver, require that you use the NVIDIA open kernel modules.</p>
</div>
</li>
</ul>
<p>Refer to the <a class="reference internal" href="platform-support.html#gpu-operator-component-matrix"><span class="std std-ref">GPU Operator Component Matrix</span></a>
on the platform support page.</p>
</li>
<li><p>Added support for NVIDIA Network Operator v23.10.0.</p></li>
</ul>
</section>
<section id="improvements">
<span id="v23-9-1-improvements"></span><h3>Improvements<a class="headerlink" href="#improvements" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">must-gather.sh</span></code> script that is used for support is enhanced to collect logs
from NVIDIA vGPU Manager pods.</p></li>
</ul>
</section>
<section id="v23-9-1-fixed-issues">
<span id="id4"></span><h3>Fixed issues<a class="headerlink" href="#v23-9-1-fixed-issues" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Previously, the technical preview NVIDIA driver custom resource did not support adding
custom labels, annotations, or tolerations to the pods that run as part of the driver daemon set.
This limitation prevented scheduling the driver daemon set in some environments.
Refer to GitHub <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/602">issue #602</a> for more details.</p></li>
<li><p>Previously, when you specified the <code class="docutils literal notranslate"><span class="pre">operator.upgradeCRD=true</span></code> argument to the <code class="docutils literal notranslate"><span class="pre">helm</span> <span class="pre">upgrade</span></code>
command, the pre-upgrade hook ran with the <code class="docutils literal notranslate"><span class="pre">gpu-operator</span></code> service account
that is added by running <code class="docutils literal notranslate"><span class="pre">helm</span> <span class="pre">install</span></code>.
This dependency is a known issue for Argo CD users.
Argo CD treats pre-install and pre-upgrade hooks the same as pre-sync hooks and leads to failures
because the hook depends on the <code class="docutils literal notranslate"><span class="pre">gpu-operator</span></code> service account that does not exist on an initial installation.</p>
<p>Now, the Operator is enhanced to run the hook with a new service account, <code class="docutils literal notranslate"><span class="pre">gpu-operator-upgrade-crd-hook-sa</span></code>.
This fix creates the new service account, a new cluster role, and a new cluster role binding.
The update prevents failures with Argo CD.</p>
</li>
<li><p>Previously, adding an NVIDIA driver custom resource with a node selector that conflicts with another
driver custom resource, the controller failed to set the error condition in the custom resource status.
The issue produced an error message like the following example:</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">{&quot;level&quot;:&quot;error&quot;,&quot;ts&quot;:1698702848.8472972,&quot;msg&quot;:&quot;NVIDIADriver.nvidia.com \&quot;&lt;conflicting-cr-name&gt;&quot;\&quot; is invalid: state: Unsupported value: \&quot;\&quot;: supported values: \&quot;ignored\&quot;, \&quot;ready\&quot;, \&quot;notReady\&quot;&quot;,&quot;controller&quot;:&quot;nvidia-driver-\</span>
<span class="go">controller&quot;,&quot;object&quot;:{&quot;name&quot;:&quot;&lt;conflicting-cr-name&gt;&quot;},&quot;namespace&quot;:&quot;&quot;,&quot;name&quot;:&quot;&lt;conflicting-cr-name&gt;&quot;,&quot;reconcileID&quot;:&quot;78d58d7b-cd94-4849-a292-391da9a0b049&quot;}</span>
</pre></div>
</div>
</li>
<li><p>Previously, the NVIDIA KubeVirt GPU Device Plugin could have a GLIBC mismatch error and produce a log
message like the following example:</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">nvidia-kubevirt-gpu-device-plugin: /lib64/libc.so.6: version `GLIBC_2.32` not found (required by nvidia-kubevirt-gpu-device-plugin)</span>
</pre></div>
</div>
<p>This issue is fixed by including v1.2.4 of the plugin in this release.</p>
</li>
<li><p>Previously, on some machines and Linux kernel versions, GPU Feature Discovery was unable to determine
the machine type because the <code class="docutils literal notranslate"><span class="pre">/sys/class/dmi/id/product_name</span></code> file did not exist on the host.
Now, accessing the file is performed by mounting <code class="docutils literal notranslate"><span class="pre">/sys</span></code> instead of the fully-qualified path and
if the file does not exist, GPU Feature Discovery is able to label the node with <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.machine=unknown</span></code>.</p></li>
<li><p>Previously, enabling GPUDirect RDMA on Red Hat OpenShift Container Platform clusters could
experience an error with the nvidia-peermem container.
The error was related to the <code class="docutils literal notranslate"><span class="pre">RHEL_VERSION</span></code> variable being unbound.</p></li>
</ul>
</section>
<section id="v23-9-1-known-limitations">
<span id="id5"></span><h3>Known Limitations<a class="headerlink" href="#v23-9-1-known-limitations" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">1g.12gb</span></code> MIG profile does not operate as expected on the NVIDIA GH200 GPU when the MIG configuration is set to <code class="docutils literal notranslate"><span class="pre">all-balanced</span></code>.</p></li>
<li><p>The GPU Driver container does not run on hosts that have a custom kernel with the SEV-SNP CPU feature
because of the missing <code class="docutils literal notranslate"><span class="pre">kernel-headers</span></code> package within the container.
With a custom kernel, NVIDIA recommends pre-installing the NVIDIA drivers on the host if you want to
run traditional container workloads with NVIDIA GPUs.</p></li>
<li><p>If you cordon a node while the GPU driver upgrade process is already in progress,
the Operator uncordons the node and upgrades the driver on the node.
You can determine if an upgrade is in progress by checking the node label
<code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu-driver-upgrade-state</span> <span class="pre">!=</span> <span class="pre">upgrade-done</span></code>.</p></li>
<li><p>NVIDIA vGPU is incompatible with KubeVirt v0.58.0, v0.58.1, and v0.59.0, as well
as OpenShift Virtualization 4.12.0—4.12.2.</p></li>
<li><p>Using NVIDIA vGPU on bare metal nodes and NVSwitch is not supported.</p></li>
<li><p>When installing the Operator on Amazon EKS and using Kubernetes versions lower than
<code class="docutils literal notranslate"><span class="pre">1.25</span></code>, specify the <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">psp.enabled=true</span></code> Helm argument because EKS enables
pod security policy (PSP).
If you use Kubernetes version <code class="docutils literal notranslate"><span class="pre">1.25</span></code> or higher, do not specify the <code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code>
argument so that the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>, is used.</p></li>
<li><p>All worker nodes in the Kubernetes cluster must run the same operating system version to use the NVIDIA GPU Driver container.
Alternatively, if you pre-install the NVIDIA GPU Driver on the nodes, then you can run different operating systems.
The technical preview feature that provides <a class="reference internal" href="gpu-driver-configuration.html"><span class="doc">NVIDIA GPU Driver Custom Resource Definition</span></a> is also an alternative.</p></li>
<li><p>NVIDIA GPUDirect Storage (GDS) is not supported with secure boot enabled systems.</p></li>
<li><p>Driver Toolkit images are broken with Red Hat OpenShift version <code class="docutils literal notranslate"><span class="pre">4.11.12</span></code> and require cluster-level entitlements to be enabled
in this case for the driver installation to succeed.</p></li>
<li><p>The NVIDIA GPU Operator can only be used to deploy a single NVIDIA GPU Driver type and version.
The NVIDIA vGPU and Data Center GPU Driver cannot be used within the same cluster.
The technical preview feature that provides <a class="reference internal" href="gpu-driver-configuration.html"><span class="doc">NVIDIA GPU Driver Custom Resource Definition</span></a> is an alternative.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver must be blacklisted when using NVIDIA vGPU.
Otherwise the driver fails to initialize the GPU with the error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">enable</span> <span class="pre">MSI-X</span></code> in the system journal logs.
Additionally, all GPU operator pods become stuck in the <code class="docutils literal notranslate"><span class="pre">Init</span></code> state.</p></li>
<li><p>When using RHEL 8 with containerd as the runtime and SELinux is enabled (either in permissive or enforcing mode)
at the host level, containerd must also be configured for SELinux, such as setting the <code class="docutils literal notranslate"><span class="pre">enable_selinux=true</span></code>
configuration option.
Additionally, network-restricted environments are not supported.</p></li>
</ul>
</section>
</section>
<section id="id6">
<h2>23.9.0<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h2>
<section id="id7">
<h3>New Features<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Added support for an NVIDIA driver custom resource definition that enables
running multiple GPU driver types and versions on the same cluster and adds
support for multiple operating system versions.
This feature is a technology preview.
Refer to <a class="reference internal" href="gpu-driver-configuration.html"><span class="doc">NVIDIA GPU Driver Custom Resource Definition</span></a> for more information.</p></li>
<li><p>Added support for additional Linux kernel variants for precompiled driver containers.</p>
<ul class="simple">
<li><p>driver:535-5.15.0-xxxx-nvidia-ubuntu22.04</p></li>
<li><p>driver:535-5.15.0-xxxx-azure-ubuntu22.04</p></li>
<li><p>driver:535-5.15.0-xxxx-aws-ubuntu22.04</p></li>
</ul>
<p>Refer to the <strong>Tags</strong> tab of the <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/driver">NVIDIA GPU Driver</a>
page in the NGC catalog to determine if a container for your kernel is built.
Refer to <a class="reference internal" href="precompiled-drivers.html"><span class="doc">Precompiled Driver Containers</span></a> for information about using precompiled driver containers
and steps to build your own driver container.</p>
</li>
<li><p>The API for the NVIDIA cluster policy custom resource definition is enhanced to include
the current state of the cluster policy.
When you view the cluster policy with a command like <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">get</span> <span class="pre">cluster-policy</span></code>, the response
now includes a <code class="docutils literal notranslate"><span class="pre">Status.Conditions</span></code> field.</p></li>
<li><p>Added support for the following software component versions:</p>
<ul class="simple">
<li><p>NVIDIA Data Center GPU Driver version 535.104.12.</p></li>
<li><p>NVIDIA Driver Manager for Kubernetes v0.6.4</p></li>
<li><p>NVIDIA Container Toolkit v1.14.3</p></li>
<li><p>NVIDIA Kubernetes Device Plugin v1.14.2</p></li>
<li><p>NVIDIA DCGM Exporter 3.2.6-3.1.9</p></li>
<li><p>NVIDIA GPU Feature Discovery for Kubernetes v0.8.2</p></li>
<li><p>NVIDIA MIG Manager for Kubernetes v0.5.5</p></li>
<li><p>NVIDIA Data Center GPU Manager (DCGM) v3.2.6-1</p></li>
<li><p>NVIDIA KubeVirt GPU Device Plugin v1.2.3</p></li>
<li><p>NVIDIA vGPU Device Manager v0.2.4</p></li>
<li><p>NVIDIA Kata Manager for Kubernetes v0.1.2</p></li>
<li><p>NVIDIA Confidential Computing Manager for Kubernetes v0.1.1</p></li>
<li><p>Node Feature Discovery v0.14.2</p></li>
</ul>
<p>Refer to the <a class="reference internal" href="platform-support.html#gpu-operator-component-matrix"><span class="std std-ref">GPU Operator Component Matrix</span></a>
on the platform support page.</p>
</li>
</ul>
</section>
<section id="id8">
<h3>Fixed issues<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Previously, if the <code class="docutils literal notranslate"><span class="pre">RHEL_VERSION</span></code> environment variable was set for the Operator, the variable was
propagated to the driver container and used in the <code class="docutils literal notranslate"><span class="pre">--releasever</span></code> argument to the <code class="docutils literal notranslate"><span class="pre">dnf</span></code> command.
With this release, you can specify the <code class="docutils literal notranslate"><span class="pre">DNF_RELEASEVER</span></code> environment variable for the driver container
to override the value of the <code class="docutils literal notranslate"><span class="pre">--releasever</span></code> argument.</p></li>
<li><p>Previously, stale node feature and node feature topology objects could remain in the Kubernetes API
server after a node is deleted from the cluster.
The upgrade to Node Feature Discovery v0.14.2 includes an enhancement to garbage collection that
ensures the objects are removed after a node is deleted.</p></li>
</ul>
</section>
<section id="id9">
<h3>Known Limitations<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The GPU Driver container does not run on hosts that have a custom kernel with the SEV-SNP CPU feature
because of the missing <code class="docutils literal notranslate"><span class="pre">kernel-headers</span></code> package within the container.
With a custom kernel, NVIDIA recommends pre-installing the NVIDIA drivers on the host if you want to
run traditional container workloads with NVIDIA GPUs.</p></li>
<li><p>If you cordon a node while the GPU driver upgrade process is already in progress,
the Operator uncordons the node and upgrades the driver on the node.
You can determine if an upgrade is in progress by checking the node label
<code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu-driver-upgrade-state</span> <span class="pre">!=</span> <span class="pre">upgrade-done</span></code>.</p></li>
<li><p>NVIDIA vGPU is incompatible with KubeVirt v0.58.0, v0.58.1, and v0.59.0, as well
as OpenShift Virtualization 4.12.0—4.12.2.</p></li>
<li><p>Using NVIDIA vGPU on bare metal nodes and NVSwitch is not supported.</p></li>
<li><p>When installing the Operator on Amazon EKS and using Kubernetes versions lower than
<code class="docutils literal notranslate"><span class="pre">1.25</span></code>, specify the <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">psp.enabled=true</span></code> Helm argument because EKS enables
pod security policy (PSP).
If you use Kubernetes version <code class="docutils literal notranslate"><span class="pre">1.25</span></code> or higher, do not specify the <code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code>
argument so that the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>, is used.</p></li>
<li><p>All worker nodes in the Kubernetes cluster must run the same operating system version to use the NVIDIA GPU Driver container.
Alternatively, if you pre-install the NVIDIA GPU Driver on the nodes, then you can run different operating systems.
The technical preview feature that provides <a class="reference internal" href="gpu-driver-configuration.html"><span class="doc">NVIDIA GPU Driver Custom Resource Definition</span></a> is also an alternative.</p></li>
<li><p>NVIDIA GPUDirect Storage (GDS) is not supported with secure boot enabled systems.</p></li>
<li><p>Driver Toolkit images are broken with Red Hat OpenShift version <code class="docutils literal notranslate"><span class="pre">4.11.12</span></code> and require cluster-level entitlements to be enabled
in this case for the driver installation to succeed.</p></li>
<li><p>The NVIDIA GPU Operator can only be used to deploy a single NVIDIA GPU Driver type and version.
The NVIDIA vGPU and Data Center GPU Driver cannot be used within the same cluster.
The technical preview feature that provides <a class="reference internal" href="gpu-driver-configuration.html"><span class="doc">NVIDIA GPU Driver Custom Resource Definition</span></a> is an alternative.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver must be blacklisted when using NVIDIA vGPU.
Otherwise the driver fails to initialize the GPU with the error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">enable</span> <span class="pre">MSI-X</span></code> in the system journal logs.
Additionally, all GPU operator pods become stuck in the <code class="docutils literal notranslate"><span class="pre">Init</span></code> state.</p></li>
<li><p>When using RHEL 8 with containerd as the runtime and SELinux is enabled (either in permissive or enforcing mode)
at the host level, containerd must also be configured for SELinux, such as setting the <code class="docutils literal notranslate"><span class="pre">enable_selinux=true</span></code>
configuration option.
Additionally, network-restricted environments are not supported.</p></li>
</ul>
</section>
</section>
<section id="v23-6-2">
<span id="id10"></span><h2>23.6.2<a class="headerlink" href="#v23-6-2" title="Permalink to this headline"></a></h2>
<p>This patch release back ports a fix that was introduced in the v23.9.1 release.</p>
<section id="v23-6-2-fixed-issues">
<span id="id11"></span><h3>Fixed Issues<a class="headerlink" href="#v23-6-2-fixed-issues" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Previously, when you specified the <code class="docutils literal notranslate"><span class="pre">operator.upgradeCRD=true</span></code> argument to the <code class="docutils literal notranslate"><span class="pre">helm</span> <span class="pre">upgrade</span></code>
command, the pre-upgrade hook ran with the <code class="docutils literal notranslate"><span class="pre">gpu-operator</span></code> service account
that is added by running <code class="docutils literal notranslate"><span class="pre">helm</span> <span class="pre">install</span></code>.
This dependency is a known issue for Argo CD users.
Argo CD treats pre-install and pre-upgrade hooks the same as pre-sync hooks and leads to failures
because the hook depends on the <code class="docutils literal notranslate"><span class="pre">gpu-operator</span></code> service account that does not exist on an initial installation.</p>
<p>Now, the Operator is enhanced to run the hook with a new service account, <code class="docutils literal notranslate"><span class="pre">gpu-operator-upgrade-crd-hook-sa</span></code>.
This fix creates the new service account, a new cluster role, and a new cluster role binding.
The update prevents failures with Argo CD.</p>
</li>
</ul>
</section>
</section>
<section id="id12">
<h2>23.6.1<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h2>
<section id="id13">
<h3>New Features<a class="headerlink" href="#id13" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added support for NVIDIA L40S GPUs.</p></li>
<li><p>Added support for the NVIDIA Data Center GPU Driver version 535.104.05.
Refer to the <a class="reference internal" href="platform-support.html#gpu-operator-component-matrix"><span class="std std-ref">GPU Operator Component Matrix</span></a>
on the platform support page.</p></li>
</ul>
</section>
<section id="id14">
<h3>Fixed issues<a class="headerlink" href="#id14" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Previously, the NVIDIA Container Toolkit daemon set could fail when running on
nodes with certain types of GPUs.
The driver-validation init container would fail when iterating over NVIDIA PCI devices
if the device PCI ID was not in the PCI database.
The error message is similar to the following example:</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Error: error validating driver installation: error creating symlinks:</span>
<span class="go">failed to get device nodes: failed to get GPU information: error getting</span>
<span class="go">all NVIDIA devices: error constructing NVIDIA PCI device 0000:21:00.0:</span>
<span class="go">unable to get device name: failed to find device with id &#39;26b9&#39;\n\n</span>
<span class="go">Failed to create symlinks under /dev/char that point to all possible NVIDIA</span>
<span class="go">character devices.\nThe existence of these symlinks is required to address</span>
<span class="go">the following bug:\n\n    https://github.com/NVIDIA/gpu-operator/issues/430\n\n</span>
<span class="go">This bug impacts container runtimes configured with systemd cgroup management</span>
<span class="go">enabled.\nTo disable the symlink creation, set the following envvar in ClusterPolicy:\n\n</span>
<span class="go">validator:\n    driver:\n     env:\n  - name: DISABLE_DEV_CHAR_SYMLINK_CREATION\n value: \&quot;true\&quot;&quot;</span>
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="id15">
<h2>23.6.0<a class="headerlink" href="#id15" title="Permalink to this headline"></a></h2>
<section id="id16">
<h3>New Features<a class="headerlink" href="#id16" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added support for configuring Kata Containers for GPU workloads as a technology preview feature.
This feature introduces NVIDIA Kata Manager for Kubernetes as an operand of GPU Operator.
Refer to <a class="reference internal" href="gpu-operator-kata.html"><span class="doc">GPU Operator with Kata Containers</span></a> for more information.</p></li>
<li><p>Added support for configuring Confidential Containers for GPU workloads as a technology preview feature.
This feature builds on the work for configuring Kata Containers and
introduces NVIDIA Confidential Computing Manager for Kubernetes as an operand of GPU Operator.
Refer to <a class="reference internal" href="gpu-operator-confidential-containers.html"><span class="doc">GPU Operator with Confidential Containers and Kata</span></a> for more information.</p></li>
<li><p>Added support for the NVIDIA Data Center GPU Driver version 535.86.10.
Refer to the <a class="reference internal" href="platform-support.html#gpu-operator-component-matrix"><span class="std std-ref">GPU Operator Component Matrix</span></a>
on the platform support page.</p></li>
<li><p>Added support for NVIDIA vGPU 16.0.</p></li>
<li><p>Added support for NVIDIA Network Operator 23.7.0.</p></li>
<li><p>Added support for new MIG profiles with the 535 driver.</p>
<ul>
<li><p>For H100 NVL and H800 NVL devices:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">1g.12gb.me</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">1g.24gb</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">2g.24gb</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">3g.47gb</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">4g.47gb</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">7g.94gb</span></code></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="id17">
<h3>Improvements<a class="headerlink" href="#id17" title="Permalink to this headline"></a></h3>
<ul>
<li><p>The Operator is updated to use the <code class="docutils literal notranslate"><span class="pre">node-role.kubernetes.io/control-plane</span></code> label
that is the default label for Kubernetes version 1.27.
As a fallback for older Kubernetes versions, the Operator runs on nodes with the
<code class="docutils literal notranslate"><span class="pre">master</span></code> label if the <code class="docutils literal notranslate"><span class="pre">control-plane</span></code> label is not available.</p></li>
<li><p>Added support for setting Pod Security Admission for the GPU Operator namespace.
Pod Security Admission applies to Kubernetes versions 1.25 and higher.
You can specify <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">psa.enabled=true</span></code> when you install or upgrade the Operator,
or you can patch the <code class="docutils literal notranslate"><span class="pre">cluster-policy</span></code> instance of the <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code> object.
The Operator sets the following standards:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">pod-security.kubernetes.io/audit=privileged</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">pod-security.kubernetes.io/enforce=privileged</span><span class="w"></span>
<span class="l l-Scalar l-Scalar-Plain">pod-security.kubernetes.io/warn=privileged</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>The Operator performs plugin validation when the Operator is installed or upgraded.
Previously, the plugin validation ran a workload pod that requires access to a GPU.
On a busy node with the GPUs consumed by other workloads, the validation can falsely
report failure because it was not scheduled.
The plugin validation still confirms that GPUs are advertised to kubelet, but it no longer
runs a workload.
To override the new behavior and run a plugin validation workload, specify
<code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">validator.plugin.env.WITH_WORKLOAD=true</span></code> when you install or upgrade the Operator.</p></li>
</ul>
</section>
<section id="id18">
<h3>Fixed issues<a class="headerlink" href="#id18" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>In clusters that use a network proxy and configure GPU Direct Storage, the <code class="docutils literal notranslate"><span class="pre">nvidia-fs-ctr</span></code>
container can use the network proxy and any other environment variable that you specify
with the <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">gds.env=key1=val1,key2=val2</span></code> option when you install or upgrade the Operator.</p></li>
<li><p>In previous releases, when you performed a GPU driver upgrade with the <code class="docutils literal notranslate"><span class="pre">OnDelete</span></code> strategy,
the status reported in the <code class="docutils literal notranslate"><span class="pre">cluster-policy</span></code> instance of the <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code> object could indicate
<code class="docutils literal notranslate"><span class="pre">Ready</span></code> even though the driver daemon set has not completed the upgrade of pods on all nodes.
In this release, the status is reported as <code class="docutils literal notranslate"><span class="pre">notReady</span></code> until the upgrade is complete.</p></li>
</ul>
</section>
<section id="id19">
<h3>Known Limitations<a class="headerlink" href="#id19" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The GPU Driver container does not run on hosts that have a custom kernel with the SEV-SNP CPU feature
because of the missing <code class="docutils literal notranslate"><span class="pre">kernel-headers</span></code> package within the container.
With a custom kernel, NVIDIA recommends pre-installing the NVIDIA drivers on the host if you want to
run traditional container workloads with NVIDIA GPUs.</p></li>
<li><p>If you cordon a node while the GPU driver upgrade process is already in progress,
the Operator uncordons the node and upgrades the driver on the node.
You can determine if an upgrade is in progress by checking the node label
<code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu-driver-upgrade-state</span> <span class="pre">!=</span> <span class="pre">upgrade-done</span></code>.</p></li>
<li><p>NVIDIA vGPU is incompatible with KubeVirt v0.58.0, v0.58.1, and v0.59.0, as well
as OpenShift Virtualization 4.12.0—4.12.2.</p></li>
<li><p>Using NVIDIA vGPU on bare metal nodes and NVSwitch is not supported.</p></li>
<li><p>When installing the Operator on Amazon EKS and using Kubernetes versions lower than
<code class="docutils literal notranslate"><span class="pre">1.25</span></code>, specify the <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">psp.enabled=true</span></code> Helm argument because EKS enables
pod security policy (PSP).
If you use Kubernetes version <code class="docutils literal notranslate"><span class="pre">1.25</span></code> or higher, do not specify the <code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code>
argument so that the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>, is used.</p></li>
<li><dl class="simple">
<dt>All worker nodes in the Kubernetes cluster must run the same operating system version to use the NVIDIA GPU Driver container.</dt><dd><p>Alternatively, if you pre-install the NVIDIA GPU Driver on the nodes, then you can run different operating systems.</p>
</dd>
</dl>
</li>
<li><p>NVIDIA GPUDirect Storage (GDS) is not supported with secure boot enabled systems.</p></li>
<li><p>Driver Toolkit images are broken with Red Hat OpenShift version <code class="docutils literal notranslate"><span class="pre">4.11.12</span></code> and require cluster-level entitlements to be enabled
in this case for the driver installation to succeed.</p></li>
<li><p>The NVIDIA GPU Operator can only be used to deploy a single NVIDIA GPU Driver type and version. The NVIDIA vGPU and Data Center GPU Driver cannot be used within the same cluster.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver must be blacklisted when using NVIDIA vGPU.
Otherwise the driver fails to initialize the GPU with the error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">enable</span> <span class="pre">MSI-X</span></code> in the system journal logs.
Additionally, all GPU operator pods become stuck in the <code class="docutils literal notranslate"><span class="pre">Init</span></code> state.</p></li>
<li><p>When using RHEL 8 with Kubernetes, SELinux must be enabled (either in permissive or enforcing mode) for use with the GPU Operator.
Additionally, network-restricted environments are not supported.</p></li>
</ul>
</section>
</section>
<section id="id20">
<h2>23.3.2<a class="headerlink" href="#id20" title="Permalink to this headline"></a></h2>
<section id="id21">
<h3>New Features<a class="headerlink" href="#id21" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added support for Kubernetes v1.27.
Refer to <a class="reference internal" href="platform-support.html#supported-operating-systems-and-kubernetes-platforms"><span class="std std-ref">Supported Operating Systems and Kubernetes Platforms</span></a>
on the platform support page.</p></li>
<li><p>Added support for Red Hat OpenShift Container Platform 4.13.
Refer to <a class="reference internal" href="platform-support.html#supported-operating-systems-and-kubernetes-platforms"><span class="std std-ref">Supported Operating Systems and Kubernetes Platforms</span></a>
on the platform support page.</p></li>
<li><p>Added support for KubeVirt v0.59 and Red Hat OpenShift Virtualization 4.13.
Starting with KubeVirt versions v0.58.2 and v0.59.1 and OpenShift Virtualization 4.12.3 and 4.13.0,
you must set the <code class="docutils literal notranslate"><span class="pre">DisableMDEVConfiguration</span></code> feature gate to use NVIDIA vGPU.
Refer to <a class="reference internal" href="gpu-operator-kubevirt.html#gpu-operator-with-kubevirt"><span class="std std-ref">GPU Operator with KubeVirt</span></a> or
<a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/openshift-virtualization.html#nvidia-gpu-operator-with-openshift-virtualization" title="(in NVIDIA GPU Operator on Red Hat OpenShift Container Platform)"><span>NVIDIA GPU Operator with OpenShift Virtualization</span></a>.</p></li>
<li><p>Added support for running the Operator with Microsoft Azure Kubernetes Service (AKS).
You must use an AKS image with a preinstalled NVIDIA GPU driver and a preinstalled
NVIDIA Container Toolkit.
Refer to <a class="reference internal" href="microsoft-aks.html"><span class="doc">NVIDIA GPU Operator with Azure Kubernetes Service</span></a> for more information.</p></li>
<li><p>Added support for VMWare vSphere 8.0 U1 with Tanzu.</p></li>
<li><p>Added support for CRI-0 v1.26 with Red Hat Enterprise Linux 8.7
and support for CRI-0 v1.27 with Ubuntu 20.04.</p></li>
</ul>
</section>
<section id="id22">
<h3>Improvements<a class="headerlink" href="#id22" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Increased the default timeout for the <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> command that is used by the
NVIDIA Driver Container startup probe and made the timeout configurable.
Previously, the timeout duration for the startup probe was <code class="docutils literal notranslate"><span class="pre">30s</span></code>.
In this release, the default duration is <code class="docutils literal notranslate"><span class="pre">60s</span></code>.
This change reduces the frequency of container restarts when <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>
runs slowly.
Refer to <a class="reference internal" href="getting-started.html#chart-customization-options"><span class="std std-ref">Chart Customization Options</span></a> for more information.</p></li>
</ul>
</section>
<section id="id23">
<h3>Fixed issues<a class="headerlink" href="#id23" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Fixed an issue with NVIDIA GPU Direct Storage (GDS) and Ubuntu 22.04.
The Operator was not able to deploy GDS and other daemon sets.</p>
<p>Previously, the Operator produced the following error log:</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">{&quot;level&quot;:&quot;error&quot;,&quot;ts&quot;:1681889507.829097,&quot;msg&quot;:&quot;Reconciler error&quot;,&quot;controller&quot;:&quot;clusterpolicy-controller&quot;,&quot;object&quot;:{&quot;name&quot;:&quot;cluster-policy&quot;},&quot;namespace&quot;:&quot;&quot;,&quot;name&quot;:&quot;cluster-policy&quot;,&quot;reconcileID&quot;:&quot;c5d55183-3ce9-4376-9d20-e3d53dc441cb&quot;,&quot;error&quot;:&quot;ERROR: failed to transform the Driver Toolkit Container: could not find the &#39;openshift-driver-toolkit-ctr&#39; container&quot;}</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="id24">
<h3>Known Limitations<a class="headerlink" href="#id24" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>If you cordon a node while the GPU driver upgrade process is already in progress,
the Operator uncordons the node and upgrades the driver on the node.
You can determine if an upgrade is in progress by checking the node label
<code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu-driver-upgrade-state</span> <span class="pre">!=</span> <span class="pre">upgrade-done</span></code>.</p></li>
<li><p>NVIDIA vGPU is incompatible with KubeVirt v0.58.0, v0.58.1, and v0.59.0, as well
as OpenShift Virtualization 4.12.0—4.12.2.</p></li>
<li><p>Using NVIDIA vGPU on bare metal nodes and NVSwitch is not supported.</p></li>
<li><p>When installing the Operator on Amazon EKS and using Kubernetes versions lower than
<code class="docutils literal notranslate"><span class="pre">1.25</span></code>, specify the <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">psp.enabled=true</span></code> Helm argument because EKS enables
pod security policy (PSP).
If you use Kubernetes version <code class="docutils literal notranslate"><span class="pre">1.25</span></code> or higher, do not specify the <code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code>
argument so that the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>, is used.</p></li>
<li><p>Ubuntu 18.04 is scheduled to reach end of standard support in May of 2023.
When Ubuntu transitions it to end of life (EOL), the NVIDIA GPU Operator and
related projects plan to cease building containers for 18.04 and to
cease providing support.</p></li>
<li><p>All worker nodes within the Kubernetes cluster must use the same operating system version.</p></li>
<li><p>NVIDIA GPUDirect Storage (GDS) is not supported with secure boot enabled systems.</p></li>
<li><p>Driver Toolkit images are broken with Red Hat OpenShift version <code class="docutils literal notranslate"><span class="pre">4.11.12</span></code> and require cluster-level entitlements to be enabled
in this case for the driver installation to succeed.</p></li>
<li><p>The NVIDIA GPU Operator can only be used to deploy a single NVIDIA GPU Driver type and version. The NVIDIA vGPU and Data Center GPU Driver cannot be used within the same cluster.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver must be blacklisted when using NVIDIA vGPU.
Otherwise the driver fails to initialize the GPU with the error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">enable</span> <span class="pre">MSI-X</span></code> in the system journal logs.
Additionally, all GPU operator pods become stuck in the <code class="docutils literal notranslate"><span class="pre">Init</span></code> state.</p></li>
<li><p>When using RHEL 8 with Kubernetes, SELinux must be enabled (either in permissive or enforcing mode) for use with the GPU Operator.
Additionally, network-restricted environments are not supported.</p></li>
</ul>
</section>
</section>
<section id="id25">
<h2>23.3.1<a class="headerlink" href="#id25" title="Permalink to this headline"></a></h2>
<p>This release provides a packaging-only update to the 23.3.0 release to fix installation on Red Hat OpenShift Container Platform. Refer to GitHub <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/513">issue #513</a>.</p>
</section>
<section id="id26">
<h2>23.3.0<a class="headerlink" href="#id26" title="Permalink to this headline"></a></h2>
<section id="id27">
<h3>New Features<a class="headerlink" href="#id27" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added support for the NVIDIA Data Center GPU Driver version 525.105.17.
Refer to the <a class="reference internal" href="platform-support.html#gpu-operator-component-matrix"><span class="std std-ref">GPU Operator Component Matrix</span></a>
on the platform support page.</p></li>
<li><p>Added support for GPUDirect Storage with Red Hat OpenShift Container Platform 4.11.
Refer to <a class="reference internal" href="platform-support.html#support-for-gpudirect-storage"><span class="std std-ref">Support for GPUDirect Storage</span></a> on the platform support page.</p></li>
<li><p>Added support for Canonical MicroK8s v1.26.
Refer to <a class="reference internal" href="platform-support.html#supported-operating-systems-and-kubernetes-platforms"><span class="std std-ref">Supported Operating Systems and Kubernetes Platforms</span></a>
on the platform support page.</p></li>
<li><p>Added support for containerd v1.7.
Refer to <a class="reference internal" href="platform-support.html#supported-container-runtimes"><span class="std std-ref">Supported Container Runtimes</span></a>
on the platform support page.</p></li>
<li><p>Added support for Node Feature Discovery v0.12.1.
This release adds support for using the NodeFeature API CRD for labelling nodes
instead of labelling nodes over gRPC.
The <a class="reference internal" href="upgrade.html#operator-upgrades"><span class="std std-ref">documentation for upgrading the Operator manually</span></a>
is updated to include applying the custom resource definitions for Node Feature Discovery.</p></li>
<li><p>Added support for running the NVIDIA GPU Operator in <a class="reference internal" href="amazon-eks.html"><span class="doc">Amazon EKS</span></a>
and <a class="reference internal" href="google-gke.html"><span class="doc">Google GKE</span></a>.
You must configure the cluster with custom nodes that run a supported operating
system, such as Ubuntu 22.04.</p></li>
<li><p>Added support for the Container Device Interface (CDI) that is implemented by the
NVIDIA Container Toolkit v1.13.0.
Refer to <a class="reference internal" href="getting-started.html#gpu-operator-helm-chart-options"><span class="std std-ref">Chart Customization Options</span></a> for information about the <code class="docutils literal notranslate"><span class="pre">cdi.enable</span></code> and
<code class="docutils literal notranslate"><span class="pre">cdi.default</span></code> options to enable CDI during installation
or <a class="reference internal" href="cdi.html"><span class="doc">Container Device Interface Support in the GPU Operator</span></a> for post-installation configuration information.</p></li>
<li><p>[Technology Preview] Added support for precompiled driver containers for select operating systems.
This feature removes the dynamic dependencies to build the driver during installation in the
cluster such as downloading kernel header packages and GCC tooling.
Sites with isolated networks that cannot access the internet can benefit.
Sites with machines that are resource constrained can also benefit by removing the computational demand
to compile the driver.
For more information, see <a class="reference internal" href="precompiled-drivers.html"><span class="doc">Precompiled Driver Containers</span></a>.</p></li>
<li><p>Added support for the NVIDIA H800 GPU in the <a class="reference internal" href="platform-support.html#supported-nvidia-gpus-and-systems"><span class="std std-ref">Supported NVIDIA Data Center GPUs and Systems</span></a> table on the Platform Support page.</p></li>
</ul>
</section>
<section id="id28">
<h3>Improvements<a class="headerlink" href="#id28" title="Permalink to this headline"></a></h3>
<ul>
<li><p>The upgrade process for the GPU driver is enhanced.
This release introduces a <code class="docutils literal notranslate"><span class="pre">maxUnavailable</span></code> field that you can use to specify
the number of nodes that can be unavailable during an upgrade.
The value can be an integer or a string that specifies a percentage.
If you specify a percentage, the number of nodes is calculated by rounding up.
The default value is <code class="docutils literal notranslate"><span class="pre">25%</span></code>.</p>
<p>If you specify a value for <code class="docutils literal notranslate"><span class="pre">maxUnavailable</span></code> and also specify <code class="docutils literal notranslate"><span class="pre">maxParallelUpgrades</span></code>,
the <code class="docutils literal notranslate"><span class="pre">maxUnavailable</span></code> value applies an additional constraint on the value of
<code class="docutils literal notranslate"><span class="pre">maxParallelUpgrades</span></code> to ensure that the number of parallel upgrades does not
cause more than the intended number of nodes to become unavailable during the upgrade.
For example, if you specify <code class="docutils literal notranslate"><span class="pre">maxUnavailable=100%</span></code> and <code class="docutils literal notranslate"><span class="pre">maxParallelUpgrades=1</span></code>,
one node at a time is upgraded.</p>
</li>
<li><p>In previous releases, when you upgrade the GPU driver, the Operator validator
pod could fail to complete all the validation checks.
As a result, the node could remain in the validation required state indefinitely
and prevent performing the driver upgrade on the other nodes in the cluster.
This release adds a <code class="docutils literal notranslate"><span class="pre">600</span></code> second timeout for the validation process.
If the validation does not complete successfully within the duration, the node is
labelled <code class="docutils literal notranslate"><span class="pre">upgrade-failed</span></code> and the upgrade process proceeds on other nodes.</p></li>
<li><p>The Multi-Instance GPU (MIG) manager is enhanced to support setting an initial
value for the <code class="docutils literal notranslate"><span class="pre">nvidia.com/mig.config</span></code> node annotation.
On nodes with MIG-capable GPUs that do not already have the annotation set, the
value is set to <code class="docutils literal notranslate"><span class="pre">all-disabled</span></code> and the MIG manager does not create MIG devices.
The value is overwritten when you label the node with a MIG profile.
For configuration information, see <a class="reference internal" href="gpu-operator-mig.html"><span class="doc">GPU Operator with MIG</span></a>.</p></li>
</ul>
</section>
<section id="id29">
<h3>Fixed issues<a class="headerlink" href="#id29" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed an issue that prevented building the GPU driver container when a <a class="reference internal" href="install-gpu-operator-air-gapped.html#local-package-repository"><span class="std std-ref">Local Package Repository</span></a>
is used.
Previously, if you needed to provide CA certificates, the certificates were not installed correctly.
The certificates are now installed in the correct directories.
Refer to GitHub <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/299">issue #299</a> for more details.</p></li>
<li><p>Fixed an issue that created audit log records related to deprecated API requests for pod security policy.
on Red Hat OpenShift Container Platform.
Refer to GitHub <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/451">issue #451</a>
and <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/490">issue #490</a> for more details.</p></li>
<li><p>Fixed an issue that caused the Operator to attempt to add a pod security policy on pre-release versions
of Kubernetes v1.25.
Refer to GitHub <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/484">issue #484</a> for more details.</p></li>
<li><p>Fixed a race condition that is related to preinstalled GPU drivers, validator pods, and the device plugin pods.
The race condition can cause the device plugin pods to set the wrong path to the GPU driver.
Refer to GitHub <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/508">issue #508</a> for more details.</p></li>
<li><p>Fixed an issue with the driver manager that prevented the manager from accurately detecting whether a
node has preinstalled GPU drivers.
This issue can appear if preinstalled GPU drivers were initially installed and later removed.
The resolution is for the manager to check that the <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> file exists on the host
and to check the output from executing the file.</p></li>
<li><p>Fixed an issue that prevented adding custom annotations to daemon sets that the Operator starts.
Refer to GitHub <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/499">issue #499</a> for more details.</p></li>
<li><p>Fixed an issue that is related to not starting the GPU Feature Discovery (GFD) pods when the DCGM Exporter
service monitor is enabled, but a service monitor custom resource definition does not exist.
Previously, there was no log record to describe why the GFD pods were not started.
In this release, the Operator logs the error <code class="docutils literal notranslate"><span class="pre">Couldn't</span> <span class="pre">find</span> <span class="pre">ServiceMonitor</span> <span class="pre">CRD</span></code> and the
message <code class="docutils literal notranslate"><span class="pre">Install</span> <span class="pre">Prometheus</span> <span class="pre">and</span> <span class="pre">necessary</span> <span class="pre">CRDs</span> <span class="pre">for</span> <span class="pre">gathering</span> <span class="pre">GPU</span> <span class="pre">metrics</span></code> to indicate
the reason.</p></li>
<li><p>Fixed a race condition that prevented the GPU driver containers from loading the nvidia-peermem Linux kernel module
and caused the driver daemon set pods to crash loop back off.
The condition could occur when both GPUDirect RDMA and GPUDirect Storage are enabled.
In this release, the start script for the driver containers confirm that Operator validator
indicates the driver container is ready before attempting to load the kernel module.</p></li>
<li><p>Fixed an issue that caused upgrade of the GPU driver to fail when GPUDirect Storage is enabled.
In this release, the driver manager unloads the nvidia-fs Linux kernel module before
performing the upgrade.</p></li>
<li><p>Added support for new MIG profiles with the 525 driver.</p>
<ul>
<li><p>For A100-40GB devices:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">1g.5gb.me</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">1g.10gb</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">4g.20gb</span></code></p></li>
</ul>
</li>
<li><p>For H100-80GB and A100-80GB devices:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">1g.10gb</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">1g.10gb.me</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">1g.20gb</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">4g.40gb</span></code></p></li>
</ul>
</li>
<li><p>For A30-24GB devices:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">1g.6gb.me</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">2g.12gb.me</span></code></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="common-vulnerabilities-and-exposures-cves">
<h3>Common Vulnerabilities and Exposures (CVEs)<a class="headerlink" href="#common-vulnerabilities-and-exposures-cves" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">gpu-operator:v23.3.0</span></code> and <code class="docutils literal notranslate"><span class="pre">gpu-operator-validator:v23.3.0</span></code> images have the following known high-vulnerability CVEs.
These CVEs are from the base images and are not in libraries that are used by the GPU operator:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">openssl-libs</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2023-0286">CVE-2023-0286</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">platform-python</span></code> and <code class="docutils literal notranslate"><span class="pre">python3-libs</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2023-24329">CVE-2023-24329</a></p></li>
</ul>
</section>
<section id="id30">
<h3>Known Limitations<a class="headerlink" href="#id30" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Using NVIDIA vGPU on bare metal nodes and NVSwitch is not supported.</p></li>
<li><p>When installing the Operator on Amazon EKS and using Kubernetes versions lower than
<code class="docutils literal notranslate"><span class="pre">1.25</span></code>, specify the <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">psp.enabled=true</span></code> Helm argument because EKS enables
pod security policy (PSP).
If you use Kubernetes version <code class="docutils literal notranslate"><span class="pre">1.25</span></code> or higher, do not specify the <code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code>
argument so that the default value, <code class="docutils literal notranslate"><span class="pre">false</span></code>, is used.</p></li>
<li><p>Ubuntu 18.04 is scheduled to reach end of standard support in May of 2023.
When Ubuntu transitions it to end of life (EOL), the NVIDIA GPU Operator and
related projects plan to cease building containers for 18.04 and to
cease providing support.</p></li>
<li><p>All worker nodes within the Kubernetes cluster must use the same operating system version.</p></li>
<li><p>NVIDIA GPUDirect Storage (GDS) is not supported with secure boot enabled systems.</p></li>
<li><p>Driver Toolkit images are broken with Red Hat OpenShift version <code class="docutils literal notranslate"><span class="pre">4.11.12</span></code> and require cluster-level entitlements to be enabled
in this case for the driver installation to succeed.</p></li>
<li><p>The NVIDIA GPU Operator can only be used to deploy a single NVIDIA GPU Driver type and version. The NVIDIA vGPU and Data Center GPU Driver cannot be used within the same cluster.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver must be blacklisted when using NVIDIA vGPU.
Otherwise the driver fails to initialize the GPU with the error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">enable</span> <span class="pre">MSI-X</span></code> in the system journal logs.
Additionally, all GPU operator pods become stuck in the <code class="docutils literal notranslate"><span class="pre">Init</span></code> state.</p></li>
<li><p>When using RHEL 8 with Kubernetes, SELinux must be enabled (either in permissive or enforcing mode) for use with the GPU Operator.
Additionally, network-restricted environments are not supported.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id31">
<h2>22.9.2<a class="headerlink" href="#id31" title="Permalink to this headline"></a></h2>
<section id="id32">
<h3>New Features<a class="headerlink" href="#id32" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added support for Kubernetes v1.26 and Red Hat OpenShift 4.12.
Refer to <a class="reference internal" href="platform-support.html"><span class="doc">Platform Support</span></a> for more details.</p></li>
<li><p>Added a new controller that is responsible for managing NVIDIA driver upgrades.
Refer to <a class="reference internal" href="gpu-driver-upgrades.html"><span class="doc">GPU Driver Upgrades</span></a> for more details.</p></li>
<li><p>Added the ability to apply custom labels and annotations for all of the GPU Operator pods.
Refer to <a class="reference internal" href="getting-started.html#gpu-operator-helm-chart-options"><span class="std std-ref">Chart Customization Options</span></a> for how to configure custom labels and annotations.</p></li>
<li><p>Added support for NVIDIA vGPU 15.1.
Refer to the <a class="reference external" href="https://docs.nvidia.com/grid/15.0/index.html">NVIDIA Virtual GPU Software Documentation</a>.</p></li>
<li><p>Added support for the NVIDIA HGX H100 System in the <a class="reference internal" href="platform-support.html#supported-nvidia-gpus-and-systems"><span class="std std-ref">Supported NVIDIA Data Center GPUs and Systems</span></a> table on the Platform Support page.</p></li>
<li><p>Added 525.85.12 as the recommended driver version and 3.1.6 as the recommended DCGM version in the <a class="reference internal" href="platform-support.html#gpu-operator-component-matrix"><span class="std std-ref">GPU Operator Component Matrix</span></a>.
These updates enable support for the NVIDIA HGX H100 System.</p></li>
</ul>
</section>
<section id="id33">
<h3>Improvements<a class="headerlink" href="#id33" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Enhanced the driver validation logic to make sure that the current instance of the driver container has successfully finished installing drivers.
This enhancement prevents other operands from incorrectly starting with previously loaded drivers.</p></li>
<li><p>Increased overall driver startup probe timeout from 10 to 20 minutes.
The increased timeout improves the installation experience for clusters with slow networks by avoiding unnecessary driver container restarts.</p></li>
</ul>
</section>
<section id="id34">
<h3>Fixed issues<a class="headerlink" href="#id34" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed an issue where containers allocated GPU lose access to them when systemd is triggered to run some reevaluation of the cgroups it manages.
The issue affects systems using runc configured with systemd cgroups.
Refer to Github <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/430">issue #430</a> for more details.</p></li>
<li><p>Fixed an issue that prevented the GPU operator from applying PSA labels on the namespace when no prior labels existed.</p></li>
</ul>
</section>
<section id="id35">
<h3>Common Vulnerabilities and Exposures (CVEs)<a class="headerlink" href="#id35" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">gpu-operator:v22.9.2</span></code> and <code class="docutils literal notranslate"><span class="pre">gpu-operator:v22.9.2-ubi8</span></code> images have the following known high-vulnerability CVEs.
These CVEs are from the base images and are not in libraries that are used by the GPU operator:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">libksba</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2022-47629">CVE-2022-47629</a></p></li>
</ul>
</div></blockquote>
</section>
<section id="id36">
<h3>Known Limitations<a class="headerlink" href="#id36" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>All worker nodes within the Kubernetes cluster must use the same operating system version.</p></li>
<li><p>NVIDIA GPUDirect Storage (GDS) is not supported with secure boot enabled systems.</p></li>
<li><p>Driver Toolkit images are broken with Red Hat OpenShift version <code class="docutils literal notranslate"><span class="pre">4.11.12</span></code> and require cluster-level entitlements to be enabled
in this case for the driver installation to succeed.</p></li>
<li><p>No support for newer MIG profiles <code class="docutils literal notranslate"><span class="pre">1g.10gb</span></code>, <code class="docutils literal notranslate"><span class="pre">1g.20gb</span></code>, <code class="docutils literal notranslate"><span class="pre">2.12gb+me</span></code> with R525 drivers.</p></li>
<li><p>The NVIDIA GPU Operator can only be used to deploy a single NVIDIA GPU Driver type and version. The NVIDIA vGPU and Data Center GPU Driver cannot be used within the same cluster.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver must be blacklisted when using NVIDIA vGPU.
Otherwise the driver fails to initialize the GPU with the error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">enable</span> <span class="pre">MSI-X</span></code> in the system journal logs.
Additionally, all GPU operator pods become stuck in the <code class="docutils literal notranslate"><span class="pre">Init</span></code> state.</p></li>
<li><p>When using RHEL 8 with Kubernetes, SELinux must be enabled (either in permissive or enforcing mode) for use with the GPU Operator.
Additionally, network-restricted environments are not supported.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id37">
<h2>22.9.1<a class="headerlink" href="#id37" title="Permalink to this headline"></a></h2>
<section id="id38">
<h3>New Features<a class="headerlink" href="#id38" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Support for CUDA 12.0 / R525 Data Center drivers on x86 / ARM servers.</p></li>
<li><p>Support for RHEL 8.7 with Kubernetes and Containerd or CRI-O.</p></li>
<li><p>Support for Ubuntu 20.4 and 22.04 with Kubernetes and CRI-O.</p></li>
<li><p>Support for NVIDIA GPUDirect Storage using Ubuntu 20.04 and Ubuntu 22.04 with Kubernetes.</p></li>
<li><p>Support for RTX 6000 ADA GPU</p></li>
<li><p>Support for A800 GPU</p></li>
<li><p>Support for vSphere 8.0 with Tanzu</p></li>
<li><p>Support for vGPU 15.0</p></li>
<li><p>Support for HPE Ezmeral Runtime Enterprise. Version 5.5 - with RHEL 8.4 and 8.5</p></li>
</ul>
</section>
<section id="id39">
<h3>Improvements<a class="headerlink" href="#id39" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added helm parameters to control operator logging levels and time encoding.</p></li>
<li><p>When using CRI-O runtime with Kubernetes, it is no longer required to update the CRI-O config file to include <code class="docutils literal notranslate"><span class="pre">/run/containers/oci/hooks.d</span></code> as an additional path for OCI hooks. By default, the NVIDIA OCI runtime hook gets installed at <code class="docutils literal notranslate"><span class="pre">/usr/share/containers/oci/hooks.d</span></code> which is the default path configured with CRI-O.</p></li>
<li><p>Allow per node configurations for NVIDIA Device Plugin using a custom ConfigMap and node label <code class="docutils literal notranslate"><span class="pre">nvidia.com/device-plugin.config=&lt;config-name&gt;</span></code>.</p></li>
<li><p>Support for <a class="reference external" href="https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/#daemonset-update-strategy">OnDelete</a> upgrade strategy for all Daemonsets deployed by the GPU Operator.
This can be configured using <code class="docutils literal notranslate"><span class="pre">daemonsets.upgradeStrategy</span></code> parameter in the <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code>. This prevents pods managed by the GPU Operator from being restarted automatically on spec updates.</p></li>
<li><p>Enable eviction of GPU Pods only during driver container upgrades with <code class="docutils literal notranslate"><span class="pre">ENABLE_GPU_POD_EVICTION</span></code> env (default: “true”) set under <code class="docutils literal notranslate"><span class="pre">driver.manager.env</span></code> in the <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code>.
This eliminates the requirement to drain the entire node currently.</p></li>
</ul>
</section>
<section id="id40">
<h3>Fixed issues<a class="headerlink" href="#id40" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fix repeated restarts of container-toolkit when used with containerd versions <code class="docutils literal notranslate"><span class="pre">v1.6.9</span></code> and above. Refer to Github <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/432">issue #432</a> for more details.</p></li>
<li><p>Disable creation of PodSecurityPolicies (PSP) with K8s versions <code class="docutils literal notranslate"><span class="pre">1.25</span></code> and above as it is removed.</p></li>
</ul>
</section>
<section id="id41">
<h3>Common Vulnerabilities and Exposures (CVEs)<a class="headerlink" href="#id41" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed - Updated driver images for <code class="docutils literal notranslate"><span class="pre">515.86.01</span></code>, <code class="docutils literal notranslate"><span class="pre">510.108.03</span></code>, <code class="docutils literal notranslate"><span class="pre">470.161.03</span></code>, <code class="docutils literal notranslate"><span class="pre">450.216.04</span></code> to address CVEs noted <a class="reference external" href="https://nvidia.custhelp.com/app/answers/detail/a_id/5415">here</a>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpu-operator:v22.9.1</span></code> and <code class="docutils literal notranslate"><span class="pre">gpu-operator:v22.9.1-ubi8</span></code> images have been released with the following known HIGH Vulnerability CVEs.
These are from the base images and are not in libraries used by GPU Operator:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">krb5-libs</span></code> - <a class="reference external" href="https://nvd.nist.gov/vuln/detail/CVE-2022-42898">CVE-2022-42898</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="id42">
<h3>Known Limitations<a class="headerlink" href="#id42" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>All worker nodes within the Kubernetes cluster must use the same operating system version.</p></li>
<li><p>NVIDIA GPUDirect Storage (GDS) is not supported with secure boot enabled systems.</p></li>
<li><p>Driver Toolkit images are broken with Red Hat OpenShift version <code class="docutils literal notranslate"><span class="pre">4.11.12</span></code> and require cluster level entitlements to be enabled
in this case for the driver installation to succeed.</p></li>
<li><p>No support for newer MIG profiles <code class="docutils literal notranslate"><span class="pre">1g.10gb</span></code>, <code class="docutils literal notranslate"><span class="pre">1g.20gb</span></code>, <code class="docutils literal notranslate"><span class="pre">2.12gb+me</span></code> with R525 drivers. It will be added in the following release.</p></li>
<li><p>The NVIDIA GPU Operator can only be used to deploy a single NVIDIA GPU Driver type and version. The NVIDIA vGPU and Data Center GPU Driver cannot be used within the same cluster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver has to be blacklisted when using NVIDIA vGPU. Otherwise the driver will fail to initialize the GPU with the error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">enable</span> <span class="pre">MSI-X</span></code> in the system journal logs and all GPU Operator pods will be stuck in <code class="docutils literal notranslate"><span class="pre">Init</span></code> state.</p></li>
<li><p>When using RHEL8 with Kubernetes, SELinux has to be enabled (either in permissive or enforcing mode) for use with the GPU Operator. Additionally, network restricted environments are not supported.</p></li>
</ul>
</section>
</section>
<section id="id43">
<h2>22.9.0<a class="headerlink" href="#id43" title="Permalink to this headline"></a></h2>
<section id="id44">
<h3>New Features<a class="headerlink" href="#id44" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Support for Hopper (H100) GPU with CUDA 11.8 / R520 Data Center drivers on x86 servers.</p></li>
<li><p>Support for RHEL 8 with Kubernetes and Containerd or CRI-O.</p></li>
<li><p>Support with Kubernetes 1.25.</p></li>
<li><p>Support for RKE2 (Rancher Kubernetes Engine 2) with Ubuntu 20.04 and RHEL8.</p></li>
<li><p>Support for GPUDirect RDMA with NVIDIA Network Operator 1.3.</p></li>
<li><p>Support for Red Hat OpenShift with Cloud Service Providers (CSPs) Amazon AWS, Google GKE and Microsoft Azure.</p></li>
<li><p>[General Availability] - Support for <a class="reference internal" href="gpu-operator-kubevirt.html#gpu-operator-kubevirt"><span class="std std-ref">KubeVirt and Red Hat OpenShift Virtualization with GPU Passthrough and NVIDIA vGPU based products</span></a>.</p></li>
<li><p>[General Availability] - OCP and Upstream Kubernetes on ARM with <a class="reference internal" href="platform-support.html#gpu-operator-arm-platforms"><span class="std std-ref">supported platforms</span></a>.</p></li>
<li><p>Support for <a class="reference external" href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission (PSA)</a> through the <code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code> flag. If enabled, the namespace where the operator is installed in will be labeled with the <code class="docutils literal notranslate"><span class="pre">privileged</span></code> pod security level.</p></li>
</ul>
</section>
<section id="id45">
<h3>Improvements<a class="headerlink" href="#id45" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Support automatic upgrade and cleanup of <code class="docutils literal notranslate"><span class="pre">clusterpolicies.nvidia.com</span></code> CRD using Helm hooks. Refer to <a class="reference internal" href="upgrade.html#operator-upgrades"><span class="std std-ref">Operator upgrades</span></a> for more info.</p></li>
<li><p>Support for dynamically enabling/disabling GFD, MIG Manager, DCGM and DCGM-Exporter.</p></li>
<li><p>Switched to calendar versioning starting from this release for better life cycle management and support. Refer to <a class="reference internal" href="platform-support.html#operator-versioning"><span class="std std-ref">NVIDIA GPU Operator Versioning</span></a> for more info.</p></li>
</ul>
</section>
<section id="id46">
<h3>Fixed issues<a class="headerlink" href="#id46" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Remove CUDA compat libs from the operator and all operand images to avoid mismatch with installed CUDA driver version. More info <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/391">here</a> and <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/389">here</a>.</p></li>
<li><p>Migrate to <code class="docutils literal notranslate"><span class="pre">node.k8s.io/v1</span></code> API for creation of <code class="docutils literal notranslate"><span class="pre">RuntimeClass</span></code> objects. More info <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/409">here</a>.</p></li>
<li><p>Remove PodSecurityPolicy (PSP) starting with Kubernetes v1.25. Setting <code class="docutils literal notranslate"><span class="pre">psp.enabled</span></code> will now enable Pod Security Admission (PSA) instead.</p></li>
</ul>
</section>
<section id="id47">
<h3>Known Limitations<a class="headerlink" href="#id47" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>All worker nodes within the Kubernetes cluster must use the same operating system version.</p></li>
<li><p>The NVIDIA GPU Operator can only be used to deploy a single NVIDIA GPU Driver type and version. The NVIDIA vGPU and Data Center GPU Driver cannot be used within the same cluster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver has to be blacklisted when using NVIDIA vGPU. Otherwise the driver will fail to initialize the GPU with the error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">enable</span> <span class="pre">MSI-X</span></code> in the system journal logs and all GPU Operator pods will be stuck in <code class="docutils literal notranslate"><span class="pre">Init</span></code> state.</p></li>
<li><p>When using <code class="docutils literal notranslate"><span class="pre">CRI-O</span></code> runtime with Kubernetes, the config file <code class="docutils literal notranslate"><span class="pre">/etc/crio/crio.conf</span></code> has to include <code class="docutils literal notranslate"><span class="pre">/run/containers/oci/hooks.d</span></code> as path for <code class="docutils literal notranslate"><span class="pre">hooks_dir</span></code>. Refer <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/edge/latest/anthos-guide.html#custom-runtime-options" title="(in edge)"><span>Custom configuration for runtime containerd</span></a> for steps to configure this.</p></li>
<li><p>When using RHEL8 with Kubernetes, SELinux has to be enabled (either in permissive or enforcing mode) for use with the GPU Operator. Additionally, network restricted environments are not supported.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpu-operator:v22.9.0</span></code> and <code class="docutils literal notranslate"><span class="pre">gpu-operator:v22.9.0-ubi8</span></code> images have been released with the following known HIGH Vulnerability CVEs.
These are from the base images and are not in libraries used by GPU Operator:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">expat</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2022-40674">CVE-2022-40674</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">systemd-pam</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2022-2526">CVE-2022-2526</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">systemd</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2022-2526">CVE-2022-2526</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">systemd-libs</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2022-2526">CVE-2022-2526</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id50">
<h2>1.11.1<a class="headerlink" href="#id50" title="Permalink to this headline"></a></h2>
<section id="id51">
<h3>Improvements<a class="headerlink" href="#id51" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added <code class="docutils literal notranslate"><span class="pre">startupProbe</span></code> to NVIDIA driver container to allow RollingUpgrades to progress to other nodes only after driver modules are successfully loaded on current one.</p></li>
<li><p>Added support for <code class="docutils literal notranslate"><span class="pre">driver.rollingUpdate.maxUnavailable</span></code> parameter to specify maximum nodes for simultaneous driver upgrades. Default is 1.</p></li>
<li><p>NVIDIA driver container will auto-disable itself on the node with pre-installed drivers by applying label <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.deploy.driver=pre-installed</span></code>. This is useful for heterogeneous clusters where only some GPU nodes have pre-installed drivers(e.g. DGX OS).</p></li>
</ul>
</section>
<section id="id52">
<h3>Fixed issues<a class="headerlink" href="#id52" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Apply tolerations to <code class="docutils literal notranslate"><span class="pre">cuda-validator</span></code> and <code class="docutils literal notranslate"><span class="pre">device-plugin-validator</span></code> Pods based on <code class="docutils literal notranslate"><span class="pre">deamonsets.tolerations</span></code> in <cite>ClusterPolicy</cite>. For more info refer <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/360">here</a>.</p></li>
<li><p>Fixed an issue causing <code class="docutils literal notranslate"><span class="pre">cuda-validator</span></code> Pod to fail when <code class="docutils literal notranslate"><span class="pre">accept-nvidia-visible-devices-envvar-when-unprivileged</span> <span class="pre">=</span> <span class="pre">false</span></code> is set with NVIDIA Container Toolkit. For more info refer <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/365">here</a>.</p></li>
<li><p>Fixed an issue which caused recursive mounts under <code class="docutils literal notranslate"><span class="pre">/run/nvidia/driver</span></code> when both <code class="docutils literal notranslate"><span class="pre">driver.rdma.enabled</span></code> and <code class="docutils literal notranslate"><span class="pre">driver.rdma.useHostMofed</span></code> are set to <code class="docutils literal notranslate"><span class="pre">true</span></code>. This caused other GPU Pods to fail to start.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id53">
<h2>1.11.0<a class="headerlink" href="#id53" title="Permalink to this headline"></a></h2>
<section id="id54">
<h3>New Features<a class="headerlink" href="#id54" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Support for NVIDIA Data Center GPU Driver version <code class="docutils literal notranslate"><span class="pre">515.48.07</span></code>.</p></li>
<li><p>Support for NVIDIA AI Enterprise 2.1.</p></li>
<li><p>Support for NVIDIA Virtual Compute Server 14.1 (vGPU).</p></li>
<li><p>Support for Ubuntu 22.04 LTS.</p></li>
<li><p>Support for secure boot with GPU Driver version 515 and Ubuntu Server 20.04 LTS and 22.04 LTS.</p></li>
<li><p>Support for Kubernetes 1.24.</p></li>
<li><p>Support for <a class="reference internal" href="gpu-sharing.html#gpu-sharing"><span class="std std-ref">Time-Slicing GPUs in Kubernetes</span></a>.</p></li>
<li><p>Support for Red Hat OpenShift on AWS, Azure and GCP instances. Refer to the Platform Support Matrix for the supported instances.</p></li>
<li><p>Support for Red Hat Openshift 4.10 on AWS EC2 G5g instances(ARM).</p></li>
<li><p>Support for Kubernetes 1.24 on AWS EC2 G5g instances(ARM).</p></li>
<li><p>Support for use with the NVIDIA Network Operator 1.2.</p></li>
<li><p>[Technical Preview] - Support for <a class="reference internal" href="gpu-operator-kubevirt.html#gpu-operator-kubevirt"><span class="std std-ref">KubeVirt and Red Hat OpenShift Virtualization with GPU Passthrough and NVIDIA vGPU based products</span></a>.</p></li>
<li><p>[Technical Preview] - Kubernetes on ARM with Server Base System Architecture (SBSA).</p></li>
</ul>
</section>
<section id="id55">
<h3>Improvements<a class="headerlink" href="#id55" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>GPUDirect RDMA is now supported with CentOS using MOFED installed on the node.</p></li>
<li><p>The NVIDIA vGPU Manager can now be upgraded to a newer branch while using an older, compatible guest driver.</p></li>
<li><p>DGX A100 and non-DGX servers can now be used within the same cluster.</p></li>
<li><p>Improved user interface while deploying a ClusterPolicy instance(CR) for the GPU Operator through Red Hat OpenShift Console.</p></li>
<li><p>Improved the container-toolkit to handle v1 containerd configurations.</p></li>
</ul>
</section>
<section id="id56">
<h3>Fixed issues<a class="headerlink" href="#id56" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fix for incorrect reporting of <code class="docutils literal notranslate"><span class="pre">DCGM_FI_DEV_FB_USED</span></code> where reserved memory is reported as used memory. For more details refer to <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/348">GitHub issue</a>.</p></li>
<li><p>Fixed nvidia-peermem sidecar container to correctly load the <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> module when MOFED is directly installed on the node.</p></li>
<li><p>Fixed duplicate mounts of <code class="docutils literal notranslate"><span class="pre">/run/mellanox/drivers</span></code> within the driver container which caused driver cleanup or re-install to fail.</p></li>
<li><p>Fixed uncordoning of the node with k8s-driver-manager whenever ENABLE_AUTO_DRAIN env is disabled.</p></li>
<li><p>Fixed readiness check for MOFED driver installation by the NVIDIA Network Operator. This will avoid the GPU driver containers to be in <code class="docutils literal notranslate"><span class="pre">CrashLoopBackOff</span></code> while waiting for MOFED drivers to be ready.</p></li>
</ul>
</section>
<section id="id57">
<h3>Known Limitations<a class="headerlink" href="#id57" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>All worker nodes within the Kubernetes cluster must use the same operating system version.</p></li>
<li><p>The NVIDIA GPU Operator can only be used to deploy a single NVIDIA GPU Driver type and version. The NVIDIA vGPU and Data Center GPU Driver cannot be used within the same cluster.</p></li>
<li><p>See the <a class="reference internal" href="gpu-operator-kubevirt.html#gpu-operator-kubevirt-limitations"><span class="std std-ref">limitations</span></a> sections for the [Technical Preview] of GPU Operator support for KubeVirt.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">clusterpolicies.nvidia.com</span></code> CRD has to be manually deleted after the GPU Operator is uninstalled using Helm.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver has to be blacklisted when using the NVIDIA vGPU. Otherwise the driver will fail to initialize the GPU with the error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">enable</span> <span class="pre">MSI-X</span></code> in the system journal logs and all GPU Operator pods will be stuck in <code class="docutils literal notranslate"><span class="pre">init</span></code> state.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpu-operator:v1.11.0</span></code> and <code class="docutils literal notranslate"><span class="pre">gpu-operator:v1.11.0-ubi8</span></code> images have been released with the following known HIGH Vulnerability CVEs.
These are from the base images and are not in libraries used by GPU Operator:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">xz-libs</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2022-1271">CVE-2022-1271</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id58">
<h2>1.10.1<a class="headerlink" href="#id58" title="Permalink to this headline"></a></h2>
<section id="id59">
<h3>Improvements<a class="headerlink" href="#id59" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Validated secure boot with signed NVIDIA Data Center Driver R510.</p></li>
<li><p>Validated cgroup v2 with Ubuntu Server 20.04 LTS.</p></li>
</ul>
</section>
<section id="id60">
<h3>Fixed issues<a class="headerlink" href="#id60" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed an issue when GPU Operator was installed and MIG was already enabled on a GPU. The GPU Operator will now install successfully and MIG can either be disabled via the label <code class="docutils literal notranslate"><span class="pre">nvidia.com/mig.config=all-disabled</span></code> or configured with the required MIG profiles.</p></li>
</ul>
</section>
<section id="id61">
<h3>Known Limitations<a class="headerlink" href="#id61" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpu-operator:v1.10.1</span></code> and <code class="docutils literal notranslate"><span class="pre">gpu-operator:v1.10.1-ubi8</span></code> images have been released with the following known HIGH Vulnerability CVEs.
These are from the base images and are not in libraries used by GPU Operator:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">openssl-libs</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2022-0778">CVE-2022-0778</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">zlib</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2018-25032">CVE-2018-25032</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gzip</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2022-1271">CVE-2022-1271</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id63">
<h2>1.10.0<a class="headerlink" href="#id63" title="Permalink to this headline"></a></h2>
<section id="id64">
<h3>New Features<a class="headerlink" href="#id64" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Support for NVIDIA Data Center GPU Driver version <cite>510.47.03</cite>.</p></li>
<li><p>Support NVIDIA A2, A100X and A30X</p></li>
<li><p>Support for A100X and A30X on the DPU’s Arm processor.</p></li>
<li><p>Support for secure boot with Ubuntu Server 20.04 and NVIDIA Data Center GPU Driver version R470.</p></li>
<li><p>Support for Red Hat OpenShift 4.10.</p></li>
<li><p>Support for GPUDirect RDMA with Red Hat OpenShift.</p></li>
<li><p>Support for NVIDIA AI Enterprise 2.0.</p></li>
<li><p>Support for NVIDIA Virtual Compute Server 14 (vGPU).</p></li>
</ul>
</section>
<section id="id65">
<h3>Improvements<a class="headerlink" href="#id65" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Enabling/Disabling of GPU System Processor (GSP) Mode through NVIDIA driver module parameters.</p></li>
<li><p>Ability to avoid deploying GPU Operator Operands on certain worker nodes through labels. Useful for running VMs with GPUs using KubeVirt.</p></li>
</ul>
</section>
<section id="id66">
<h3>Fixed issues<a class="headerlink" href="#id66" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Increased lease duration of GPU Operator to 60s to avoid restarts during etcd defrag. More details <a class="reference external" href="https://github.com/NVIDIA/gpu-operator/issues/326">here</a>.</p></li>
<li><p>Avoid spurious alerts generated of type <code class="docutils literal notranslate"><span class="pre">GPUOperatorOpenshiftDriverToolkitEnabledNfdTooOld</span></code> on RedHat OpenShift when there are no GPU nodes in the cluster.</p></li>
<li><p>Avoid uncordoning nodes during driver pod startup when <code class="docutils literal notranslate"><span class="pre">ENABLE_AUTO_DRAIN</span></code> is set to <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></li>
<li><p>Collection of GPU metrics in MIG mode is now supported with 470+ drivers.</p></li>
<li><p>Fabric Manager (required for NVSwitch based systems) with CentOS 7 is now supported.</p></li>
</ul>
</section>
<section id="id67">
<h3>Known Limitations<a class="headerlink" href="#id67" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Upgrading to a new NVIDIA AI Enterprise major branch:</p>
<p>Upgrading the vGPU host driver to a newer major branch than the vGPU guest driver will result in GPU driver pod transitioning to a failed state. This happens for instance when the Host is upgraded to vGPU version 14.x while the Kubernetes nodes are still running with vGPU version 13.x.</p>
<p>To overcome this situation, before upgrading the host driver to the new vGPU branch, apply the following steps:</p>
<ol class="arabic">
<li><p>kubectl edit clusterpolicy</p></li>
<li><p>modify the policy and set the environment variable DISABLE_VGPU_VERSION_CHECK to true as shown below:</p>
<blockquote>
<div><div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">driver</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">env</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DISABLE_VGPU_VERSION_CHECK</span><span class="w"></span>
<span class="w">    </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span><span class="w"></span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>write and quit the clusterpolicy edit</p></li>
</ol>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpu-operator:v1.10.0</span></code> and <code class="docutils literal notranslate"><span class="pre">gpu-operator:v1.10.0-ubi8</span></code> images have been released with the following known HIGH Vulnerability CVEs.
These are from the base images and are not in libraries used by GPU Operator:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">openssl-libs</span></code> - <a class="reference external" href="https://access.redhat.com/security/cve/CVE-2022-0778">CVE-2022-0778</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id69">
<h2>1.9.1<a class="headerlink" href="#id69" title="Permalink to this headline"></a></h2>
<section id="id70">
<h3>Improvements<a class="headerlink" href="#id70" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Improved logic in the driver container for waiting on MOFED driver readiness. This ensures that <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> is built and installed correctly.</p></li>
</ul>
</section>
<section id="id71">
<h3>Fixed issues<a class="headerlink" href="#id71" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Allow <code class="docutils literal notranslate"><span class="pre">driver</span></code> container to fallback to using cluster entitlements on Red Hat OpenShift on build failures. This issue exposed itself when using GPU Operator with some Red Hat OpenShift 4.8.z versions and Red Hat OpenShift 4.9.8. GPU Operator 1.9+ with Red Hat OpenShift 4.9.9+ doesn’t require entitlements.</p></li>
<li><p>Fixed an issue when DCGM-Exporter didn’t work correctly with using the separate DCGM host engine that is part of the standalone DCGM pod. Fixed the issue and changed the default behavior to use the DCGM Host engine that is embedded in DCGM-Exporter. The standalone DCGM pod will not be launched by default but can be enabled for use with DGX A100.</p></li>
<li><p>Update to latest Go vendor packages to avoid any CVE’s.</p></li>
<li><p>Fixed an issue to allow GPU Operator to work with <code class="docutils literal notranslate"><span class="pre">CRI-O</span></code> runtime on Kubernetes.</p></li>
<li><p>Mount correct source path for Mellanox OFED 5.x drivers for enabling GPUDirect RDMA.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id72">
<h2>1.9.0<a class="headerlink" href="#id72" title="Permalink to this headline"></a></h2>
<section id="id73">
<h3>New Features<a class="headerlink" href="#id73" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Support for NVIDIA Data Center GPU Driver version <cite>470.82.01</cite>.</p></li>
<li><p>Support for DGX A100 with DGX OS 5.1+.</p></li>
<li><p>Support for preinstalled GPU Driver with MIG Manager.</p></li>
<li><p>Removed dependency to maintain active Red Hat OpenShift entitlements to build the GPU Driver. Introduce entitlement free driver builds starting with Red Hat OpenShift 4.9.9.</p></li>
<li><p>Support for GPUDirect RDMA with preinstalled Mellanox OFED drivers.</p></li>
<li><p>Support for GPU Operator and operands upgrades using Red Hat OpenShift Lifecycle Manager (OLM).</p></li>
<li><p>Support for NVIDIA Virtual Compute Server 13.1 (vGPU).</p></li>
</ul>
</section>
<section id="id74">
<h3>Improvements<a class="headerlink" href="#id74" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Automatic detection of default runtime used in the cluster. Deprecate the operator.defaultRuntime parameter.</p></li>
<li><p>GPU Operator and its operands are installed into a single user specified namespace.</p></li>
<li><p>A loaded Nouveau driver is automatically detected and unloaded as part of the GPU Operator install.</p></li>
<li><p>Added an option to mount a ConfigMap of self-signed certificates into the driver container. Enables SSL connections to private package repositories.</p></li>
</ul>
</section>
<section id="id75">
<h3>Fixed issues<a class="headerlink" href="#id75" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed an issue when DCGM Exporter was in CrashLoopBackOff as it could not connect to the DCGM port on the same node.</p></li>
</ul>
</section>
<section id="id76">
<h3>Known Limitations<a class="headerlink" href="#id76" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>GPUDirect RDMA is only supported with R470 drivers on Ubuntu 20.04 LTS and is not supported on other distributions (e.g. CoreOS, CentOS etc.)</p></li>
<li><p>The GPU Operator supports GPUDirect RDMA only in conjunction with the Network Operator. The Mellanox OFED drivers can be installed by the Network Operator or pre-installed on the host.</p></li>
<li><p>Upgrades from v1.8.x to v1.9.x are not supported due to GPU Operator 1.9 installing the GPU Operator and its operands into a single namespace. Previous GPU Operator versions installed them into different namespaces. Upgrading to GPU Operator 1.9 requires uninstalling pre 1.9 GPU Operator versions prior to installing GPU Operator 1.9</p></li>
<li><p>Collection of GPU metrics in MIG mode is not supported with 470+ drivers.</p></li>
<li><p>The GPU Operator requires all MIG related configurations to be executed by MIG Manager. Enabling/Disabling MIG and other MIG related configurations directly on the host is discouraged.</p></li>
<li><p>Fabric Manager (required for NVSwitch based systems) with CentOS 7 is not supported.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id77">
<h2>1.8.2<a class="headerlink" href="#id77" title="Permalink to this headline"></a></h2>
<section id="id78">
<h3>Fixed issues<a class="headerlink" href="#id78" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed an issue where Driver Daemonset was spuriously updated on RedHat OpenShift causing repeated restarts in Proxy environments.</p></li>
<li><p>The MIG Manager version was bumped to <cite>v0.1.3</cite> to fix an issue when checking whether a GPU was in MIG mode or not.
Previously, it would always check for MIG mode directly over the PCIe bus instead of using NVML. Now it checks with NVML when it can, only falling back to the PCIe bus when NVML is not available.
Please refer to the <a class="reference external" href="https://github.com/NVIDIA/mig-parted/releases/tag/v0.1.3">Release notes</a>  for a complete list of fixed issues.</p></li>
<li><p>Container Toolkit bumped to version <cite>v1.7.1</cite> to fix an issue when using A100 80GB.</p></li>
</ul>
</section>
<section id="id80">
<h3>Improvements<a class="headerlink" href="#id80" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added support for user-defined MIG partition configuration via a <cite>ConfigMap</cite>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id81">
<h2>1.8.1<a class="headerlink" href="#id81" title="Permalink to this headline"></a></h2>
<section id="id82">
<h3>Fixed issues<a class="headerlink" href="#id82" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed an issue with using the <a class="reference external" href="https://docs.nvidia.com/license-system/latest/">NVIDIA License System</a> in NVIDIA AI Enterprise deployments.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id83">
<h2>1.8.0<a class="headerlink" href="#id83" title="Permalink to this headline"></a></h2>
<section id="id84">
<h3>New Features<a class="headerlink" href="#id84" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Support for NVIDIA Data Center GPU Driver version <cite>470.57.02</cite>.</p></li>
<li><p>Added support for NVSwitch systems such as HGX A100. The driver container detects the presence of NVSwitches
in the system and automatically deploys the <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/pdf/fabric-manager-user-guide.pdf">Fabric Manager</a>
for setting up the NVSwitch fabric.</p></li>
<li><p>The driver container now builds and loads the <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module when GPUDirect RDMA is enabled and Mellanox devices are present in the system.
This allows the GPU Operator to complement the <a class="reference external" href="https://github.com/Mellanox/network-operator">NVIDIA Network Operator</a> to enable GPUDirect RDMA in the
Kubernetes cluster. Refer to the <a class="reference internal" href="gpu-operator-rdma.html#operator-rdma"><span class="std std-ref">RDMA</span></a> documentation on getting started.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This feature is available only when used with R470 drivers on Ubuntu 20.04 LTS.</p>
</div>
</li>
<li><p>Added support for <a class="reference internal" href="upgrade.html#operator-upgrades"><span class="std std-ref">upgrades</span></a> of the GPU Operator components. A new <code class="docutils literal notranslate"><span class="pre">k8s-driver-manager</span></code> component handles upgrades
of the NVIDIA drivers on nodes in the cluster.</p></li>
<li><p>NVIDIA DCGM is now deployed as a component of the GPU Operator. The standalone DCGM container allows multiple clients such as
<a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-telemetry/dcgm-exporter.html">DCGM-Exporter</a> and <a class="reference external" href="https://docs.nvidia.com/nvidia-system-management-nvsm/">NVSM</a>
to be deployed and connect to the existing DCGM container.</p></li>
<li><p>Added a <code class="docutils literal notranslate"><span class="pre">nodeStatusExporter</span></code> component that exports operator and node metrics in a Prometheus format. The component provides
information on the status of the operator (e.g. reconciliation status, number of GPU enabled nodes).</p></li>
</ul>
</section>
<section id="id85">
<h3>Improvements<a class="headerlink" href="#id85" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Reduced the size of the ClusterPolicy CRD by removing duplicates and redundant fields.</p></li>
<li><p>The GPU Operator now supports detection of the virtual PCIe topology of the system and makes the topology available to
vGPU drivers via a configuration file. The driver container starts the <code class="docutils literal notranslate"><span class="pre">nvidia-topologyd</span></code> daemon in vGPU configurations.</p></li>
<li><p>Added support for specifying the <code class="docutils literal notranslate"><span class="pre">RuntimeClass</span></code> variable via Helm.</p></li>
<li><p>Added <code class="docutils literal notranslate"><span class="pre">nvidia-container-toolkit</span></code> images to support CentOS 7 and CentOS 8.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nvidia-container-toolkit</span></code> now supports configuring <cite>containerd</cite> correctly for RKE2.</p></li>
<li><p>Added new debug options (logging, verbosity levels) for <code class="docutils literal notranslate"><span class="pre">nvidia-container-toolkit</span></code></p></li>
</ul>
</section>
<section id="id86">
<h3>Fixed issues<a class="headerlink" href="#id86" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The driver container now loads <code class="docutils literal notranslate"><span class="pre">ipmi_devintf</span></code> by default. This allows tools such as <code class="docutils literal notranslate"><span class="pre">ipmitool</span></code> that rely on <code class="docutils literal notranslate"><span class="pre">ipmi</span></code> char devices
to be created and available.</p></li>
</ul>
</section>
<section id="id87">
<h3>Known Limitations<a class="headerlink" href="#id87" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>GPUDirect RDMA is only supported with R470 drivers on Ubuntu 20.04 LTS and is not supported on other distributions (e.g. CoreOS, CentOS etc.)</p></li>
<li><p>The operator supports building and loading of <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> only in conjunction with the Network Operator. Use with pre-installed MOFED drivers
on the host is not supported. This capability will be added in a future release.</p></li>
<li><p>Support for DGX A100 with GPU Operator 1.8 will be available in an upcoming patch release.</p></li>
<li><p>This version of GPU Operator does not work well on RedHat OpenShift when a cluster-wide proxy is configured and causes constant restarts of driver container.
This will be fixed in an upcoming patch release <cite>v1.8.2</cite>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id88">
<h2>1.7.1<a class="headerlink" href="#id88" title="Permalink to this headline"></a></h2>
<section id="id89">
<h3>Fixed issues<a class="headerlink" href="#id89" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>NFD version bumped to <cite>v0.8.2</cite> to support correct kernel version labelling on Anthos nodes. See <a class="reference external" href="https://github.com/kubernetes-sigs/node-feature-discovery/pull/402">NFD issue</a> for more details.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id90">
<h2>1.7.0<a class="headerlink" href="#id90" title="Permalink to this headline"></a></h2>
<section id="id91">
<h3>New Features<a class="headerlink" href="#id91" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Support for NVIDIA Data Center GPU Driver version <cite>460.73.01</cite>.</p></li>
<li><p>Added support for automatic configuration of MIG geometry on NVIDIA Ampere products (e.g. A100) using the <code class="docutils literal notranslate"><span class="pre">k8s-mig-manager</span></code>.</p></li>
<li><p>GPU Operator can now be deployed on systems with pre-installed NVIDIA drivers and the NVIDIA Container Toolkit.</p></li>
<li><p>DCGM-Exporter now supports telemetry for MIG devices on supported Ampere products (e.g. A100).</p></li>
<li><p>Added support for a new <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> <code class="docutils literal notranslate"><span class="pre">RuntimeClass</span></code> with <cite>containerd</cite>.</p></li>
<li><p>The Operator now supports <code class="docutils literal notranslate"><span class="pre">PodSecurityPolicies</span></code> when enabled in the cluster.</p></li>
</ul>
</section>
<section id="id92">
<h3>Improvements<a class="headerlink" href="#id92" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Changed the label selector used by the DaemonSets of the different states of the GPU Operator. Instead of having a global
label <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.present=true</span></code>, each DaemonSet now has its own label, <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.deploy.&lt;state&gt;=true</span></code>. This
new behavior allows a finer grain of control over the components deployed on each of the GPU nodes.</p></li>
<li><p>Migrated to using the latest operator-sdk for building the GPU Operator.</p></li>
<li><p>The operator components are deployed with <code class="docutils literal notranslate"><span class="pre">node-critical</span></code> <code class="docutils literal notranslate"><span class="pre">PriorityClass</span></code> to minimize the possibility of eviction.</p></li>
<li><p>Added a spec for the <code class="docutils literal notranslate"><span class="pre">initContainer</span></code> image, to allow flexibility to change the base images as required.</p></li>
<li><p>Added the ability to configure the MIG strategy to be applied by the Operator.</p></li>
<li><p>The driver container now auto-detects OpenShift/RHEL versions to better handle node/cluster upgrades.</p></li>
<li><p>Validations of the container-toolkit and device-plugin installations are done on all GPU nodes in the cluster.</p></li>
<li><p>Added an option to skip plugin validation workload pod during the Operator deployment.</p></li>
</ul>
</section>
<section id="id93">
<h3>Fixed issues<a class="headerlink" href="#id93" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpu-operator-resources</span></code> namespace is now created by the Operator so that they can be used by both Helm
and OpenShift installations.</p></li>
</ul>
</section>
<section id="id94">
<h3>Known Limitations<a class="headerlink" href="#id94" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>DCGM does not support profiling metrics on RTX 6000 and RTX 8000. Support will be added in a future release of DCGM Exporter.</p></li>
<li><p>After un-install of GPU Operator, NVIDIA driver modules might still be loaded. Either reboot the node or forcefully remove them using
<code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">rmmod</span> <span class="pre">nvidia</span> <span class="pre">nvidia_modeset</span> <span class="pre">nvidia_uvm</span></code> command before re-installing GPU Operator again.</p></li>
<li><p>When MIG strategy of <code class="docutils literal notranslate"><span class="pre">mixed</span></code> is configured, device-plugin-validation may stay in <code class="docutils literal notranslate"><span class="pre">Pending</span></code> state due to incorrect GPU resource request type. User would need to
modify the pod spec to apply correct resource type to match the MIG devices configured in the cluster.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id95">
<h2>1.6.2<a class="headerlink" href="#id95" title="Permalink to this headline"></a></h2>
<section id="id96">
<h3>Fixed issues<a class="headerlink" href="#id96" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed an issue with NVIDIA Container Toolkit 1.4.6 which causes an error with containerd as <code class="docutils literal notranslate"><span class="pre">Error</span> <span class="pre">while</span> <span class="pre">dialing</span> <span class="pre">dial</span> <span class="pre">unix</span> <span class="pre">/run/containerd/containerd.sock:</span> <span class="pre">connect:</span> <span class="pre">connection</span> <span class="pre">refused</span></code>. NVIDIA Container Toolkit 1.4.7 now sets <code class="docutils literal notranslate"><span class="pre">version</span></code> as an integer to fix this error.</p></li>
<li><p>Fixed an issue with NVIDIA Container Toolkit which causes nvidia-container-runtime settings to be persistent across node reboot and causes driver pod to fail. Now nvidia-container-runtime will fallback to using <code class="docutils literal notranslate"><span class="pre">runc</span></code> when driver modules are not yet loaded during node reboot.</p></li>
<li><p>GPU Operator now mounts runtime hook configuration for CRIO under <code class="docutils literal notranslate"><span class="pre">/run/containers/oci/hooks.d</span></code>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id97">
<h2>1.6.1<a class="headerlink" href="#id97" title="Permalink to this headline"></a></h2>
<section id="id98">
<h3>Fixed issues<a class="headerlink" href="#id98" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed an issue with NVIDIA Container Toolkit 1.4.5 when used with containerd and an empty containerd configuration which file causes error <code class="docutils literal notranslate"><span class="pre">Error</span> <span class="pre">while</span> <span class="pre">dialing</span> <span class="pre">dial</span> <span class="pre">unix</span> <span class="pre">/run/containerd/containerd.sock:</span> <span class="pre">connect:</span> <span class="pre">connection</span> <span class="pre">refused</span></code>. NVIDIA Container Toolkit 1.4.6 now explicitly sets the <code class="docutils literal notranslate"><span class="pre">version=2</span></code> along with other changes when the default containerd configuration file is empty.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id99">
<h2>1.6.0<a class="headerlink" href="#id99" title="Permalink to this headline"></a></h2>
<section id="id100">
<h3>New Features<a class="headerlink" href="#id100" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Support for Red Hat OpenShift 4.7.</p></li>
<li><p>Support for NVIDIA Data Center GPU Driver version <cite>460.32.03</cite>.</p></li>
<li><p>Automatic injection of Proxy settings and custom CA certificates into driver container for Red Hat OpenShift.</p></li>
</ul>
<p>DCGM-Exporter support includes the following:</p>
<ul class="simple">
<li><p>Updated DCGM to v2.1.4</p></li>
<li><p>Increased reporting interval to 30s instead of 2s to reduce overhead</p></li>
<li><p>Report NVIDIA vGPU licensing status and row-remapping metrics for Ampere GPUs</p></li>
</ul>
</section>
<section id="id101">
<h3>Improvements<a class="headerlink" href="#id101" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>NVIDIA vGPU licensing configuration (gridd.conf) can be provided as a ConfigMap</p></li>
<li><p>ClusterPolicy CRD has been updated from v1beta1 to v1. As a result minimum supported Kubernetes version is 1.16 from GPU Operator 1.6.0 onwards.</p></li>
</ul>
</section>
<section id="id102">
<h3>Fixed issues<a class="headerlink" href="#id102" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixes for DCGM Exporter to work with CPU Manager.</p></li>
<li><p>nvidia-gridd daemon logs are now collected on host by rsyslog.</p></li>
</ul>
</section>
<section id="id103">
<h3>Known Limitations<a class="headerlink" href="#id103" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>DCGM does not support profiling metrics on RTX 6000 and RTX 8000. Support will be added in a future release of DCGM Exporter.</p></li>
<li><p>After un-install of GPU Operator, NVIDIA driver modules might still be loaded. Either reboot the node or forcefully remove them using
<code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">rmmod</span> <span class="pre">nvidia</span> <span class="pre">nvidia_modeset</span> <span class="pre">nvidia_uvm</span></code> command before re-installing GPU Operator again.</p></li>
<li><p>When MIG strategy of <code class="docutils literal notranslate"><span class="pre">mixed</span></code> is configured, device-plugin-validation may stay in <code class="docutils literal notranslate"><span class="pre">Pending</span></code> state due to incorrect GPU resource request type. User would need to
modify the pod spec to apply correct resource type to match the MIG devices configured in the cluster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu-operator-resources</span></code> project in Red Hat OpenShift requires label <code class="docutils literal notranslate"><span class="pre">openshift.io/cluster-monitoring=true</span></code> for Prometheus to collect DCGM metrics. User will need to add this
label manually when project is created.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id104">
<h2>1.5.2<a class="headerlink" href="#id104" title="Permalink to this headline"></a></h2>
<section id="id105">
<h3>Improvements<a class="headerlink" href="#id105" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Allow <code class="docutils literal notranslate"><span class="pre">mig.strategy=single</span></code> on nodes with non-MIG GPUs.</p></li>
<li><p>Pre-create MIG related <code class="docutils literal notranslate"><span class="pre">nvcaps</span></code> at startup.</p></li>
<li><p>Updated device-plugin and toolkit validation to work with CPU Manager.</p></li>
</ul>
</section>
<section id="id106">
<h3>Fixed issues<a class="headerlink" href="#id106" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed issue which causes GFD pods to fail with error <code class="docutils literal notranslate"><span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">load</span> <span class="pre">NVML</span></code> error even after driver is loaded.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id107">
<h2>1.5.1<a class="headerlink" href="#id107" title="Permalink to this headline"></a></h2>
<section id="id108">
<h3>Improvements<a class="headerlink" href="#id108" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Kubelet’s cgroup driver as <code class="docutils literal notranslate"><span class="pre">systemd</span></code> is now supported.</p></li>
</ul>
</section>
<section id="id109">
<h3>Fixed issues<a class="headerlink" href="#id109" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Device-Plugin stuck in <code class="docutils literal notranslate"><span class="pre">init</span></code> phase on node reboot or when new node is added to the cluster.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id110">
<h2>1.5.0<a class="headerlink" href="#id110" title="Permalink to this headline"></a></h2>
<section id="id111">
<h3>New Features<a class="headerlink" href="#id111" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added support for NVIDIA vGPU</p></li>
</ul>
</section>
<section id="id112">
<h3>Improvements<a class="headerlink" href="#id112" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Driver Validation container is run as an initContainer within device-plugin Daemonset pods. Thus driver installation on each NVIDIA GPU/vGPU node will be validated.</p></li>
<li><p>GFD will label vGPU nodes with driver version and branch name of NVIDIA vGPU installed on Hypervisor.</p></li>
<li><p>Driver container will perform automatic compatibility check of NVIDIA vGPU driver with the version installed on the underlying Hypervisor.</p></li>
</ul>
</section>
<section id="id113">
<h3>Fixed issues<a class="headerlink" href="#id113" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>GPU Operator will no longer crash when no GPU nodes are found.</p></li>
<li><p>Container Toolkit pods wait for drivers to be loaded on the system before setting the default container runtime as <cite>nvidia</cite>.</p></li>
<li><p>On host reboot, ordering of pods is maintained to ensure that drivers are always loaded first.</p></li>
<li><p>Fixed device-plugin issue causing <code class="docutils literal notranslate"><span class="pre">symbol</span> <span class="pre">lookup</span> <span class="pre">error:</span> <span class="pre">nvidia-device-plugin:</span> <span class="pre">undefined</span> <span class="pre">symbol:</span> <span class="pre">nvmlEventSetWait_v2</span></code> error.</p></li>
</ul>
</section>
<section id="id114">
<h3>Known Limitations<a class="headerlink" href="#id114" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The GPU Operator v1.5.x does not support mixed types of GPUs in the same cluster. All GPUs within a cluster need to be either NVIDIA vGPUs, GPU Passthrough GPUs or Bare Metal GPUs.</p></li>
<li><p>GPU Operator v1.5.x with NVIDIA vGPUs support Turing and newer GPU architectures.</p></li>
<li><p>DCGM does not support profiling metrics on RTX 6000 and RTX 8000. Support will be added in a future release of DCGM Exporter.</p></li>
<li><p>After un-install of GPU Operator, NVIDIA driver modules might still be loaded. Either reboot the node or forcefully remove them using
<code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">rmmod</span> <span class="pre">nvidia</span> <span class="pre">nvidia_modeset</span> <span class="pre">nvidia_uvm</span></code> command before re-installing GPU Operator again.</p></li>
<li><p>When MIG strategy of <code class="docutils literal notranslate"><span class="pre">mixed</span></code> is configured, device-plugin-validation may stay in <code class="docutils literal notranslate"><span class="pre">Pending</span></code> state due to incorrect GPU resource request type. User would need to
modify the pod spec to apply correct resource type to match the MIG devices configured in the cluster.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu-operator-resources</span></code> project in Red Hat OpenShift requires label <code class="docutils literal notranslate"><span class="pre">openshift.io/cluster-monitoring=true</span></code> for Prometheus to collect DCGM metrics. User will need to add this
label manually when project is created.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id115">
<h2>1.4.0<a class="headerlink" href="#id115" title="Permalink to this headline"></a></h2>
<section id="id116">
<h3>New Features<a class="headerlink" href="#id116" title="Permalink to this headline"></a></h3>
<ul>
<li><p>Added support for CentOS 7 and 8.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Due to a known limitation with the GPU Operator’s default values on CentOS, install the operator on CentOS 7/8
using the following Helm command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
  nvidia/gpu-operator <span class="se">\</span>
  --set toolkit.version<span class="o">=</span><span class="m">1</span>.4.0-ubi8
</pre></div>
</div>
<p>This issue will be fixed in the next release.</p>
</div>
</li>
<li><p>Added support for airgapped enterprise environments.</p></li>
<li><p>Added support for <code class="docutils literal notranslate"><span class="pre">containerd</span></code> as a container runtime under Kubernetes.</p></li>
</ul>
</section>
<section id="id117">
<h3>Improvements<a class="headerlink" href="#id117" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Updated DCGM-Exporter to <code class="docutils literal notranslate"><span class="pre">2.1.2</span></code>, which uses DCGM 2.0.13.</p></li>
<li><p>Added the ability to pass arguments to the NVIDIA device plugin to enable <code class="docutils literal notranslate"><span class="pre">migStrategy</span></code> and <code class="docutils literal notranslate"><span class="pre">deviceListStrategy</span></code> flags
that allow additional configuration of the plugin.</p></li>
<li><p>Added more resiliency to <code class="docutils literal notranslate"><span class="pre">dcgm-exporter</span></code>- <code class="docutils literal notranslate"><span class="pre">dcgm-exporter</span></code> would not check whether GPUs support profiling metrics and would result in a <code class="docutils literal notranslate"><span class="pre">CrashLoopBackOff</span></code>
state at launch in these configurations.</p></li>
</ul>
</section>
<section id="id118">
<h3>Fixed issues<a class="headerlink" href="#id118" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed the issue where the removal of the GPU Operator from the cluster required a restart of the Docker daemon (since the Operator
sets the <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> as the default runtime).</p></li>
<li><p>Fixed volume mounts for <code class="docutils literal notranslate"><span class="pre">dcgm-exporter</span></code> under the GPU Operator to allow pod&lt;-&gt;device metrics attribution.</p></li>
<li><p>Fixed an issue where the GFD and <code class="docutils literal notranslate"><span class="pre">dcgm-exporter</span></code> container images were artificially limited to R450+ (CUDA 11.0+) drivers.</p></li>
</ul>
</section>
<section id="id119">
<h3>Known Limitations<a class="headerlink" href="#id119" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>After un-install of GPU Operator, NVIDIA driver modules might still be loaded. Either reboot the node or forcefully remove them using
<code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">rmmod</span> <span class="pre">nvidia</span> <span class="pre">nvidia_modeset</span> <span class="pre">nvidia_uvm</span></code> command before re-installing GPU Operator again.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id120">
<h2>1.3.0<a class="headerlink" href="#id120" title="Permalink to this headline"></a></h2>
<section id="id121">
<h3>New Features<a class="headerlink" href="#id121" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Integrated <a class="reference external" href="https://github.com/NVIDIA/gpu-feature-discovery">GPU Feature Discovery</a> to automatically generate labels for GPUs leveraging NFD.</p></li>
<li><p>Added support for Red Hat OpenShift 4.4+ (i.e. 4.4.29+, 4.5 and 4.6). The GPU Operator can be deployed from OpenShift OperatorHub. See the catalog
<a class="reference external" href="https://catalog.redhat.com/software/operators/nvidia/gpu-operator/5ea882962937381642a232cd">listing</a> for more information.</p></li>
</ul>
</section>
<section id="id122">
<h3>Improvements<a class="headerlink" href="#id122" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Updated DCGM-Exporter to <code class="docutils literal notranslate"><span class="pre">2.1.0</span></code> and added profiling metrics by default.</p></li>
<li><p>Added further capabilities to configure tolerations, node affinity, node selectors, pod security context, resource requirements through the <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code>.</p></li>
<li><p>Optimized the footprint of the validation containers images - the image sizes are now down to ~200MB.</p></li>
<li><p>Validation images are now configurable for air-gapped installations.</p></li>
</ul>
</section>
<section id="id123">
<h3>Fixed issues<a class="headerlink" href="#id123" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed the ordering of the state machine to ensure that the driver daemonset is deployed before the other components. This fix addresses the issue
where the NVIDIA container toolkit would be setup as the default runtime, causing the driver container initialization to fail.</p></li>
</ul>
</section>
<section id="id124">
<h3>Known Limitations<a class="headerlink" href="#id124" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>After un-install of GPU Operator, NVIDIA driver modules might still be loaded. Either reboot the node or forcefully remove them using
<code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">rmmod</span> <span class="pre">nvidia</span> <span class="pre">nvidia_modeset</span> <span class="pre">nvidia_uvm</span></code> command before re-installing GPU Operator again.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id125">
<h2>1.2.0<a class="headerlink" href="#id125" title="Permalink to this headline"></a></h2>
<section id="id126">
<h3>New Features<a class="headerlink" href="#id126" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added support for Ubuntu 20.04.z LTS.</p></li>
<li><p>Added support for the NVIDIA A100 GPU (and appropriate updates to the underlying components of the operator).</p></li>
</ul>
</section>
<section id="id127">
<h3>Improvements<a class="headerlink" href="#id127" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Updated Node Feature Discovery (NFD) to 0.6.0.</p></li>
<li><p>Container images are now hosted (and mirrored) on both <a class="reference external" href="https://hub.docker.com/u/nvidiadocker.io">DockerHub</a> and <a class="reference external" href="https://ngc.nvidia.com/catalog/containers/nvidia:gpu-operator">NGC</a>.</p></li>
</ul>
</section>
<section id="id128">
<h3>Fixed issues<a class="headerlink" href="#id128" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed an issue where the GPU operator would not correctly detect GPU nodes due to inconsistent PCIe node labels.</p></li>
<li><p>Fixed a race condition where some of the NVIDIA pods would start out of order resulting in some pods in <code class="docutils literal notranslate"><span class="pre">RunContainerError</span></code> state.</p></li>
<li><p>Fixed an issue in the driver container where the container would fail to install on systems with the <code class="docutils literal notranslate"><span class="pre">linux-gke</span></code> kernel due to not finding the kernel headers.</p></li>
</ul>
</section>
<section id="id129">
<h3>Known Limitations<a class="headerlink" href="#id129" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>After un-install of GPU Operator, NVIDIA driver modules might still be loaded. Either reboot the node or forcefully remove them using
<code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">rmmod</span> <span class="pre">nvidia</span> <span class="pre">nvidia_modeset</span> <span class="pre">nvidia_uvm</span></code> command before re-installing GPU Operator again.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id130">
<h2>1.1.0<a class="headerlink" href="#id130" title="Permalink to this headline"></a></h2>
<section id="id131">
<h3>New features<a class="headerlink" href="#id131" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>DCGM is now deployed as part of the GPU Operator on OpenShift 4.3.</p></li>
</ul>
</section>
<section id="id132">
<h3>Improvements<a class="headerlink" href="#id132" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The operator CRD has been renamed to <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code>.</p></li>
<li><p>The operator image is now based on UBI8.</p></li>
<li><p>Helm chart has been refactored to fix issues and follow some best practices.</p></li>
</ul>
</section>
<section id="id133">
<h3>Fixed issues<a class="headerlink" href="#id133" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Fixed an issue with the toolkit container which would setup the NVIDIA runtime under <code class="docutils literal notranslate"><span class="pre">/run/nvidia</span></code> with a symlink to <code class="docutils literal notranslate"><span class="pre">/usr/local/nvidia</span></code>.
If a node was rebooted, this would prevent any containers from being run with Docker as the container runtime configured in <code class="docutils literal notranslate"><span class="pre">/etc/docker/daemon.json</span></code>
would not be available after reboot.</p></li>
<li><p>Fixed a race condition with the creation of the CRD and registration.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id134">
<h2>1.0.0<a class="headerlink" href="#id134" title="Permalink to this headline"></a></h2>
<section id="id135">
<h3>New Features<a class="headerlink" href="#id135" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Added support for Helm v3. Note that installing the GPU Operator using Helm v2 is no longer supported.</p></li>
<li><p>Added support for Red Hat OpenShift 4 (4.1, 4.2 and 4.3) using Red Hat Enterprise Linux Core OS (RHCOS) and CRI-O runtime on GPU worker nodes.</p></li>
<li><p>GPU Operator now deploys NVIDIA DCGM for GPU telemetry on Ubuntu 18.04 LTS</p></li>
</ul>
</section>
<section id="id136">
<h3>Fixed Issues<a class="headerlink" href="#id136" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The driver container now sets up the required dependencies on <code class="docutils literal notranslate"><span class="pre">i2c</span></code> and <code class="docutils literal notranslate"><span class="pre">ipmi_msghandler</span></code> modules.</p></li>
<li><p>Fixed an issue with the validation steps (for the driver and device plugin) taking considerable time. Node provisioning times are now improved by 5x.</p></li>
<li><p>The SRO custom resource definition is setup as part of the operator.</p></li>
<li><p>Fixed an issue with the clean up of driver mount files when deleting the operator from the cluster. This issue used to require a reboot of the node, which is no longer required.</p></li>
</ul>
</section>
<section id="operator-known-limitations">
<span id="id137"></span><h3>Known Limitations<a class="headerlink" href="#operator-known-limitations" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>After un-install of GPU Operator, NVIDIA driver modules might still be loaded. Either reboot the node or forcefully remove them using
<code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">rmmod</span> <span class="pre">nvidia</span> <span class="pre">nvidia_modeset</span> <span class="pre">nvidia_uvm</span></code> command before re-installing GPU Operator again.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="platform-support.html" class="btn btn-neutral float-left" title="Platform Support" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="troubleshooting.html" class="btn btn-neutral float-right" title="Troubleshooting the NVIDIA GPU Operator" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020-2024, NVIDIA.
      <span class="lastupdated">Last updated on Mar 13, 2024.
      </span></p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 



</body>
</html>