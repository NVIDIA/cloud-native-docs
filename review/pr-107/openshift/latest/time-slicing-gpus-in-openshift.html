<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Time-slicing NVIDIA GPUs in OpenShift &mdash; NVIDIA GPU Operator on Red Hat OpenShift Container Platform 24.6.2 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/api-styles.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/design-tabs.js"></script>
        <script src="_static/version.js"></script>
        <script src="_static/social-media.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="NVIDIA GPU Operator with OpenShift Virtualization" href="openshift-virtualization.html" />
    <link rel="prev" title="Enabling the GPU Monitoring Dashboard" href="enable-gpu-monitoring-dashboard.html" />
 
<script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
          </a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="prerequisites.html">Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="steps-overview.html">Installation and Upgrade Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-nfd.html">Installing the Node Feature Discovery (NFD) Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-ocp.html">Installing the NVIDIA GPU Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="nvaie-with-ocp.html">NVIDIA AI Enterprise with OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="mig-ocp.html">MIG Support in OpenShift Container Platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="clean-up.html">Cleanup</a></li>
<li class="toctree-l1"><a class="reference internal" href="mirror-gpu-ocp-disconnected.html">Deploy GPU Operators in a disconnected or airgapped environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable-gpu-monitoring-dashboard.html">Enabling the GPU Monitoring Dashboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable-gpu-monitoring-dashboard.html#viewing-gpu-metrics">Viewing GPU Metrics</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Time-slicing NVIDIA GPUs in OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="openshift-virtualization.html">NVIDIA GPU Operator with OpenShift Virtualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-with-precompiled-drivers.html">Precompiled Drivers for the NVIDIA GPU Operator for RHCOS</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting-gpu-ocp.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-ocp.html">Appendix</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA GPU Operator on Red Hat OpenShift Container Platform</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
<li>
    <a href="https://docs.nvidia.com">NVIDIA Docs Hub</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>
    <a href="https://docs.nvidia.com/datacenter/cloud-native/">NVIDIA Cloud Native Technologies</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>Time-slicing NVIDIA GPUs in OpenShift</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="time-slicing-nvidia-gpus-in-openshift">
<span id="id1"></span><h1>Time-slicing NVIDIA GPUs in OpenShift<a class="headerlink" href="#time-slicing-nvidia-gpus-in-openshift" title="Permalink to this headline"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>The latest generations of NVIDIA GPUs provide a mode of operation called Multi-Instance GPU (MIG).
MIG allows you to partition a GPU into several smaller, predefined instances, each of which looks like a mini-GPU that
provides memory and fault isolation at the hardware layer. Users can share access to a GPU by running their workloads
on one of these predefined instances instead of the full GPU.</p>
<p>This document describes a new mechanism for enabling time-sharing of GPUs in OpenShift. It allows a cluster
administrator to define a set of replicas for a GPU, each of which can be handed out independently to a pod
to run workloads on.</p>
<p>Unlike MIG, there is no memory or fault-isolation between replicas, but for some workloads this is better than not
being able to share at all. Under the hood, Compute Unified Device Architecture (CUDA) time-slicing is used to multiplex workloads from replicas of the
same underlying GPU.</p>
</section>
<section id="configuring-gpus-with-time-slicing">
<h2>Configuring GPUs with time slicing<a class="headerlink" href="#configuring-gpus-with-time-slicing" title="Permalink to this headline"></a></h2>
<p>The following sections show you how to configure NVIDIA Tesla T4 GPUs, as they do not support MIG, but can easily accept multiple small jobs.</p>
<section id="enabling-gpu-feature-discovery">
<h3>Enabling GPU Feature Discovery<a class="headerlink" href="#enabling-gpu-feature-discovery" title="Permalink to this headline"></a></h3>
<p>The feature release on GPU Feature Discovery (GFD) exposes the GPU types as labels and allows users to create node selectors based on these labels to help the scheduler place the pods. By default, when you create a <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code>
custom resource, GFD is enabled. In case, you disabled it, you can re-enable it with the following command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc patch clusterpolicy gpu-cluster-policy -n nvidia-gpu-operator <span class="se">\</span>
    --type json <span class="se">\</span>
    --patch <span class="s1">&#39;[{&quot;op&quot;: &quot;replace&quot;, &quot;path&quot;: &quot;/spec/gfd/enable&quot;, &quot;value&quot;: true}]&#39;</span>
</pre></div>
</div>
</section>
<section id="creating-the-slicing-configurations">
<h3>Creating the slicing configurations<a class="headerlink" href="#creating-the-slicing-configurations" title="Permalink to this headline"></a></h3>
<ol class="arabic">
<li><p>Before enabling a time slicing configuration, you need to tell the device plugin what are the possible configurations.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span><span class="w"></span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ConfigMap</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">device-plugin-config</span><span class="w"></span>
<span class="w">  </span><span class="nt">namespace</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia-gpu-operator</span><span class="w"></span>
<span class="nt">data</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">A100-SXM4-40GB</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|-</span><span class="w"></span>
<span class="w">    </span><span class="no">version: v1</span><span class="w"></span>
<span class="w">    </span><span class="no">sharing:</span><span class="w"></span>
<span class="w">      </span><span class="no">timeSlicing:</span><span class="w"></span>
<span class="w">        </span><span class="no">resources:</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/gpu</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 8</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/mig-1g.5gb</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 1</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/mig-2g.10gb</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 2</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/mig-3g.20gb</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 3</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/mig-7g.40gb</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 7</span><span class="w"></span>
<span class="w">  </span><span class="nt">A100-SXM4-80GB</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|-</span><span class="w"></span>
<span class="w">    </span><span class="no">version: v1</span><span class="w"></span>
<span class="w">    </span><span class="no">sharing:</span><span class="w"></span>
<span class="w">      </span><span class="no">timeSlicing:</span><span class="w"></span>
<span class="w">        </span><span class="no">resources:</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/gpu</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 8</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/mig-1g.10gb</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 1</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/mig-2g.20gb</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 2</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/mig-3g.40gb</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 3</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/mig-7g.80gb</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 7</span><span class="w"></span>
<span class="w">  </span><span class="nt">Tesla-T4</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|-</span><span class="w"></span>
<span class="w">    </span><span class="no">version: v1</span><span class="w"></span>
<span class="w">    </span><span class="no">sharing:</span><span class="w"></span>
<span class="w">      </span><span class="no">timeSlicing:</span><span class="w"></span>
<span class="w">        </span><span class="no">resources:</span><span class="w"></span>
<span class="w">          </span><span class="no">- name: nvidia.com/gpu</span><span class="w"></span>
<span class="w">            </span><span class="no">replicas: 8</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Create the ConfigMap:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc create -f device-plugin-config.yaml
</pre></div>
</div>
</li>
<li><p>Tell the GPU Operator which ConfigMap to use for the device plugin configuration. You can simply patch the <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code> custom resource.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc patch clusterpolicy gpu-cluster-policy <span class="se">\</span>
    -n nvidia-gpu-operator --type merge <span class="se">\</span>
    -p <span class="s1">&#39;{&quot;spec&quot;: {&quot;devicePlugin&quot;: {&quot;config&quot;: {&quot;name&quot;: &quot;device-plugin-config&quot;}}}}&#39;</span>
</pre></div>
</div>
</li>
<li><p>Apply the configuration to all the nodes you have with Tesla TA GPUs. GFD, labels the nodes with the GPU product, in this example <code class="docutils literal notranslate"><span class="pre">Tesla-T4</span></code>, so you can use a node selector to label all of the nodes at once.</p>
<p>You can also set <code class="docutils literal notranslate"><span class="pre">devicePlugin.config.default=Tesla-T4</span></code>, which applies the configuration across the cluster by default without requiring node specific labels.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc label --overwrite node <span class="se">\</span>
    --selector<span class="o">=</span>nvidia.com/gpu.product<span class="o">=</span>Tesla-T4 <span class="se">\</span>
    nvidia.com/device-plugin.config<span class="o">=</span>Tesla-T4
</pre></div>
</div>
</li>
<li><p>After a few seconds, the configuration is applied and you can verify that GPU resource replicas have been created. The following configuration creates eight replicas for Tesla T4 GPUs, so the <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu</span></code> external resource is set to <code class="docutils literal notranslate"><span class="pre">8</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get node --selector<span class="o">=</span>nvidia.com/gpu.product<span class="o">=</span>Tesla-T4-SHARED -o json <span class="p">|</span> jq <span class="s1">&#39;.items[0].status.capacity&#39;</span>
</pre></div>
</div>
<p><strong>Example output</strong></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &quot;attachable-volumes-aws-ebs&quot;: &quot;39&quot;,</span>
<span class="go">  &quot;cpu&quot;: &quot;4&quot;,</span>
<span class="go">  &quot;ephemeral-storage&quot;: &quot;125293548Ki&quot;,</span>
<span class="go">  &quot;hugepages-1Gi&quot;: &quot;0&quot;,</span>
<span class="go">  &quot;hugepages-2Mi&quot;: &quot;0&quot;,</span>
<span class="go">  &quot;memory&quot;: &quot;16105592Ki&quot;,</span>
<span class="go">  &quot;nvidia.com/gpu&quot;: &quot;8&quot;,</span>
<span class="go">  &quot;pods&quot;: &quot;250&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</li>
<li><p>Note that a -SHARED suffix has been added to the <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.product</span></code> label to reflect that time slicing is enabled. You can disable this in the configuration. For example, the Tesla T4 configuration would look like this:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">sharing</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">timeSlicing</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">renameByDefault</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span><span class="w"></span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/gpu</span><span class="w"></span>
<span class="w">        </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Verify that GFD labels have been added to indicate time-sharing.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get node --selector<span class="o">=</span>nvidia.com/gpu.product<span class="o">=</span>Tesla-T4-SHARED -o json <span class="se">\</span>
 <span class="p">|</span> jq <span class="s1">&#39;.items[0].metadata.labels&#39;</span> <span class="p">|</span> grep nvidia
</pre></div>
</div>
<p><strong>Example Output</strong></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">&quot;nvidia.com/cuda.driver.major&quot;: &quot;510&quot;,</span>
<span class="go">&quot;nvidia.com/cuda.driver.minor&quot;: &quot;73&quot;,</span>
<span class="go">&quot;nvidia.com/cuda.driver.rev&quot;: &quot;08&quot;,</span>
<span class="go">&quot;nvidia.com/cuda.runtime.major&quot;: &quot;11&quot;,</span>
<span class="go">&quot;nvidia.com/cuda.runtime.minor&quot;: &quot;7&quot;,</span>
<span class="go">&quot;nvidia.com/device-plugin.config&quot;: &quot;Tesla-T4&quot;,</span>
<span class="go">&quot;nvidia.com/gfd.timestamp&quot;: &quot;1655482336&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.compute.major&quot;: &quot;7&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.compute.minor&quot;: &quot;5&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.count&quot;: &quot;1&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.deploy.container-toolkit&quot;: &quot;true&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.deploy.dcgm&quot;: &quot;true&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.deploy.dcgm-exporter&quot;: &quot;true&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.deploy.device-plugin&quot;: &quot;true&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.deploy.driver&quot;: &quot;true&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.deploy.gpu-feature-discovery&quot;: &quot;true&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.deploy.node-status-exporter&quot;: &quot;true&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.deploy.nvsm&quot;: &quot;&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.deploy.operator-validator&quot;: &quot;true&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.family&quot;: &quot;turing&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.machine&quot;: &quot;g4dn.xlarge&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.memory&quot;: &quot;16106127360&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.present&quot;: &quot;true&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.product&quot;: &quot;Tesla-T4-SHARED&quot;,</span>
<span class="go">&quot;nvidia.com/gpu.replicas&quot;: &quot;8&quot;,</span>
<span class="go">&quot;nvidia.com/mig.strategy&quot;: &quot;single&quot;,</span>
</pre></div>
</div>
<p>If you remove the label, the node configuration is reset to its default.</p>
</li>
</ol>
</section>
</section>
<section id="applying-the-configuration-to-a-machineset">
<h2>Applying the configuration to a MachineSet<a class="headerlink" href="#applying-the-configuration-to-a-machineset" title="Permalink to this headline"></a></h2>
<p>With OpenShift, you can leverage the <a class="reference external" href="https://docs.openshift.com/container-platform/4.10/machine_management/index.html">Machine Management</a> feature to dynamically provision nodes on
platforms that support it.</p>
<p>For example, an administrator can create a MachineSet for nodes with Tesla T4 GPUs configured with time-slicing enabled.
This provides a pool of replicas for workloads that don’t require a full T4 GPU.</p>
<p>Consider a MachineSet named <code class="docutils literal notranslate"><span class="pre">worker-gpu-nvidia-t4-us-east-1</span></code>, with
<a class="reference external" href="https://docs.openshift.com/container-platform/4.10/machine_management/applying-autoscaling.html#machine-autoscaler-about_applying-autoscaling">Machine Autoscaler</a> configured.
You want to ensure the new nodes will have time slicing enabled automatically, that is, you want to apply the
label to every new node. This can be done by setting the label in the MachineSet template.</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc patch machineset worker-gpu-nvidia-t4-us-east-1a <span class="se">\</span>
    -n openshift-machine-api --type merge <span class="se">\</span>
    --patch <span class="s1">&#39;{&quot;spec&quot;: {&quot;template&quot;: {&quot;spec&quot;: {&quot;metadata&quot;: {&quot;labels&quot;: {&quot;nvidia.com/device-plugin.config&quot;: &quot;Tesla-T4&quot;}}}}}}&#39;</span>
</pre></div>
</div>
</div></blockquote>
<p>Now, any new machine created by the Machine Autoscaler for this MachineSet will have the label, and time-slicing enabled.</p>
</section>
<section id="sample-configmap-values">
<h2>Sample ConfigMap values<a class="headerlink" href="#sample-configmap-values" title="Permalink to this headline"></a></h2>
<p>The following table shows sample values for a ConfigMap that contains
multiple <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> files (small, medium, and large).</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 41%" />
<col style="width: 32%" />
<col style="width: 9%" />
<col style="width: 10%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Small</p></th>
<th class="head"><p>Medium</p></th>
<th class="head"><p>Large</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">replicas</span></code></p></td>
<td><p>The number of replicas
that can be specified
for each named resource.</p></td>
<td><p>2</p></td>
<td><p>5</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">renameByDefault</span></code></p></td>
<td><p>When <code class="docutils literal notranslate"><span class="pre">false</span></code>, the
<code class="docutils literal notranslate"><span class="pre">SHARED</span></code> suffix is
added to the product
label.</p></td>
<td><p>false</p></td>
<td><p>false</p></td>
<td><p>false</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">failRequestsGreaterThanOne</span></code></p></td>
<td><p>This flag is <code class="docutils literal notranslate"><span class="pre">false</span></code>
for backward
compatibility.</p></td>
<td><p>false</p></td>
<td><p>false</p></td>
<td><p>false</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike with standard GPU requests, requesting more than one shared GPU does not guarantee that you will have access to a proportional amount of compute power. It only specifies that you will have access to a GPU that is shared by other clients, each of which has the freedom to run as many processes on the underlying GPU as they want. Internally, the GPU will simply give an equal share of time to all GPU processes across all of the clients. The <code class="docutils literal notranslate"><span class="pre">failRequestsGreaterThanOne</span></code> flag is meant to help users understand this subtlety, by treating a request of 1 as an access request rather than an exclusive resource request. Setting <code class="docutils literal notranslate"><span class="pre">failRequestsGreaterThanOne=true</span></code> is recommended, but it is set to false by default to retain backwards compatibility.</p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="enable-gpu-monitoring-dashboard.html" class="btn btn-neutral float-left" title="Enabling the GPU Monitoring Dashboard" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="openshift-virtualization.html" class="btn btn-neutral float-right" title="NVIDIA GPU Operator with OpenShift Virtualization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
<img src="_static/NVIDIA-LogoBlack.svg" class="only-light"/>
<img src="_static/NVIDIA-LogoWhite.svg" class="only-dark"/>
<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>
<p>
  Copyright &#169; 2020-2024, NVIDIA Corporation.
</p>

    <p>
      <span class="lastupdated">Last updated on Sep 27, 2024.
      </span></p>
<script type="text/javascript">if (typeof _satellite !== "undefined"){ _satellite.pageBottom();}</script>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>