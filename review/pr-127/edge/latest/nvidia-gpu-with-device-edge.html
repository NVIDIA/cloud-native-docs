<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Accelerating workloads with NVIDIA GPUs with Red Hat Device Edge &mdash; NVIDIA Cloud Native Reference Architectures 1.0.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/api-styles.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/design-tabs.js"></script>
        <script src="_static/version.js"></script>
        <script src="_static/social-media.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="NVIDIA GPUs with Google Anthos" href="anthos-guide.html" />
    <link rel="prev" title="NVIDIA Cloud Native Reference Architectures" href="index.html" />
 
<script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
          </a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Accelerating workloads with NVIDIA GPUs with Red Hat Device Edge</a></li>
<li class="toctree-l1"><a class="reference internal" href="anthos-guide.html">NVIDIA GPUs with Google Anthos</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA Cloud Native Reference Architectures</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
<div> <!-- class="omni-version-warning" -->
  <p class="omni-version-warning-content"> Upgrade to NVIDIA Container Toolkit v1.16.2 or higher, or GPU Operator v24.6.2 or higher to install a critical security update.<br/>
  Refer to <a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5582">Security Bulletin: NVIDIA Container Toolkit - September 2024</a> for more information.</p>
</div>

<li>
    <a href="https://docs.nvidia.com">NVIDIA Docs Hub</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>
    <a href="https://docs.nvidia.com/datacenter/cloud-native/">NVIDIA Cloud Native Technologies</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>Accelerating workloads with NVIDIA GPUs with Red Hat Device Edge</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="accelerating-workloads-with-nvidia-gpus-with-red-hat-device-edge">
<span id="mirror-gpu-ocp-disconnected"></span><h1>Accelerating workloads with NVIDIA GPUs with Red Hat Device Edge<a class="headerlink" href="#accelerating-workloads-with-nvidia-gpus-with-red-hat-device-edge" title="Permalink to this headline"></a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id1">Introduction</a></p></li>
<li><p><a class="reference internal" href="#prerequisites" id="id2">Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#installing-the-nvidia-gpu-driver" id="id3">Installing the NVIDIA GPU driver</a></p></li>
<li><p><a class="reference internal" href="#installing-the-nvidia-container-toolkit" id="id4">Installing the NVIDIA Container Toolkit</a></p></li>
<li><p><a class="reference internal" href="#installing-the-nvidia-device-plugin" id="id5">Installing the NVIDIA Device Plugin</a></p></li>
<li><p><a class="reference internal" href="#running-a-gpu-accelerated-workload-on-red-hat-device-edge" id="id6">Running a GPU-Accelerated Workload on Red Hat Device Edge</a></p></li>
</ul>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Developer Preview features are not supported with Red Hat production service level agreements (SLAs) and are not functionally complete. Red Hat does not advise using them in a production setting. Developer Preview features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process. These releases may not have any documentation, and testing is limited. Red Hat may provide ways to submit feedback on Developer Preview releases without an associated SLA.</p>
</div>
<p>Red Hat has released Red Hat Device Edge, which provides access to MicroShift. MicroShift offers the simplicity of single-node deployment with the functionality and services you need for computing in resource-constrained locations. You can have many deployments on different hosts, creating the specific system image needed for each of your applications. Installing MicroShift on top of your managed RHEL devices in hard-to-service locations also allows for streamlined over-the-air updates.</p>
<p>Red Hat Device Edge combines light-weight Kubernetes using MicroShift with Red Hat Enterprise Linux at the edge. MicroShift is a Kubernetes implementation derived from OpenShift, focusing on a minimal  footprint. Red Hat Device Edge addresses the needs of bare metal, virtual, containerized, or kubernetes workloads deployed to resource constrained environments.</p>
<p>Perform the procedures on this page to enable workloads to use NVIDIA GPUs on an x86 system running Red Hat Device Edge.</p>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline"></a></h2>
<ul>
<li><p>Install <a class="reference external" href="https://access.redhat.com/documentation/en-us/red_hat_build_of_microshift/4.13/html/installing/microshift-install-rpm">MicroShift from an RPM package</a> on the Red Hat Enterprise Linux 8.7 machine.</p></li>
<li><p>Verify an NVIDIA GPU is installed on the machine:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lspci -nnv <span class="p">|</span> grep -i nvidia
</pre></div>
</div>
<p><strong>Example Output</strong></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">17:00.0 3D controller [0302]: NVIDIA Corporation GA100GL [A30 PCIe] [10de:20b7] (rev a1)</span>
<span class="go">        Subsystem: NVIDIA Corporation Device [10de:1532]</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="installing-the-nvidia-gpu-driver">
<h2>Installing the NVIDIA GPU driver<a class="headerlink" href="#installing-the-nvidia-gpu-driver" title="Permalink to this headline"></a></h2>
<p>NVIDIA provides a precompiled driver in RPM repositories that implement the modularity mechanism.
For more information, see <a class="reference external" href="https://developer.nvidia.com/blog/streamlining-nvidia-driver-deployment-on-rhel-8-with-modularity-streams/">Streamlining NVIDIA Driver Deployment on RHEL 8 with Modularity Streams</a>.</p>
<ol class="arabic">
<li><p>At this stage, you should have already subscribed your machine and enabled the <code class="docutils literal notranslate"><span class="pre">rhel-9-for-x86_64-baseos-rpms</span></code> and <code class="docutils literal notranslate"><span class="pre">rhel-9-for-x86_64-appstream-rpms</span></code> repositories.
Add the NVIDIA CUDA repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo dnf config-manager --add-repo<span class="o">=</span>https://developer.download.nvidia.com/compute/cuda/repos/rhel9/x86_64/cuda-rhel9.repo
</pre></div>
</div>
</li>
<li><p>NVIDIA provides different branches of their drivers, with different lifecycles, that are described in <a class="reference external" href="https://docs.nvidia.com/datacenter/tesla/drivers/index.html#cuda-drivers">NVIDIA Datacenter Drivers documentation</a>.
Use the latest version from the production branch, for example, version <code class="docutils literal notranslate"><span class="pre">R525</span></code>. Install the driver, fabric-manager and NSCQ:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo dnf module install nvidia-driver:525
<span class="gp">$ </span>sudo dnf install nvidia-fabric-manager libnvidia-nscq-525
</pre></div>
</div>
</li>
<li><p>After installing the driver, disable the <code class="docutils literal notranslate"><span class="pre">nouveau</span></code> driver because it conflict with the NVIDIA driver:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">echo</span> <span class="s1">&#39;blacklist nouveau&#39;</span> <span class="p">|</span> sudo tee /etc/modprobe.d/nouveau-blacklist.conf
</pre></div>
</div>
</li>
<li><p>Update initramfs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo dracut --force
</pre></div>
</div>
</li>
<li><p>Enable the <code class="docutils literal notranslate"><span class="pre">nvidia-fabricmanager</span></code> and <code class="docutils literal notranslate"><span class="pre">nvidia-persistenced</span></code> services:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo systemctl <span class="nb">enable</span> nvidia-fabricmanager.service
<span class="gp">$ </span>sudo systemctl <span class="nb">enable</span> nvidia-persistenced.service
</pre></div>
</div>
</li>
<li><p>Reboot the machine:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo systemctl reboot
</pre></div>
</div>
</li>
<li><p>After the machine boots, verify that the NVIDIA drivers are installed properly:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>nvidia-smi
</pre></div>
</div>
<p><strong>Example Output</strong></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Thu Jun 22 14:29:53 2023</span>
<span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |</span>
<span class="go">|-------------------------------+----------------------+----------------------+</span>
<span class="go">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span>
<span class="go">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span>
<span class="go">|                               |                      |               MIG M. |</span>
<span class="go">|===============================+======================+======================|</span>
<span class="go">|   0  NVIDIA A30          Off  | 00000000:17:00.0 Off |                    0 |</span>
<span class="go">| N/A   29C    P0    35W / 165W |      0MiB / 24576MiB |     25%      Default |</span>
<span class="go">|                               |                      |             Disabled |</span>
<span class="go">+-------------------------------+----------------------+----------------------+</span>

<span class="go">+-----------------------------------------------------------------------------+</span>
<span class="go">| Processes:                                                                  |</span>
<span class="go">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span>
<span class="go">|        ID   ID                                                   Usage      |</span>
<span class="go">|=============================================================================|</span>
<span class="go">|  No running processes found                                                 |</span>
<span class="go">+-----------------------------------------------------------------------------+</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="installing-the-nvidia-container-toolkit">
<h2>Installing the NVIDIA Container Toolkit<a class="headerlink" href="#installing-the-nvidia-container-toolkit" title="Permalink to this headline"></a></h2>
<p>The <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html">NVIDIA Container Toolkit</a> enables users
to build and run GPU accelerated containers. The toolkit includes a container runtime library and utilities to automatically configure containers
to leverage NVIDIA GPUs. You have to install it to enable the container runtime to transparently configure the NVIDIA GPUs for the pods deployed in MicroShift.</p>
<p>The NVIDIA container toolkit supports the distributions listed in the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installation-guide/">NVIDIA Container Toolkit repository</a>.</p>
<ol class="arabic">
<li><p>Add the <code class="docutils literal notranslate"><span class="pre">libnvidia-container</span></code> repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl -s -L https://nvidia.github.io/libnvidia-container/rhel8.7/libnvidia-container.repo <span class="p">|</span> sudo tee /etc/yum.repos.d/libnvidia-container.repo
</pre></div>
</div>
</li>
<li><p>Install the NVIDIA Container Toolkit for RHEL:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo dnf install nvidia-container-toolkit -y
</pre></div>
</div>
</li>
<li><p>The NVIDIA Container Toolkit requires some SELinux permissions to work properly. These permissions are set in three steps.</p>
<ol class="upperalpha">
<li><p>Use DNF to install the <code class="docutils literal notranslate"><span class="pre">container-selinux.noarch</span></code> package:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo dnf install container-selinux.noarch
</pre></div>
</div>
</li>
<li><p>Set the SELinux configuration flag for <code class="docutils literal notranslate"><span class="pre">container_use_devices</span></code> to <code class="docutils literal notranslate"><span class="pre">on</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo setsebool -P container_use_devices on
</pre></div>
</div>
</li>
<li><p>It is still missing a permission, so create a policy file:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat &lt;&lt;EOF &gt; nvidia-container-microshift.te
<span class="go">module nvidia-container-microshift 1.0;</span>

<span class="go">require {</span>
<span class="go">              type xserver_misc_device_t;</span>
<span class="go">              type container_t;</span>
<span class="go">              class chr_file { map read write };</span>
<span class="go">}</span>

<span class="gp">#</span><span class="o">=============</span> <span class="nv">container_t</span> <span class="o">==============</span>
<span class="go">allow container_t xserver_misc_device_t:chr_file map;</span>
<span class="go">EOF</span>
</pre></div>
</div>
</li>
<li><p>Compile the policy:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>checkmodule -m -M -o nvidia-container-microshift.mod nvidia-container-microshift.te
</pre></div>
</div>
</li>
<li><p>Create the <code class="docutils literal notranslate"><span class="pre">semodule</span></code> package:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>semodule_package --outfile nvidia-container-microshift.pp --module nvidia-container-microshift.mod
</pre></div>
</div>
</li>
</ol>
</li>
</ol>
<blockquote>
<div><ol class="upperalpha" start="6">
<li><p>Apply the policy:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo semodule -i nvidia-container-microshift.pp
</pre></div>
</div>
</li>
</ol>
</div></blockquote>
</section>
<section id="installing-the-nvidia-device-plugin">
<h2>Installing the NVIDIA Device Plugin<a class="headerlink" href="#installing-the-nvidia-device-plugin" title="Permalink to this headline"></a></h2>
<p>To enable MicroShift to allocate GPU resource to the pods, deploy the <a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA Device Plugin</a>.  The plugin runs as a daemon set that provides the following features:</p>
<ul class="simple">
<li><p>Exposes the number of GPUs on each node of your cluster.</p></li>
<li><p>Keeps track of the health of your GPUs.</p></li>
<li><p>Runs GPU-enabled containers in your Kubernetes cluster.</p></li>
</ul>
<p>The deployment consists of adding manifests and a <code class="docutils literal notranslate"><span class="pre">kustomize</span></code> configuration to the <code class="docutils literal notranslate"><span class="pre">/etc/microshift/manifests</span></code> folder where MicroShift checks for manifests to create at start time. This is explained in the <a class="reference external" href="https://access.redhat.com/documentation/en-us/red_hat_build_of_microshift/4.12/html/configuring/index">Configuring section of the MicroShift documentation</a>.</p>
<ol class="arabic">
<li><p>Create the <code class="docutils literal notranslate"><span class="pre">manifests</span></code> folder:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo mkdir -p /etc/microshift/manifests
</pre></div>
</div>
</li>
<li><p>The device plugin runs in privileged mode, so you need to isolate it from other workloads by running it in its own namespace, <code class="docutils literal notranslate"><span class="pre">nvidia-device-plugin</span></code>. To add the plugin to the manifests deployed by MicroShift at start time, download the configuration file and save it at <code class="docutils literal notranslate"><span class="pre">/etc/microshift/manifests/nvidia-device-plugin.yml</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>curl -s -L https://gitlab.com/nvidia/kubernetes/device-plugin/-/raw/main/deployments/static/nvidia-device-plugin-privileged-with-service-account.yml <span class="p">|</span> sudo tee /etc/microshift/manifests/nvidia-device-plugin.yml
</pre></div>
</div>
</li>
<li><p>The resources are not created automatically even though the files exist. You need to add them to the <code class="docutils literal notranslate"><span class="pre">kustomize</span></code> configuration. Do this by adding a single <code class="docutils literal notranslate"><span class="pre">kustomization.yaml</span></code> file in the <code class="docutils literal notranslate"><span class="pre">manifests</span></code> folder that references all the resources you want to create.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat &lt;&lt;EOF <span class="p">|</span> sudo tee /etc/microshift/manifests/kustomization.yaml
<span class="go">---</span>
<span class="go">apiVersion: kustomize.config.k8s.io/v1beta1</span>
<span class="go">kind: Kustomization</span>
<span class="go">resources:</span>
<span class="go">  - nvidia-device-plugin.yml</span>
<span class="go">EOF</span>
</pre></div>
</div>
</li>
<li><p>Restart the MicroShift service so that it creates the resources:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sudo systemctl restart microshift
</pre></div>
</div>
</li>
<li><p>After MicroShift restarts, verify that the pod is running in the <code class="docutils literal notranslate"><span class="pre">nvidia-device-plugin</span></code> namespace:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get pod -n nvidia-device-plugin
</pre></div>
</div>
<p><strong>Example Output</strong></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAMESPACE                  NAME                                   READY   STATUS        RESTARTS     AGE</span>
<span class="go">nvidia-device-plugin       nvidia-device-plugin-daemonset-jx8s8   1/1     Running       0            1m</span>
</pre></div>
</div>
</li>
<li><p>Verify in the log that it has registered itself as a device plugin for the <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu</span></code> resources:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc logs -n nvidia-device-plugin nvidia-device-plugin-jx8s8
</pre></div>
</div>
<p><strong>Example Output</strong></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">[...]</span>
<span class="go">2023/06/22 14:25:38 Retreiving plugins.</span>
<span class="go">2023/06/22 14:25:38 Detected NVML platform: found NVML library</span>
<span class="go">2023/06/22 14:25:38 Detected non-Tegra platform: /sys/devices/soc0/family file not found</span>
<span class="go">2023/06/22 14:25:38 Starting GRPC server for &#39;nvidia.com/gpu&#39;</span>
<span class="go">2023/06/22 14:25:38 Starting to serve &#39;nvidia.com/gpu&#39; on /var/lib/kubelet/device-plugins/nvidia-gpu.sock</span>
<span class="go">2023/06/22 14:25:38 Registered device plugin for &#39;nvidia.com/gpu&#39; with Kubelet</span>
</pre></div>
</div>
</li>
<li><p>You can also verify that the node exposes the <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu</span></code> resources in its capacity:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get node -o json <span class="p">|</span> jq -r <span class="s1">&#39;.items[0].status.capacity&#39;</span>
</pre></div>
</div>
<p><strong>Example Output</strong></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &quot;cpu&quot;: &quot;48&quot;,</span>
<span class="go">  &quot;ephemeral-storage&quot;: &quot;142063152Ki&quot;,</span>
<span class="go">  &quot;hugepages-1Gi&quot;: &quot;0&quot;,</span>
<span class="go">  &quot;hugepages-2Mi&quot;: &quot;0&quot;,</span>
<span class="go">  &quot;memory&quot;: &quot;196686216Ki&quot;,</span>
<span class="go">  &quot;nvidia.com/gpu&quot;: &quot;1&quot;,</span>
<span class="go">  &quot;pods&quot;: &quot;250&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="running-a-gpu-accelerated-workload-on-red-hat-device-edge">
<h2>Running a GPU-Accelerated Workload on Red Hat Device Edge<a class="headerlink" href="#running-a-gpu-accelerated-workload-on-red-hat-device-edge" title="Permalink to this headline"></a></h2>
<p>You can run a test workload to verify that the configuration is correct. A simple workload is the CUDA vectorAdd program that NVIDIA provides in a container image.</p>
<ol class="arabic">
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">test</span></code> namespace:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc create namespace <span class="nb">test</span>
</pre></div>
</div>
</li>
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">pod-cuda-vector-add.yaml</span></code>, with a pod specification. Note the <code class="docutils literal notranslate"><span class="pre">spec.containers[0].resources.limits</span></code> field where the <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu</span></code> resource specifies a value of <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat &lt;&lt; EOF &gt; pod-cuda-vector-add.yaml
<span class="go">---</span>
<span class="go">apiVersion: v1</span>
<span class="go">kind: Pod</span>
<span class="go">metadata:</span>
<span class="go">  name: test-cuda-vector-add</span>
<span class="go">  namespace: test</span>
<span class="go">spec:</span>
<span class="go">  restartPolicy: OnFailure</span>
<span class="go">  containers:</span>
<span class="go">  - name: cuda-vector-add</span>
<span class="go">    image: &quot;nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubi8&quot;</span>
<span class="go">    resources:</span>
<span class="go">      limits:</span>
<span class="go">        nvidia.com/gpu: 1</span>
<span class="go">    securityContext:</span>
<span class="go">      allowPrivilegeEscalation: false</span>
<span class="go">      capabilities:</span>
<span class="go">        drop: [&quot;ALL&quot;]</span>
<span class="go">      runAsNonRoot: true</span>
<span class="go">      seccompProfile:</span>
<span class="go">        type: &quot;RuntimeDefault&quot;</span>
<span class="go">EOF</span>
</pre></div>
</div>
</li>
<li><p>Create the pod:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc apply -f pod-cuda-vector-add.yaml
</pre></div>
</div>
</li>
<li><p>Verify the pod log has found a CUDA device:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc logs -n <span class="nb">test</span> test-cuda-vector-add
</pre></div>
</div>
<p><strong>Example Output</strong></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">[Vector addition of 50000 elements]</span>
<span class="go">Copy input data from the host memory to the CUDA device</span>
<span class="go">CUDA kernel launch with 196 blocks of 256 threads</span>
<span class="go">Copy output data from the CUDA device to the host memory</span>
<span class="go">Test PASSED</span>
<span class="go">Done</span>
</pre></div>
</div>
</li>
<li><p>Undeploy the pods in the <code class="docutils literal notranslate"><span class="pre">pod-cuda-vector-add.yaml</span></code> file:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc delete -f pod-cuda-vector-add.yaml
</pre></div>
</div>
</li>
<li><p>Delete the <code class="docutils literal notranslate"><span class="pre">test</span></code> namespace:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc delete ns <span class="nb">test</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="NVIDIA Cloud Native Reference Architectures" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="anthos-guide.html" class="btn btn-neutral float-right" title="NVIDIA GPUs with Google Anthos" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
<img src="_static/NVIDIA-LogoBlack.svg" class="only-light"/>
<img src="_static/NVIDIA-LogoWhite.svg" class="only-dark"/>
<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>
<p>
  Copyright &#169; 2020-2024, NVIDIA Corporation.
</p>

    <p>
      <span class="lastupdated">Last updated on Nov 13, 2024.
      </span></p>
<script type="text/javascript">if (typeof _satellite !== "undefined"){ _satellite.pageBottom();}</script>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>