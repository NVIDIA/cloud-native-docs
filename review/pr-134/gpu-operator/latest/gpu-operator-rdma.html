<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>GPUDirect RDMA and GPUDirect Storage &mdash; NVIDIA GPU Operator 24.9.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" type="text/css" />
      <link rel="stylesheet" href="_static/omni-style.css" type="text/css" />
      <link rel="stylesheet" href="_static/api-styles.css" type="text/css" />
      <link rel="stylesheet" href="_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/mermaid-init.js"></script>
        <script src="_static/clipboard.min.js"></script>
        <script src="_static/copybutton.js"></script>
        <script src="_static/design-tabs.js"></script>
        <script src="_static/version.js"></script>
        <script src="_static/social-media.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Considerations when Installing with Outdated Kernels in Cluster" href="install-gpu-operator-outdated-kernels.html" />
    <link rel="prev" title="Time-Slicing GPUs in Kubernetes" href="gpu-sharing.html" />
 
<script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html">
            <img src="_static/nvidia-logo-white.png" class="logo" alt="Logo"/>
          </a>

<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="overview.html">About the Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="upgrade.html">Upgrade</a></li>
<li class="toctree-l1"><a class="reference internal" href="uninstall.html">Uninstall</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-vgpu.html">Using NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Operator Configuration</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-sharing.html">Time-Slicing GPUs</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-outdated-kernels.html">Outdated Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom-driver-params.html">Custom GPU Driver Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="precompiled-drivers.html">Precompiled Driver Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-configuration.html">GPU Driver CRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="cdi.html">Container Device Interface Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Sandboxed Workloads</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kubevirt.html">KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kata.html">Kata Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-confidential-containers.html">Confidential Containers and Kata</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Specialized Networks</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-proxy.html">HTTP Proxy</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-air-gapped.html">Air-Gapped Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-service-mesh.html">Service Mesh</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CSP configurations</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="amazon-eks.html">Amazon EKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="microsoft-aks.html">Azure AKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="google-gke.html">Google GKE</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA GPU Operator</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
<div> <!-- class="omni-version-warning" -->
  <p class="omni-version-warning-content"> Upgrade to NVIDIA Container Toolkit v1.16.2 or higher, or GPU Operator v24.6.2 or higher to install a critical security update.<br/>
  Refer to <a href="https://nvidia.custhelp.com/app/answers/detail/a_id/5582">Security Bulletin: NVIDIA Container Toolkit - September 2024</a> for more information.</p>
</div>

<li>
    <a href="https://docs.nvidia.com">NVIDIA Docs Hub</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>
    <a href="https://docs.nvidia.com/datacenter/cloud-native/">NVIDIA Cloud Native Technologies</a>
    <i class="fa fa-chevron-right" aria-hidden="true"></i>
</li>
<li>GPUDirect RDMA and GPUDirect Storage</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="gpudirect-rdma-and-gpudirect-storage">
<span id="operator-rdma"></span><h1>GPUDirect RDMA and GPUDirect Storage<a class="headerlink" href="#gpudirect-rdma-and-gpudirect-storage" title="Permalink to this headline">ÔÉÅ</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#about-gpudirect-rdma-and-gpudirect-storage" id="id2">About GPUDirect RDMA and GPUDirect Storage</a></p></li>
<li><p><a class="reference internal" href="#common-prerequisites" id="id3">Common Prerequisites</a></p></li>
<li><p><a class="reference internal" href="#configuring-gpudirect-rdma" id="id4">Configuring GPUDirect RDMA</a></p>
<ul>
<li><p><a class="reference internal" href="#platform-support" id="id5">Platform Support</a></p></li>
<li><p><a class="reference internal" href="#installing-the-gpu-operator-and-enabling-gpudirect-rdma" id="id6">Installing the GPU Operator and Enabling GPUDirect RDMA</a></p></li>
<li><p><a class="reference internal" href="#verifying-the-installation-of-gpudirect-with-rdma" id="id7">Verifying the Installation of GPUDirect with RDMA</a></p></li>
<li><p><a class="reference internal" href="#verifying-the-installation-by-performing-a-data-transfer" id="id8">Verifying the Installation by Performing a Data Transfer</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#using-gpudirect-storage" id="id9">Using GPUDirect Storage</a></p>
<ul>
<li><p><a class="reference internal" href="#id1" id="id10">Platform Support</a></p></li>
<li><p><a class="reference internal" href="#installing-the-gpu-operator-and-enabling-gpudirect-storage" id="id11">Installing the GPU Operator and Enabling GPUDirect Storage</a></p></li>
<li><p><a class="reference internal" href="#verification" id="id12">Verification</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#related-information" id="id13">Related Information</a></p></li>
</ul>
</div>
<section id="about-gpudirect-rdma-and-gpudirect-storage">
<h2>About GPUDirect RDMA and GPUDirect Storage<a class="headerlink" href="#about-gpudirect-rdma-and-gpudirect-storage" title="Permalink to this headline">ÔÉÅ</a></h2>
<p><a class="reference external" href="https://docs.nvidia.com/cuda/gpudirect-rdma/index.html">GPUDirect RDMA</a> is a technology in NVIDIA GPUs that enables direct
data exchange between GPUs and a third-party peer device using PCI Express. The third-party devices could be network interfaces
such as NVIDIA ConnectX SmartNICs or BlueField DPUs, or video acquisition adapters.</p>
<p><a class="reference external" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">GPUDirect Storage</a> (GDS) enables a direct data path between local or remote storage, such as NFS servers or NVMe/NVMe over Fabric (NVMe-oF), and GPU memory.
GDS performs direct memory access (DMA) transfers between GPU memory and storage.
DMA avoids a bounce buffer through the CPU.
This direct path increases system bandwidth and decreases the latency and utilization load on the CPU.</p>
<p>To support GPUDirect RDMA, userspace CUDA APIs are required.
The kernel mode support is provided by one of two approaches: DMA-BUF from the Linux kernel or the legacy <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module.
NVIDIA recommends using the DMA-BUF rather than using the <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module from the GPU Driver.</p>
<p>Starting with v23.9.1 of the Operator, the Operator uses GDS driver version 2.17.5 or newer.
This version and higher is only supported with the NVIDIA Open GPU Kernel module driver.
The sample commands for installing the Operator include the <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">useOpenKernelModules=true</span></code>
command-line argument for Helm.</p>
<p>In conjunction with the Network Operator, the GPU Operator can be used to
set up the networking related components such as network device kernel drivers and Kubernetes device plugins to enable
workloads to take advantage of GPUDirect RDMA and GPUDirect Storage.
Refer to the Network Operator <a class="reference external" href="https://docs.nvidia.com/networking/software/cloud-orchestration/index.html">documentation</a> for installation information.</p>
</section>
<section id="common-prerequisites">
<h2>Common Prerequisites<a class="headerlink" href="#common-prerequisites" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>The prerequisites for configuring GPUDirect RDMA or GPUDirect Storage depend on whether you use DMA-BUF from the Linux kernel or the legacy <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module.</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 40%" />
<col style="width: 40%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"><p>Technology</p></th>
<th class="head"><p>DMA-BUF</p></th>
<th class="head"><p>Legacy NVIDIA-peermem</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p>GPU Driver</p></th>
<td><p>An Open Kernel module driver is required.</p></td>
<td><p>Any supported driver.</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>CUDA</p></th>
<td><p>CUDA 11.7 or higher.
The CUDA runtime is provided by the driver.</p></td>
<td><p>No minimum version.
The CUDA runtime is provided by the driver.</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>GPU</p></th>
<td><p>Turing architecture data center, Quadro RTX, and RTX GPU or higher.</p></td>
<td><p>All data center, Quadro RTX, and RTX GPU or higher.</p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>Network Device Drivers</p></th>
<td><p>MLNX_OFED or DOCA-OFED are optional.
You can use the Linux driver packages from the package manager.</p></td>
<td><p>MLNX_OFED or DOCA-OFED are required.</p></td>
</tr>
<tr class="row-even"><th class="stub"><p>Linux Kernel</p></th>
<td><p>5.12 or higher.</p></td>
<td><p>No minimum version.</p></td>
</tr>
</tbody>
</table>
<ul>
<li><p>Make sure the network device drivers are installed.</p>
<p>You can use the <a class="reference external" href="https://docs.nvidia.com/networking/software/cloud-orchestration/index.html">Network Operator</a>
to manage the driver lifecycle for MLNX_OFED and DOCA-OFED drivers.</p>
<p>You can install the drivers on each host.
Refer to <a class="reference external" href="https://docs.nvidia.com/networking/software/adapter-software/index.html">Adapter Software</a>
in the networking documentation for information about the MLNX_OFED, DOCA-OFED, and Linux inbox drivers.</p>
</li>
<li><p>For installations on VMware vSphere, refer to the following additional prerequisites:</p>
<ul>
<li><p>Make sure the network interface controller and the NVIDIA GPU are in the same PCIe IO root complex.</p></li>
<li><p>Enable the following PCI options:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pciPassthru.allowP2P</span> <span class="pre">=</span> <span class="pre">true</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pciPassthru.RelaxACSforP2P</span> <span class="pre">=</span> <span class="pre">true</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pciPassthru.use64bitMMIO</span> <span class="pre">=</span> <span class="pre">true</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pciPassthru.64bitMMIOSizeGB</span> <span class="pre">=</span> <span class="pre">128</span></code></p></li>
</ul>
<p>For information about configuring the settings, refer to the
<a class="reference external" href="https://core.vmware.com/resource/deploy-ai-ready-vsphere-7#vm-settings-A">Deploy an AI-Ready Enterprise Platform on vSphere 7</a>
document from VMWare.</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="configuring-gpudirect-rdma">
<h2>Configuring GPUDirect RDMA<a class="headerlink" href="#configuring-gpudirect-rdma" title="Permalink to this headline">ÔÉÅ</a></h2>
<section id="platform-support">
<h3>Platform Support<a class="headerlink" href="#platform-support" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The following platforms are supported for GPUDirect with RDMA:</p>
<ul class="simple">
<li><p>Kubernetes on bare metal and on vSphere VMs with GPU passthrough and vGPU.</p></li>
<li><p>VMware vSphere with Tanzu.</p></li>
<li><p>For Red Hat OpenShift Container Platform on bare metal and on vSphere VMs with GPU passthrough and vGPU configurations,
refer to <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/nvaie-with-ocp.html#nvaie-ocp" title="(in NVIDIA GPU Operator on Red Hat OpenShift Container Platform)"><span class="xref std std-ref">NVIDIA AI Enterprise with OpenShift</span></a>.</p></li>
</ul>
<p>For information about the supported versions, refer to <a class="reference internal" href="platform-support.html#support-for-gpudirect-rdma"><span class="std std-ref">Support for GPUDirect RDMA</span></a> on the platform support page.</p>
</section>
<section id="installing-the-gpu-operator-and-enabling-gpudirect-rdma">
<h3>Installing the GPU Operator and Enabling GPUDirect RDMA<a class="headerlink" href="#installing-the-gpu-operator-and-enabling-gpudirect-rdma" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>To use DMA-BUF and network device drivers that are installed by the Network Operator:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
     -n gpu-operator --create-namespace <span class="se">\</span>
     nvidia/gpu-operator <span class="se">\</span>
     --version<span class="o">=</span>v24.9.1 <span class="se">\</span>
     --set driver.useOpenKernelModules<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p>To use DMA-BUF and network device drivers that are installed on the host:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
     -n gpu-operator --create-namespace <span class="se">\</span>
     nvidia/gpu-operator <span class="se">\</span>
     --version<span class="o">=</span>v24.9.1 <span class="se">\</span>
     --set driver.useOpenKernelModules<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
     --set driver.rdma.useHostMofed<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p>To use the legacy <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module instead of DMA-BUF, add <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">driver.rdma.enabled=true</span></code> to either of the preceding commands.
The <code class="docutils literal notranslate"><span class="pre">driver.useOpenKernelModules=true</span></code> argument is optional for using the legacy kernel driver.</p>
</section>
<section id="verifying-the-installation-of-gpudirect-with-rdma">
<h3>Verifying the Installation of GPUDirect with RDMA<a class="headerlink" href="#verifying-the-installation-of-gpudirect-with-rdma" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>During the installation, the NVIDIA driver daemon set runs an <cite>init container</cite> to wait on the network device kernel drivers to be ready.
This init container checks for Mellanox NICs on the node and ensures that the necessary kernel symbols are exported by the kernel drivers.</p>
<p>If you were required to use the <code class="docutils literal notranslate"><span class="pre">driver.rdma.enabled=true</span></code> argument when you installed the Operator, the nvidia-peermem-ctr container is started inside each driver pod after the verification.</p>
<ol class="arabic">
<li><p>Confirm that the pod template for the driver daemon set includes the mofed-validation init container and
the nvidia-driver-ctr containers:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl describe ds -n gpu-operator nvidia-driver-daemonset
</pre></div>
</div>
<p><em>Example Output</em></p>
<p>The following partial output omits the init containers and containers that are common to all installations.</p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">...</span>
<span class="go"> Init Containers:</span>
<span class="go">  mofed-validation:</span>
<span class="go">  Container ID:  containerd://5a36c66b43f676df616e25ba7ae0c81aeaa517308f28ec44e474b2f699218de3</span>
<span class="go">  Image:         nvcr.io/nvidia/cloud-native/gpu-operator-validator:v1.8.1</span>
<span class="go">  Image ID:      nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:7a70e95fd19c3425cd4394f4b47bbf2119a70bd22d67d72e485b4d730853262c</span>
<span class="go">...</span>
<span class="go"> Containers:</span>
<span class="go">  nvidia-driver-ctr:</span>
<span class="go">  Container ID:  containerd://199a760946c55c3d7254fa0ebe6a6557dd231179057d4909e26c0e6aec49ab0f</span>
<span class="go">  Image:         nvcr.io/nvaie/vgpu-guest-driver:470.63.01-ubuntu20.04</span>
<span class="go">  Image ID:      nvcr.io/nvaie/vgpu-guest-driver@sha256:a1b7d2c8e1bad9bb72d257ddfc5cec341e790901e7574ba2c32acaddaaa94625</span>
<span class="go">...</span>
<span class="go">  nvidia-peermem-ctr:</span>
<span class="go">  Container ID:  containerd://0742d86f6017bf0c304b549ebd8caad58084a4185a1225b2c9a7f5c4a171054d</span>
<span class="go">  Image:         nvcr.io/nvaie/vgpu-guest-driver:470.63.01-ubuntu20.04</span>
<span class="go">  Image ID:      nvcr.io/nvaie/vgpu-guest-driver@sha256:a1b7d2c8e1bad9bb72d257ddfc5cec341e790901e7574ba2c32acaddaaa94625</span>
<span class="go">...</span>
</pre></div>
</div>
<p>The nvidia-peermem-ctr container is present only if you were required to specify the <code class="docutils literal notranslate"><span class="pre">driver.rdma.enabled=true</span></code> argument when you installed the Operator.</p>
</li>
<li><p>Legacy only: Confirm that the nvidia-peermem-ctr container successfully loaded the nvidia-peermem kernel module:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl logs -n gpu-operator ds/nvidia-driver-daemonset -c nvidia-peermem-ctr
</pre></div>
</div>
<p>Alternatively, run <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">logs</span> <span class="pre">-n</span> <span class="pre">gpu-operator</span> <span class="pre">nvidia-driver-daemonset-xxxxx</span> <span class="pre">-c</span> <span class="pre">nvidia-peermem-ctr</span></code> for each pod in the daemonset.</p>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">waiting for mellanox ofed and nvidia drivers to be installed</span>
<span class="go">waiting for mellanox ofed and nvidia drivers to be installed</span>
<span class="go">successfully loaded nvidia-peermem module</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="verifying-the-installation-by-performing-a-data-transfer">
<h3>Verifying the Installation by Performing a Data Transfer<a class="headerlink" href="#verifying-the-installation-by-performing-a-data-transfer" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>You can perform the following steps to verify that GPUDirect with RDMA is configured
correctly and that pods can perform RDMA data transfers.</p>
<ol class="arabic">
<li><p>Get the network interface name of the InfiniBand device on the host:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl <span class="nb">exec</span> -it -n network-operator mofed-ubuntu22.04-ds-xxxxx -- ibdev2netdev
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">mlx5_0 port 1 ==&gt; ens64np1 (Up)</span>
</pre></div>
</div>
</li>
<li><p>Configure a secondary network on the device using a macvlan network attachment:</p>
<ul>
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">demo-macvlannetwork.yaml</span></code>, with contents like the following example:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mellanox.com/v1alpha1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">MacvlanNetwork</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-macvlannetwork</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">networkNamespace</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;default&quot;</span><span class="w"></span>
<span class="hll"><span class="nt">master</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;ens64np1&quot;</span><span class="w"></span>
</span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;bridge&quot;</span><span class="w"></span>
<span class="nt">mtu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1500</span><span class="w"></span>
<span class="nt">ipam</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span><span class="w"></span>
<span class="w">  </span><span class="no">{</span><span class="w"></span>
<span class="w">    </span><span class="no">&quot;type&quot;: &quot;whereabouts&quot;,</span><span class="w"></span>
<span class="w">    </span><span class="no">&quot;range&quot;: &quot;192.168.2.225/28&quot;,</span><span class="w"></span>
<span class="w">    </span><span class="no">&quot;exclude&quot;: [</span><span class="w"></span>
<span class="w">      </span><span class="no">&quot;192.168.2.229/30&quot;,</span><span class="w"></span>
<span class="w">      </span><span class="no">&quot;192.168.2.236/32&quot;</span><span class="w"></span>
<span class="w">    </span><span class="no">]</span><span class="w"></span>
<span class="w">  </span><span class="no">}</span><span class="w"></span>
</pre></div>
</div>
<p>Replace <code class="docutils literal notranslate"><span class="pre">ens64np1</span></code> with the the network interface name reported by the <code class="docutils literal notranslate"><span class="pre">ibdev2netdev</span></code> command
from the preceding step.</p>
</li>
<li><p>Apply the manifest:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f demo-macvlannetwork.yaml
</pre></div>
</div>
</li>
<li><p>Confirm that the additional network is ready:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get macvlannetworks demo-macvlannetwork
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                  STATUS   AGE</span>
<span class="go">demo-macvlannetwork   ready    2023-03-10T18:22:28Z</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Start two pods that run the <code class="docutils literal notranslate"><span class="pre">mellanox/cuda-perftest</span></code> container on two different nodes in the cluster.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="b5f13e1d-99cc-459c-85c9-db24600f17ab" name="9925f680-c33e-4c8e-b383-027898b3a376" type="radio">
</input><label class="sd-tab-label" for="b5f13e1d-99cc-459c-85c9-db24600f17ab">
demo-pod-1</label><div class="sd-tab-content docutils">
<ul>
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">demo-pod-1.yaml</span></code>, for the first pod with contents like the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="hll"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-pod-1</span><span class="w"></span>
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">k8s.v1.cni.cncf.io/networks</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-macvlannetwork</span><span class="w"></span>
<span class="w">    </span><span class="c1"># If a network with static IPAM is used replace network annotation with the below.</span><span class="w"></span>
<span class="w">    </span><span class="c1"># k8s.v1.cni.cncf.io/networks: &#39;[</span><span class="w"></span>
<span class="w">    </span><span class="c1">#   { &quot;name&quot;: &quot;rdma-net&quot;,</span><span class="w"></span>
<span class="w">    </span><span class="c1">#     &quot;ips&quot;: [&quot;192.168.111.101/24&quot;],</span><span class="w"></span>
<span class="w">    </span><span class="c1">#     &quot;gateway&quot;: [&quot;192.168.111.1&quot;]</span><span class="w"></span>
<span class="w">    </span><span class="c1">#   }</span><span class="w"></span>
<span class="w">    </span><span class="c1"># ]&#39;</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="c1"># Note: Replace hostname or remove selector altogether</span><span class="w"></span>
<span class="hll"><span class="w">    </span><span class="nt">kubernetes.io/hostname</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvnode1</span><span class="w"></span>
</span><span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OnFailure</span><span class="w"></span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mellanox/cuda-perftest</span><span class="w"></span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rdma-gpu-test-ctr</span><span class="w"></span>
<span class="w">    </span><span class="nt">securityContext</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">capabilities</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">add</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="s">&quot;IPC_LOCK&quot;</span><span class="w"> </span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">limits</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">        </span><span class="nt">rdma/rdma_shared_device_a</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">      </span><span class="nt">requests</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">        </span><span class="nt">rdma/rdma_shared_device_a</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Apply the manifest:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f demo-pod-1.yaml
</pre></div>
</div>
</li>
</ul>
</div>
<input id="5a65f4a8-d5f2-4b20-b275-5732498da965" name="9925f680-c33e-4c8e-b383-027898b3a376" type="radio">
</input><label class="sd-tab-label" for="5a65f4a8-d5f2-4b20-b275-5732498da965">
demo-pod-2</label><div class="sd-tab-content docutils">
<ul>
<li><p>Create a file, such as <code class="docutils literal notranslate"><span class="pre">demo-pod-2.yaml</span></code>, for the second pod with contents like the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="hll"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-pod-2</span><span class="w"></span>
</span><span class="w">  </span><span class="nt">annotations</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">k8s.v1.cni.cncf.io/networks</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">demo-macvlannetwork</span><span class="w"></span>
<span class="w">    </span><span class="c1"># If a network with static IPAM is used replace network annotation with the below.</span><span class="w"></span>
<span class="w">    </span><span class="c1"># k8s.v1.cni.cncf.io/networks: &#39;[</span><span class="w"></span>
<span class="w">    </span><span class="c1">#   { &quot;name&quot;: &quot;rdma-net&quot;,</span><span class="w"></span>
<span class="w">    </span><span class="c1">#     &quot;ips&quot;: [&quot;192.168.111.101/24&quot;],</span><span class="w"></span>
<span class="w">    </span><span class="c1">#     &quot;gateway&quot;: [&quot;192.168.111.1&quot;]</span><span class="w"></span>
<span class="w">    </span><span class="c1">#   }</span><span class="w"></span>
<span class="w">    </span><span class="c1"># ]&#39;</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">nodeSelector</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="c1"># Note: Replace hostname or remove selector altogether</span><span class="w"></span>
<span class="hll"><span class="w">    </span><span class="nt">kubernetes.io/hostname</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvnode2</span><span class="w"></span>
</span><span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">OnFailure</span><span class="w"></span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mellanox/cuda-perftest</span><span class="w"></span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rdma-gpu-test-ctr</span><span class="w"></span>
<span class="w">    </span><span class="nt">securityContext</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">capabilities</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">add</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="s">&quot;IPC_LOCK&quot;</span><span class="w"> </span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">limits</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">        </span><span class="nt">rdma/rdma_shared_device_a</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">      </span><span class="nt">requests</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">        </span><span class="nt">rdma/rdma_shared_device_a</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Apply the manifest:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f demo-pod-2.yaml
</pre></div>
</div>
</li>
</ul>
</div>
</div>
</li>
<li><p>Get the IP addresses of the pods:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pods -o wide
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME         READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE   READINESS GATES</span>
<span class="go">demo-pod-1   1/1     Running   0          3d4h   192.168.38.90   nvnode1   &lt;none&gt;           &lt;none&gt;</span>
<span class="go">demo-pod-2   1/1     Running   0          3d4h   192.168.47.89   nvnode2   &lt;none&gt;           &lt;none&gt;</span>
</pre></div>
</div>
</li>
<li><p>From one terminal, open a shell in the container on the first pod and start the performance test server:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl <span class="nb">exec</span> -it demo-pod-1 -- ib_write_bw --use_cuda<span class="o">=</span><span class="m">0</span> --use_cuda_dmabuf <span class="se">\</span>
    -d mlx5_0 -a -F --report_gbits -q <span class="m">1</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">************************************</span>
<span class="go">* Waiting for client to connect... *</span>
<span class="go">************************************</span>
</pre></div>
</div>
</li>
<li><p>From another terminal, open a shell in the container on the second pod and run the performance client:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl <span class="nb">exec</span> -it demo-pod-2 -- ib_write_bw -n <span class="m">5000</span> --use_cuda<span class="o">=</span><span class="m">0</span> --use_cuda_dmabuf <span class="se">\</span>
    -d mlx5_0 -a -F --report_gbits -q <span class="m">1</span> <span class="m">192</span>.168.38.90
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go"> ---------------------------------------------------------------------------------------</span>
<span class="go">                    RDMA_Write BW Test</span>
<span class="go"> Dual-port       : OFF          Device         : mlx5_0</span>
<span class="go"> Number of qps   : 1            Transport type : IB</span>
<span class="go"> Connection type : RC           Using SRQ      : OFF</span>
<span class="go"> PCIe relax order: ON</span>
<span class="go"> ibv_wr* API     : ON</span>
<span class="go"> TX depth        : 128</span>
<span class="go"> CQ Moderation   : 100</span>
<span class="go"> Mtu             : 1024[B]</span>
<span class="go"> Link type       : Ethernet</span>
<span class="go"> GID index       : 5</span>
<span class="go"> Max inline data : 0[B]</span>
<span class="go"> rdma_cm QPs     : OFF</span>
<span class="go"> Data ex. method : Ethernet</span>
<span class="go">---------------------------------------------------------------------------------------</span>
<span class="go"> local address: LID 0000 QPN 0x01ac PSN 0xc76db1 RKey 0x23beb2 VAddr 0x007f26a2c8b000</span>
<span class="go"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:226</span>
<span class="go"> remote address: LID 0000 QPN 0x01a9 PSN 0x2f722 RKey 0x23beaf VAddr 0x007f820b24f000</span>
<span class="go"> GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:02:225</span>
<span class="go">---------------------------------------------------------------------------------------</span>
<span class="go"> #bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]   MsgRate[Mpps]</span>
<span class="go"> 2          5000             0.11               0.11               6.897101</span>
<span class="go"> 4          5000             0.22               0.22               6.995646</span>
<span class="go"> 8          5000             0.45               0.45               7.014752</span>
<span class="go"> 16         5000             0.90               0.90               7.017509</span>
<span class="go"> 32         5000             1.80               1.80               7.020162</span>
<span class="go"> 64         5000             3.59               3.59               7.007110</span>
<span class="go"> 128        5000             7.19               7.18               7.009540</span>
<span class="go"> 256        5000             15.06              14.98              7.313517</span>
<span class="go"> 512        5000             30.04              29.73              7.259329</span>
<span class="go"> 1024       5000             59.65              58.81              7.178529</span>
<span class="go"> 2048       5000             91.53              91.47              5.582931</span>
<span class="go"> 4096       5000             92.13              92.06              2.809574</span>
<span class="go"> 8192       5000             92.35              92.31              1.408535</span>
<span class="go"> 16384      5000             92.46              92.46              0.705381</span>
<span class="go"> 32768      5000             92.36              92.35              0.352302</span>
<span class="go"> 65536      5000             92.39              92.38              0.176196</span>
<span class="go"> 131072     5000             92.42              92.41              0.088131</span>
<span class="go"> 262144     5000             92.45              92.44              0.044080</span>
<span class="go"> 524288     5000             92.42              92.42              0.022034</span>
<span class="go"> 1048576    5000             92.40              92.40              0.011015</span>
<span class="go"> 2097152    5000             92.40              92.39              0.005507</span>
<span class="go"> 4194304    5000             92.40              92.39              0.002753</span>
<span class="go"> 8388608    5000             92.39              92.39              0.001377</span>
<span class="go">---------------------------------------------------------------------------------------</span>
</pre></div>
</div>
<p>The command output indicates that the data transfer rate was approximately 92 Gbps.</p>
</li>
<li><p>Delete the pods:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl delete -f demo-pod-1.yaml -f demo-pod-2.yaml
</pre></div>
</div>
</li>
<li><p>Delete the secondary network:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl delete -f demo-macvlannetworks.yaml
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="using-gpudirect-storage">
<h2>Using GPUDirect Storage<a class="headerlink" href="#using-gpudirect-storage" title="Permalink to this headline">ÔÉÅ</a></h2>
<section id="id1">
<h3>Platform Support<a class="headerlink" href="#id1" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>See <a class="reference internal" href="platform-support.html#support-for-gpudirect-storage"><span class="std std-ref">Support for GPUDirect Storage</span></a> on the platform support page.</p>
</section>
<section id="installing-the-gpu-operator-and-enabling-gpudirect-storage">
<h3>Installing the GPU Operator and Enabling GPUDirect Storage<a class="headerlink" href="#installing-the-gpu-operator-and-enabling-gpudirect-storage" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>The following section is applicable to the following configurations and describe how to deploy the GPU Operator using the Helm Chart:</p>
<ul class="simple">
<li><p>Kubernetes on bare metal and on vSphere VMs with GPU passthrough and vGPU.</p></li>
</ul>
<p>Starting with v22.9.1, the GPU Operator provides an option to load the <code class="docutils literal notranslate"><span class="pre">nvidia-fs</span></code> kernel module during the bootstrap of the NVIDIA driver daemon set.
Starting with v23.9.1, the GPU Operator deploys a version of GDS that requires using the NVIDIA Open Kernel module driver.</p>
<p>The following sample command applies to clusters that use the Network Operator to install the network device kernel drivers.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
     -n gpu-operator --create-namespace <span class="se">\</span>
     nvidia/gpu-operator <span class="se">\</span>
     --version<span class="o">=</span>v24.9.1 <span class="se">\</span>
     --set driver.useOpenKernelModules<span class="o">=</span><span class="nb">true</span> <span class="se">\</span>
     --set gds.enabled<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p>Add <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">driver.rdma.enabled=true</span></code> to the command to use the legacy <code class="docutils literal notranslate"><span class="pre">nvidia-peermem</span></code> kernel module.</p>
</section>
<section id="verification">
<h3>Verification<a class="headerlink" href="#verification" title="Permalink to this headline">ÔÉÅ</a></h3>
<p>During the installation, an init container is used with the driver daemon set to wait on the network device kernel drivers to be ready.
This init container checks for Mellanox NICs on the node and ensures that the necessary kernel symbols are exported by the kernel drivers.
After the verification completes, the nvidia-fs-ctr container starts inside the driver pods.</p>
<p>If you were required to use the <code class="docutils literal notranslate"><span class="pre">driver.rdma.enabled=true</span></code> argument when you installed the Operator, the nvidia-peermem-ctr container is started inside each driver pod after the verification.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pod -n gpu-operator
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">gpu-operator   gpu-feature-discovery-pktzg                                       1/1     Running     0          11m</span>
<span class="go">gpu-operator   gpu-operator-1672257888-node-feature-discovery-master-7ccb7txmc   1/1     Running     0          12m</span>
<span class="go">gpu-operator   gpu-operator-1672257888-node-feature-discovery-worker-bqhrl       1/1     Running     0          11m</span>
<span class="go">gpu-operator   gpu-operator-6f64c86bc-zjqdh                                      1/1     Running     0          12m</span>
<span class="go">gpu-operator   nvidia-container-toolkit-daemonset-rgwqg                          1/1     Running     0          11m</span>
<span class="go">gpu-operator   nvidia-cuda-validator-8whvt                                       0/1     Completed   0          8m50s</span>
<span class="go">gpu-operator   nvidia-dcgm-exporter-pt9q9                                        1/1     Running     0          11m</span>
<span class="go">gpu-operator   nvidia-device-plugin-daemonset-472fc                              1/1     Running     0          11m</span>
<span class="go">gpu-operator   nvidia-device-plugin-validator-29nhc                              0/1     Completed   0          8m34s</span>
<span class="go">gpu-operator   nvidia-driver-daemonset-j9vw6                                     3/3     Running     0          12m</span>
<span class="go">gpu-operator   nvidia-mig-manager-mtjcw                                          1/1     Running     0          7m35s</span>
<span class="go">gpu-operator   nvidia-operator-validator-b8nz2                                   1/1     Running     0          11m</span>
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl describe pod -n gpu-operator nvidia-driver-daemonset-xxxx
<span class="go">&lt;snip&gt;</span>
<span class="go"> Init Containers:</span>
<span class="go">  mofed-validation:</span>
<span class="go">   Container ID:  containerd://a31a8c16ce7596073fef7cb106da94c452fdff111879e7fc3ec58b9cef83856a</span>
<span class="go">   Image:         nvcr.io/nvidia/cloud-native/gpu-operator-validator:v22.9.1</span>
<span class="go">   Image ID:      nvcr.io/nvidia/cloud-native/gpu-operator-validator@sha256:18c9ea88ae06d479e6657b8a4126a8ee3f4300a40c16ddc29fb7ab3763d46005</span>

<span class="go"> &lt;snip&gt;</span>
<span class="go"> Containers:</span>
<span class="go">  nvidia-driver-ctr:</span>
<span class="go">   Container ID:  containerd://7cf162e4ee4af865c0be2023d61fbbf68c828d396207e7eab2506f9c2a5238a4</span>
<span class="go">   Image:         nvcr.io/nvidia/driver:525.60.13-ubuntu20.04</span>
<span class="go">   Image ID:      nvcr.io/nvidia/driver@sha256:0ee0c585fa720f177734b3295a073f402d75986c1fe018ae68bd73fe9c21b8d8</span>


<span class="go">  &lt;snip&gt;</span>
<span class="go">  nvidia-peermem-ctr:</span>
<span class="go">   Container ID:  containerd://5c71c9f8ccb719728a0503500abecfb5423e8088f474d686ee34b5fe3746c28e</span>
<span class="go">   Image:         nvcr.io/nvidia/driver:525.60.13-ubuntu20.04</span>
<span class="go">   Image ID:      nvcr.io/nvidia/driver@sha256:0ee0c585fa720f177734b3295a073f402d75986c1fe018ae68bd73fe9c21b8d8</span>

<span class="go">  &lt;snip&gt;</span>
<span class="go">  nvidia-fs-ctr:</span>
<span class="go">   Container ID:  containerd://f5c597d59e1cf8747aa20b8c229a6f6edd3ed588b9d24860209ba0cc009c0850</span>
<span class="go">   Image:         nvcr.io/nvidia/cloud-native/nvidia-fs:2.14.13-ubuntu20.04</span>
<span class="go">   Image ID:      nvcr.io/nvidia/cloud-native/nvidia-fs@sha256:109485365f68caeaee1edee0f3f4d722fe5b5d7071811fc81c630c8a840b847b</span>

<span class="go"> &lt;snip&gt;</span>
</pre></div>
</div>
<p>Lastly, verify that NVIDIA kernel modules are loaded on the worker node:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lsmod <span class="p">|</span> grep nvidia

<span class="go">nvidia_fs             245760  0</span>
<span class="go">nvidia_peermem         16384  0</span>
<span class="go">nvidia_modeset       1159168  0</span>
<span class="go">nvidia_uvm           1048576  0</span>
<span class="go">nvidia              39059456  115 nvidia_uvm,nvidia_modeset</span>
<span class="go">ib_core               319488  9 rdma_cm,ib_ipoib,iw_cm,ib_umad,rdma_ucm,ib_uverbs,mlx5_ib,ib_cm</span>
<span class="go">drm                   491520  6 drm_kms_helper,drm_vram_helper,nvidia,mgag200,ttm</span>
</pre></div>
</div>
</section>
</section>
<section id="related-information">
<h2>Related Information<a class="headerlink" href="#related-information" title="Permalink to this headline">ÔÉÅ</a></h2>
<p>Refer to the following resources for more information:</p>
<blockquote>
<div><ul class="simple">
<li><p>GPUDirect RDMA: <a class="reference external" href="https://docs.nvidia.com/cuda/gpudirect-rdma/index.html">https://docs.nvidia.com/cuda/gpudirect-rdma/index.html</a></p></li>
<li><p>NVIDIA Network Operator: <a class="reference external" href="https://github.com/Mellanox/network-operator">https://github.com/Mellanox/network-operator</a></p></li>
<li><p>Blog post on deploying the Network Operator: <a class="reference external" href="https://developer.nvidia.com/blog/deploying-gpudirect-rdma-on-egx-stack-with-the-network-operator/">https://developer.nvidia.com/blog/deploying-gpudirect-rdma-on-egx-stack-with-the-network-operator/</a></p></li>
</ul>
</div></blockquote>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="gpu-sharing.html" class="btn btn-neutral float-left" title="Time-Slicing GPUs in Kubernetes" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="install-gpu-operator-outdated-kernels.html" class="btn btn-neutral float-right" title="Considerations when Installing with Outdated Kernels in Cluster" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
<img src="_static/NVIDIA-LogoBlack.svg" class="only-light"/>
<img src="_static/NVIDIA-LogoWhite.svg" class="only-dark"/>
<p class="notices">
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">Privacy Policy</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">Manage My Privacy</a>
|
<a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">Do Not Sell or Share My Data</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">Terms of Service</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">Accessibility</a>
|
<a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">Corporate Policies</a>
|
<a href="https://www.nvidia.com/en-us/product-security/" target="_blank">Product Security</a>
|
<a href="https://www.nvidia.com/en-us/contact/" target="_blank">Contact</a>
</p>
<p>
  Copyright &#169; 2020-2024, NVIDIA Corporation.
</p>

    <p>
      <span class="lastupdated">Last updated on Dec 05, 2024.
      </span></p>
<script type="text/javascript">if (typeof _satellite !== "undefined"){ _satellite.pageBottom();}</script>

  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WBRZJBCJEW"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-WBRZJBCJEW', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>