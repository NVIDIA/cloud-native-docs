


<!DOCTYPE html>


<html data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>NVIDIA DRA Driver for GPUs &#8212; NVIDIA GPU Operator</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/nvidia-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/mermaid-init.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/design-tabs.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dra-driver';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = '../versions1.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '25.3.1';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>
    <script src="_static/version.js"></script>
    <script src="_static/social-media.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="GPU Operator with KubeVirt" href="gpu-operator-kubevirt.html" />
    <link rel="prev" title="Container Device Interface Support in the GPU Operator" href="cdi.html" />


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  <meta name="docsearch:version" content="" />
    <meta name="docbuild:last-update" content="Jul 11, 2025"/>



  <script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js" ></script>
  


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA GPU Operator - Home"/>
    <img src="_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="NVIDIA GPU Operator - Home"/>
  
  
    <p class="title logo__title">NVIDIA GPU Operator</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA GPU Operator - Home"/>
    <img src="_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="NVIDIA GPU Operator - Home"/>
  
  
    <p class="title logo__title">NVIDIA GPU Operator</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><p aria-level="2" class="caption" role="heading"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="overview.html">About the Operator</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting-started.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="upgrade.html">Upgrade</a></li>
<li class="toctree-l1"><a class="reference internal" href="uninstall.html">Uninstall</a></li>
<li class="toctree-l1"><a class="reference internal" href="platform-support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-upgrades.html">GPU Driver Upgrades</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-vgpu.html">Using NVIDIA vGPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-nvaie.html">NVIDIA AI Enterprise</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security Considerations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Operator Configuration</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-mig.html">Multi-Instance GPU</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-sharing.html">Time-Slicing GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-rdma.html">GPUDirect RDMA and GPUDirect Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-outdated-kernels.html">Outdated Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="custom-driver-params.html">Custom GPU Driver Parameters</a></li>
<li class="toctree-l1"><a class="reference internal" href="precompiled-drivers.html">Precompiled Driver Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-driver-configuration.html">GPU Driver CRD</a></li>
<li class="toctree-l1"><a class="reference internal" href="cdi.html">Container Device Interface Support</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">NVIDIA DRA Driver for GPUs</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Sandboxed Workloads</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kubevirt.html">KubeVirt</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-kata.html">Kata Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-confidential-containers.html">Confidential Containers and Kata</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Specialized Networks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-proxy.html">HTTP Proxy</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-air-gapped.html">Air-Gapped Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-operator-service-mesh.html">Service Mesh</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">CSP configurations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="amazon-eks.html">Amazon EKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="microsoft-aks.html">Azure AKS</a></li>
<li class="toctree-l1"><a class="reference internal" href="google-gke.html">Google GKE</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    <li class="breadcrumb-item breadcrumb-home">
      <a href="https://docs.nvidia.com">NVIDIA Docs Hub</a>
    </li>
    <li class="breadcrumb-item">
      <a href="https://docs.nvidia.com/datacenter/cloud-native">Cloud Native Technologies</a>
    </li>
    <li class="breadcrumb-item">
      <a href="index.html">NVIDIA GPU Operator</a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">NVIDIA DRA Driver for GPUs</li>
  </ul>
</nav></div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="nvidia-dra-driver-for-gpus">
<h1>NVIDIA DRA Driver for GPUs<a class="headerlink" href="#nvidia-dra-driver-for-gpus" title="Permalink to this headline">#</a></h1>
<p>This page provides an overview of the NVIDIA DRA Driver for GPUs, its supported functionality, instructions for how to install it alongside the GPU Operator, and usage examples.</p>
<p>Before continuing, you should be familiar with the concept of Dynamic Resource Allocation (DRA) in Kubernetes (<a class="reference external" href="https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/">docs</a>).</p>
<section id="supported-use-cases">
<h2>Supported Use Cases<a class="headerlink" href="#supported-use-cases" title="Permalink to this headline">#</a></h2>
<p>The NVIDIA DRA Driver for GPUs v25.3.0 enables allocating the following resources for your workloads:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">ComputeDomain</span></code> for enabling Multi-Node NVLink (MNNVL) workloads on NVIDIA GB200 systems (full support).</p></li>
<li><p>GPUs, as an alternative to the traditional device plugin method (Technology Preview support).</p></li>
</ul>
<section id="multi-node-nvlink-support-for-nvidia-gb200">
<h3>Multi-Node NVLink Support for NVIDIA GB200<a class="headerlink" href="#multi-node-nvlink-support-for-nvidia-gb200" title="Permalink to this headline">#</a></h3>
<p>NVIDIA GB200 systems are designed specifically around Multi-Node NVLink (MNNVL) to turn a rack of GPU machines – each with a small number of GPUs – into a giant supercomputer with up to 72 GPUs communicating at full NVLink bandwidth.</p>
<p>The NVIDIA DRA Driver for GPUs supports MNNVL by introducing a new concept called <code class="docutils literal notranslate"><span class="pre">ComputeDomain</span></code>.</p>
<p>When a workload is created that references a specific <code class="docutils literal notranslate"><span class="pre">ComputeDomain</span></code>, the NVIDIA DRA Driver for GPUs will handle establishing a shared IMEX domain and IMEX channel that guarantees MNNVL-reachability between all pods in the workload.</p>
</section>
<section id="gpu-resource-allocation-technology-preview">
<h3>GPU resource allocation (Technology Preview)<a class="headerlink" href="#gpu-resource-allocation-technology-preview" title="Permalink to this headline">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The GPU allocation features of the NVIDIA DRA Driver for GPUs are not supported in production environment  and are not functionally complete.
Technology Preview features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
These releases may not have full documentation, and testing is limited.</p>
</div>
<p>Dynamic resource allocation was imlpemented in Kubernetes to allow users to more esily define and request specialized reousrces for their workloads
Before DRA, requesting GPUs and other specilized resources handled by a device plugin, like the [NVIDIA Kubernetes Device plugin](<a class="github reference external" href="https://github.com/NVIDIA/k8s-device-plugin">NVIDIA/k8s-device-plugin</a>).
The device-plugin along with a set of node labels added by GPU Feature Discovery, enabled users to allocate the desired number of GPUs on a node with desired type of GPUs.</p>
<p>The improvements made with Kuberntes DRA introduce an API that allows you to define resource claim templates for your GPUs resources that can be referenced in your workloads as a resource claim and allocated at deploy time.
This new API allows you to move away from the limited  “countable” API  provided by the previous implementation using device-plugins, to something much more flexible in terms of controlling which resources are consumed and where.</p>
<p>Full support and implementation of the DRA using the DRA Driver for GPUs is not yet available.
The current release offers a Technology Preview of DRA support with the GPU Operator.</p>
</section>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">#</a></h2>
<p>The NVIDIA DRA Driver for GPUs is an additional component that can be installed alongside the GPU Operator on your Kubernetes cluster.
The instructions below cover installing the DRA Driver for GPUs to be used to allocate reources for workloads that require Multi-Node NVLink on NVIDIA GB200 GPUs.</p>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">#</a></h3>
<ul>
<li><p>A Multi-Node NVIDIA GB200 system.</p></li>
<li><p>A Kubernetes cluster (v1.32 or newer) with the <cite>DynamicResourceAllocation</cite> feature gate enabled and the <cite>resource.k8s.io</cite> API group enabled.</p>
<p>The following is a sample for enabling the required feature gates and API groups.
Refer to the Kubernetes documentation for full details on <a class="reference external" href="https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#enabling-dynamic-resource-allocation">enabling DRA on your cluster</a>.</p>
<div class="literal-block-wrapper docutils container" id="id1">
<div class="code-block-caption"><span class="caption-text">Sample Kubeadm Init Config with DRA Feature Gates Enabled</span><a class="headerlink" href="#id1" title="Permalink to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kubeadm.k8s.io/v1beta4</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ClusterConfiguration</span><span class="w"></span>
<span class="nt">apiServer</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">extraArgs</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;feature-gates&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;DynamicResourceAllocation=true&quot;</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;runtime-config&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;resource.k8s.io/v1beta1=true&quot;</span><span class="w"></span>
<span class="nt">controllerManager</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">extraArgs</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;feature-gates&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;DynamicResourceAllocation=true&quot;</span><span class="w"></span>
<span class="nt">scheduler</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">extraArgs</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;feature-gates&quot;</span><span class="w"></span>
<span class="w">    </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;DynamicResourceAllocation=true&quot;</span><span class="w"></span>
<span class="nn">---</span><span class="w"></span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kubelet.config.k8s.io/v1beta1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">KubeletConfiguration</span><span class="w"></span>
<span class="nt">featureGates</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">DynamicResourceAllocation</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
</pre></div>
</div>
</div>
</li>
<li><p>The NVIDIA GPU Operator v25.3.0 or later installed with CDI enabled on all nodes and NVIDIA GPU Driver 565 or later.</p>
<p>A sample Helm install command below includes enabling CDI with <code class="docutils literal notranslate"><span class="pre">cdi.enabled=true</span></code>.
Refer to the install documentation for details on <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#common-chart-customization-options">enabling CDI</a>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install --wait --generate-name <span class="se">\</span>
        -n gpu-operator --create-namespace <span class="se">\</span>
        nvidia/gpu-operator <span class="se">\</span>
        --version<span class="o">=</span>v25.3.1 <span class="se">\</span>
        --set cdi.enabled<span class="o">=</span><span class="nb">true</span>
</pre></div>
</div>
<p>If you want to install the DRA Driver for GPUs using pre-installed drivers, you must install NVIDIA GPU Driver 565 or later, the corresponding IMEX packages on GPU nodes, and disable the IMEX systemd service before installing the GPU Operator.
Refer to the documentation on <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#pre-installed-nvidia-gpu-drivers">installing the GPU Operator with pre-installed drivers</a> for more details.</p>
</li>
</ul>
</section>
<section id="install-the-nvidia-dra-driver-for-gpus-with-helm">
<h3>Install the NVIDIA DRA Driver for GPUs with Helm<a class="headerlink" href="#install-the-nvidia-dra-driver-for-gpus-with-helm" title="Permalink to this headline">#</a></h3>
<ol class="arabic">
<li><p>Add the NVIDIA Helm repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia <span class="se">\</span>
    <span class="o">&amp;&amp;</span> helm repo update
</pre></div>
</div>
</li>
<li><p>Install the NVIDIA DRA Driver for GPUs:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>helm install nvidia-dra-driver-gpu nvidia/nvidia-dra-driver-gpu <span class="se">\</span>
      --version<span class="o">=</span><span class="s2">&quot;25.3.0&quot;</span> <span class="se">\</span>
      --create-namespace <span class="se">\</span>
      --namespace nvidia-dra-driver-gpu <span class="se">\</span>
      --set <span class="nv">nvidiaDriverRoot</span><span class="o">=</span>/run/nvidia/driver <span class="se">\</span>
      --set resources.gpus.enabled<span class="o">=</span><span class="nb">false</span>
</pre></div>
</div>
</li>
</ol>
<p>Note that <code class="docutils literal notranslate"><span class="pre">--set</span> <span class="pre">nvidiaDriverRoot=/run/nvidia/driver</span></code> above expects a GPU Operator-provided GPU driver at that location.
That configuration parameter must be changed in case the GPU drivers are installed straight on the host (typically at <code class="docutils literal notranslate"><span class="pre">/</span></code>, which is the default value for <code class="docutils literal notranslate"><span class="pre">nvidiaDriverRoot</span></code>).</p>
</section>
<section id="common-chart-customization-options">
<h3>Common Chart Customization Options<a class="headerlink" href="#common-chart-customization-options" title="Permalink to this headline">#</a></h3>
<p>The following options are available when installing the Helm chart.
These options can be used with <code class="docutils literal notranslate"><span class="pre">--set</span></code> when installing with Helm (these are just the most frequently used options –
all parameters can be listed by running <code class="docutils literal notranslate"><span class="pre">helm</span> <span class="pre">show</span> <span class="pre">values</span> <span class="pre">nvidia/nvidia-dra-driver-gpu</span></code>).</p>
<div class="pst-scrollable-table-container"><table class="colwidths-given table">
<colgroup>
<col style="width: 20%" />
<col style="width: 50%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">nvidiaDriverRoot</span></code></p></td>
<td><p>Specifies the driver root on the host.
For GPU Operator-managed drivers (recommended), use <code class="docutils literal notranslate"><span class="pre">/run/nvidia/driver</span></code>.
For pre-installed drivers, use <code class="docutils literal notranslate"><span class="pre">/</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">nvidiaCtkPath</span></code></p></td>
<td><p>Specifies the path of the NVIDIA Container Toolkit CLI binary (nvidia-ctk) on the host.
For GPU Operator-installed NVIDIA Container Toolkit (recommended), use <code class="docutils literal notranslate"><span class="pre">/usr/local/nvidia/toolkit/nvidia-ctk</span></code>.
For a pre-installed NVIDIA Container Toolkit, use <code class="docutils literal notranslate"><span class="pre">/usr/bin/nvidia-ctk</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/usr/bin/nvidia-ctk</span></code></p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">resources.gpus.enabled</span></code></p></td>
<td><p>Specifies whether to enable the NVIDIA DRA Driver for GPUs to manage GPU resource allocation.
This feature is in Technolody Preview and only recommended for testing, not production enviroments.
To use with MNNVL use cases only, set to <code class="docutils literal notranslate"><span class="pre">false</span></code>.</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">true</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="verify-installation">
<h3>Verify Installation<a class="headerlink" href="#verify-installation" title="Permalink to this headline">#</a></h3>
<ol class="arabic">
<li><p>Validate that the components are running and in a <code class="docutils literal notranslate"><span class="pre">READY</span></code> state.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pod -n nvidia-dra-driver-gpu
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                                                           READY   STATUS    RESTARTS   AGE</span>
<span class="go">nvidia-dra-driver-k8s-dra-driver-controller-67cb99d84b-5q7kj   1/1     Running   0          7m26s</span>
<span class="go">nvidia-dra-driver-k8s-dra-driver-kubelet-plugin-7kdg9          1/1     Running   0          7m27s</span>
<span class="go">nvidia-dra-driver-k8s-dra-driver-kubelet-plugin-bd6gn          1/1     Running   0          7m27s</span>
<span class="go">nvidia-dra-driver-k8s-dra-driver-kubelet-plugin-bzm6p          1/1     Running   0          7m26s</span>
<span class="go">nvidia-dra-driver-k8s-dra-driver-kubelet-plugin-xjm4p          1/1     Running   0          7m27s</span>
</pre></div>
</div>
</li>
<li><p>Confirm that all GPU nodes are labeled with clique ids.
The following command used <a class="reference external" href="https://jqlang.org/">jq</a> to format the output.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="o">(</span><span class="nb">echo</span> -e <span class="s2">&quot;NODE\tLABEL\tCLIQUE&quot;</span><span class="p">;</span> kubectl get nodes -o json <span class="p">|</span> <span class="se">\</span>
  jq -r <span class="s1">&#39;.items[] | [.metadata.name, &quot;nvidia.com/gpu.clique&quot;, .metadata.labels[&quot;nvidia.com/gpu.clique&quot;]] | @tsv&#39;</span><span class="o">)</span> <span class="p">|</span> <span class="se">\</span>
  column -t
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NODE                  LABEL                  CLIQUE</span>
<span class="go">node1                 nvidia.com/gpu.clique  9277d399-0674-44a9-b64e-d85bb19ce2b0.32766</span>
<span class="go">node2                 nvidia.com/gpu.clique  9277d399-0674-44a9-b64e-d85bb19ce2b0.32766</span>
</pre></div>
</div>
</li>
</ol>
<p>The <a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin/tree/main/docs/gpu-feature-discovery">NVIDIA GPU Feature Discovery</a> adds a Clique ID to each GPU node.
This is a unique identifier within an NVLink domain (physically connected GPUs over NVLink) that indicates which GPUs within that domain are physically capable of talking to each other.</p>
<p>The partitioning of GPUs into a set of cliques is done at the NVSwitch layer, not at the individual node layer.
All GPUs on a given node are guaranteed to have the same &lt;ClusterUUID.Clique ID&gt; pair.</p>
<p>The ClusterUUID is a unique identifier for a given NVLink Domain.
It can be queried on a GPU by GPU basis via the <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> commandline tool.
All GPUs on a given node are guaranteed to have the same Cluster UUID.</p>
</section>
</section>
<section id="about-the-computedomain-custom-resource">
<h2>About the ComputeDomain Custom Resource<a class="headerlink" href="#about-the-computedomain-custom-resource" title="Permalink to this headline">#</a></h2>
<p>The NVIDIA DRA Driver for GPUs introduces a Kubernetes custom resource named <code class="docutils literal notranslate"><span class="pre">ComputeDomain</span></code> which you use to define multi-node resource requirements.
As you deploy multi-node workloads, you can reference the ComputeDomain and the DRA Driver for GPUs will handle automtically provisioning the required resources to allow a set of GPUs to directly read and write each other’s memory over a high-bandwidth NVLink.
The ComputeDomain custom resource defines a <a class="reference external" href="https://kubernetes.io/docs/concepts/scheduling-eviction/dynamic-resource-allocation/#api">Kubernetes DRA ResourceClaimTemplate</a> and <code class="docutils literal notranslate"><span class="pre">numNodes</span></code> needed to run your multi-node workload on Multi-Node NVLink (MNNVL) capable GPUs.</p>
<div class="literal-block-wrapper docutils container" id="id2">
<div class="code-block-caption"><span class="caption-text">Sample NVIDIA DRA Driver ComputeDomain Custom Resource Manifest</span><a class="headerlink" href="#id2" title="Permalink to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">resource.nvidia.com/v1beta1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ComputeDomain</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-injection</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">numNodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">  </span><span class="nt">channel</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">resourceClaimTemplate</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-0</span><span class="w"></span>
</pre></div>
</div>
</div>
<p>You can then reference the ResourceClaimTemplate in your workload specs as a <code class="docutils literal notranslate"><span class="pre">resourceClaims.resourceClaimTemplateName</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-injection</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">...</span><span class="w"></span>
<span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">containers</span><span class="p p-Indicator">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ctr</span><span class="w"></span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu:22.04</span><span class="w"></span>
<span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;bash&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;-c&quot;</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">    </span><span class="nt">args</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;ls</span><span class="nv"> </span><span class="s">-la</span><span class="nv"> </span><span class="s">/dev/nvidia-caps-imex-channels;</span><span class="nv"> </span><span class="s">trap</span><span class="nv"> </span><span class="s">&#39;exit</span><span class="nv"> </span><span class="s">0&#39;</span><span class="nv"> </span><span class="s">TERM;</span><span class="nv"> </span><span class="s">sleep</span><span class="nv"> </span><span class="s">9999</span><span class="nv"> </span><span class="s">&amp;</span><span class="nv"> </span><span class="s">wait&quot;</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="hll"><span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
</span><span class="hll"><span class="w">      </span><span class="nt">claims</span><span class="p">:</span><span class="w"></span>
</span><span class="hll"><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-0</span><span class="w"></span>
</span><span class="hll"><span class="w">  </span><span class="nt">resourceClaims</span><span class="p">:</span><span class="w"></span>
</span><span class="hll"><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-0</span><span class="w"></span>
</span><span class="hll"><span class="w">    </span><span class="nt">resourceClaimTemplateName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-0</span><span class="w"></span>
</span></pre></div>
</div>
<p>If a subset of the nodes associated with a ComputeDomain are capable of communicating over MNNVL, the NVIDIA DRA Driver for GPUs will set up a one-off IMEX domain to allow GPUs to communicate over their multi-node NVLink connections.
Multiple IMEX domains will be created as necessary depending on the number and availability of nodes allocated to the ComputeDomain.</p>
<p>A multi-node workload should run in its own compute domain.
When you create the compute domain you can specify how many nodes you want to be a part of it in the <code class="docutils literal notranslate"><span class="pre">numNodes</span></code> field.
This can be any number, less than a rack, equal to a rack, more than a rack.
The compute domain controller is able to create 0-or-more IMEX domains depending on where the workers of a multi-node job that reference the compute domain actually land in your cluster</p>
<p>When a worker for a multi-node job that references a ComputeDomain’s ResourceClaimTemplate is scheduled on your cluster, the DRA Driver for GPUs triggers an IMEX daemon to started on the node the worker lands on and will block the worker from running until the compute domain is ready.
Once the number of IMEX daemons running equals the number of nodes specified in the compute domain, the DRA Driver for GPUs will mark the compute domain as ready and will release the worker pods themselves, allowing them to start running.
Since the compute domain is per workload, only one channel is needed to link all of the worker pods of the workload.</p>
<p>The value of the &lt;cluster-uuid, clique-id&gt; tuple associated with the node where a workload lands determines which IMEX domain it will be a part of.
Nodes with the same &lt;cluster-uuid, clique-id&gt; values will be part of the same IMEX domain and will be able to communicate over MNNVL with each other.
Nodes with different &lt;cluster-uuid, clique-id&gt; values will be associated with different IMEX domains and will not be able to communicate over MNNVL with each other.
Nodes without a &lt;cluster-uuid, clique-id&gt; setting at all are still allowed, but no IMEX daemon will be started on such nodes and no MNNVL communication with them is possible from any other nodes in the compute domain.
The nodes are still be able to communicate over IB or Ethernet.</p>
<p>Once all workloads running in a ComputeDomain have run to completion, the label gets removed even if the ComputeDomain itself hasn’t been deleted yet.
This allows these nodes to be reused for other ComputeDomains.</p>
<section id="configuration-options-for-computedomain">
<h3>Configuration Options for ComputeDomain<a class="headerlink" href="#configuration-options-for-computedomain" title="Permalink to this headline">#</a></h3>
<p>The following table describes some of the fields in the ComputeDomain custom resource.</p>
<div class="pst-scrollable-table-container"><table class="colwidths-given table">
<colgroup>
<col style="width: 20%" />
<col style="width: 60%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Field</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">channel.resourceClaimTemplate.name</span></code> (required)</p></td>
<td><p>Specifies the <code class="docutils literal notranslate"><span class="pre">name</span></code> of the ResourceClaimTemplate to create.</p></td>
<td><p>None</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">numNodes</span></code> (required)</p></td>
<td><p>Specifies the number of nodes in the ComputeDomain.</p></td>
<td><p>None</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="node-and-pod-affinity-strategies">
<h3>Node and Pod Affinity Strategies<a class="headerlink" href="#node-and-pod-affinity-strategies" title="Permalink to this headline">#</a></h3>
<p>For the DRA Driver for GPUs, a ComputeDomain means running workloads across a group of compute nodes.
This means even if some nodes are not MNNVL capable, they can still be part of the same ComputeDomain.
You must apply NodeAffinity and PodAffinity rules to your nodes and pods to make sure your workloads run on MNNVL capable nodes.</p>
<p>For example you could set PodAffinity with a required topologyKey set to <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.clique</span></code> when you require all workloads deployed into the same NVLink domain, but don’t care which one.
Or use a preferred topologyKey set to <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.clique</span></code> for workloads to span MNNVL domains but want them packed as tightly as possible.</p>
</section>
<section id="example-create-a-computedomain-and-run-a-workload">
<h3>Example: Create a ComputeDomain and Run a Workload<a class="headerlink" href="#example-create-a-computedomain-and-run-a-workload" title="Permalink to this headline">#</a></h3>
<ol class="arabic">
<li><p>Create a file like <code class="docutils literal notranslate"><span class="pre">imex-channel-injection.yaml</span></code> below.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span><span class="w"></span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">resource.nvidia.com/v1beta1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ComputeDomain</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-injection</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">numNodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">  </span><span class="nt">channel</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">resourceClaimTemplate</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-0</span><span class="w"></span>
<span class="nn">---</span><span class="w"></span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-injection</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">affinity</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">nodeAffinity</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/gpu.clique</span><span class="w"></span>
<span class="w">            </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Exists</span><span class="w"></span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ctr</span><span class="w"></span>
<span class="w">    </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu:22.04</span><span class="w"></span>
<span class="w">    </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;bash&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;-c&quot;</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">    </span><span class="nt">args</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;ls</span><span class="nv"> </span><span class="s">-la</span><span class="nv"> </span><span class="s">/dev/nvidia-caps-imex-channels;</span><span class="nv"> </span><span class="s">trap</span><span class="nv"> </span><span class="s">&#39;exit</span><span class="nv"> </span><span class="s">0&#39;</span><span class="nv"> </span><span class="s">TERM;</span><span class="nv"> </span><span class="s">sleep</span><span class="nv"> </span><span class="s">9999</span><span class="nv"> </span><span class="s">&amp;</span><span class="nv"> </span><span class="s">wait&quot;</span><span class="p p-Indicator">]</span><span class="w"></span>
<span class="w">    </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">claims</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-0</span><span class="w"></span>
<span class="w">  </span><span class="nt">resourceClaims</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-0</span><span class="w"></span>
<span class="w">    </span><span class="nt">resourceClaimTemplateName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">imex-channel-0</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Apply the manifest.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f imex-channel-injection.yaml
</pre></div>
</div>
</li>
<li><p>Optional: View the <code class="docutils literal notranslate"><span class="pre">imex-channel-injection</span></code> pod.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pods
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                     READY   STATUS    RESTARTS   AGE</span>
<span class="go">imex-channel-injection   1/1     Running   0          3s</span>
</pre></div>
</div>
</li>
<li><p>Optional: View logs for the <code class="docutils literal notranslate"><span class="pre">imex-channel-injection</span></code> pod, where the IMEX channel was injected.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl logs imex-channel-injection
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">total 0</span>
<span class="go">drwxr-xr-x 2 root root     60 Feb 19 10:43 .</span>
<span class="go">drwxr-xr-x 6 root root    380 Feb 19 10:43 ..</span>
<span class="go">crw-rw-rw- 1 root root 507, 0 Feb 19 10:43 channel0</span>
</pre></div>
</div>
</li>
<li><p>Optional: View the ComputeDomain pod.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pods -n nvidia-dra-driver-gpu -l resource.nvidia.com/computeDomain
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                                 READY   STATUS    RESTARTS   AGE</span>
<span class="go">imex-channel-injection-6k9sx-ffgpf   1/1     Running   0          3s</span>
</pre></div>
</div>
</li>
<li><p>Optional: View IMEX channel creation logs.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl logs -n nvidia-dra-driver-gpu -l resource.nvidia.com/computeDomain --tail<span class="o">=</span>-1
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">/etc/nvidia-imex/nodes_config.cfg:</span>
<span class="go">10.115.131.8</span>
<span class="go">IMEX Log initializing at: 3/27/2025 15:47:10.092</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] IMEX version 570.124.06 is running with the following configuration options</span>

<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] Logging level = 4</span>

<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] Logging file name/path = /var/log/nvidia-imex.log</span>

<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] Append to log file = 0</span>

<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] Max Log file size = 1024 (MBs)</span>

<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] Use Syslog file = 0</span>

<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] IMEX Library communication bind interface =</span>

<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] IMEX library communication bind port = 50000</span>

<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] Identified this node as ID 0, using bind IP of &#39;10.115.131.8&#39;, and network interface of enP5p9s0</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] nvidia-imex persistence file /var/run/nvidia-imex/persist.dat does not exist.  Assuming no previous importers.</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] NvGpu Library version matched with GPU Driver version</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 63] Started processing of incoming messages.</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 64] Started processing of incoming messages.</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 65] Started processing of incoming messages.</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] Creating gRPC channels to all peers (nPeers = 1).</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 66] Started processing of incoming messages.</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] IMEX_WAIT_FOR_QUORUM != FULL, continuing initialization without waiting for connections to all nodes.</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 67] Connection established to node 0 with ip address 10.115.131.8. Number of times connected: 1</span>
<span class="go">[Mar 27 2025 15:47:10] [INFO] [tid 39] GPU event successfully subscribed</span>
</pre></div>
</div>
</li>
<li><p>Delete <code class="docutils literal notranslate"><span class="pre">imex-channel-injection</span></code> example.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl delete -f imex-channel-injection.yaml
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">computedomain.resource.nvidia.com &quot;imex-channel-injection&quot; deleted</span>
<span class="go">pod &quot;imex-channel-injection&quot; deleted</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="run-a-multi-node-nvbandwidth-test-requiring-imex-channels-with-mpi">
<h2>Run a Multi-node nvbandwidth Test Requiring IMEX Channels with MPI<a class="headerlink" href="#run-a-multi-node-nvbandwidth-test-requiring-imex-channels-with-mpi" title="Permalink to this headline">#</a></h2>
<p>This example demonstrates how to run a workload across multiple nodes using a ComputeDomain.
The nvbandwidth test will measure the bandwidth between GPUs across different nodes using IMEX channels, helping you verify that your MNNVL setup is working correctly.</p>
<p><strong>Example notes:</strong></p>
<ul>
<li><p>This example uses <a class="reference external" href="https://www.kubeflow.org/docs/components/trainer/legacy-v1/user-guides/mpi/#installationr">Kubeflow MPI Operator</a>.</p></li>
<li><p>This example is configured for a 2 node cluster with 4 GPUs per node.</p>
<p>If you are using a cluster with a different number of nodes and GPUs per node, you must adjust the following parameters in the sample files:</p>
</li>
</ul>
<div class="pst-scrollable-table-container"><table class="colwidths-given table">
<colgroup>
<col style="width: 15%" />
<col style="width: 55%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Parameter to update</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Value in example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">ComputeDomain.spec.numNodes</span></code></p></td>
<td><p>Total number of nodes in the cluster</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">MPIJob.spec.slotsPerWorker</span></code></p></td>
<td><p>Number of GPUs per node, this should match the ppr number</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">MPIJob.spec.mpiReplicaSpecs.Worker.replicas</span></code></p></td>
<td><p>Number of worker nodes</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command argument <code class="docutils literal notranslate"><span class="pre">-ppr:4:node</span></code></p></td>
<td><ul class="simple">
<li><p>Number of GPUs per node as the process-per-resource number</p></li>
</ul>
</td>
<td><p>4</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">mpirun</span></code> command argument <code class="docutils literal notranslate"><span class="pre">-np</span></code> value</p></td>
<td><p>Total processes to be the number of GPU per node * the number of nodes in the cluster</p></td>
<td><p>8</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Example Steps:</strong></p>
<ol class="arabic">
<li><p>Install Kubeflow MPI Operator.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl create -f https://github.com/kubeflow/mpi-operator/releases/download/v0.6.0/mpi-operator.yaml
</pre></div>
</div>
</li>
<li><p>Create a nvbandwidth test job file called <code class="docutils literal notranslate"><span class="pre">nvbandwidth-test-job.yaml</span></code>.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span><span class="w"></span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">resource.nvidia.com/v1beta1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ComputeDomain</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvbandwidth-test-compute-domain</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Update numNodes to match the total number of nodes in your cluster</span><span class="w"></span>
<span class="w">  </span><span class="nt">numNodes</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"></span>
<span class="w">  </span><span class="nt">channel</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">resourceClaimTemplate</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvbandwidth-test-compute-domain-channel</span><span class="w"></span>

<span class="nn">---</span><span class="w"></span>
<span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kubeflow.org/v2beta1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">MPIJob</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvbandwidth-test</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="c1"># Update slotsPerWorker to match the number of GPUs per node</span><span class="w"></span>
<span class="w">  </span><span class="nt">slotsPerWorker</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class="w"></span>
<span class="w">  </span><span class="nt">launcherCreationPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">WaitForWorkersReady</span><span class="w"></span>
<span class="w">  </span><span class="nt">runPolicy</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">cleanPodPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Running</span><span class="w"></span>
<span class="w">  </span><span class="nt">sshAuthMountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/home/mpiuser/.ssh</span><span class="w"></span>
<span class="w">  </span><span class="nt">mpiReplicaSpecs</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">Launcher</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">      </span><span class="nt">template</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="nt">labels</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="nt">nvbandwidth-test-replica</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mpi-launcher</span><span class="w"></span>
<span class="w">        </span><span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="nt">affinity</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="nt">nodeAffinity</span><span class="p">:</span><span class="w"></span>
<span class="w">              </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w"></span>
<span class="w">                </span><span class="nt">nodeSelectorTerms</span><span class="p">:</span><span class="w"></span>
<span class="w">                </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w"></span>
<span class="w">                  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">node-role.kubernetes.io/control-plane</span><span class="w"></span>
<span class="w">                    </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Exists</span><span class="w"></span>
<span class="w">          </span><span class="nt">containers</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ghcr.io/nvidia/k8s-samples:nvbandwidth-v0.7-8d103163</span><span class="w"></span>
<span class="w">            </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mpi-launcher</span><span class="w"></span>
<span class="w">            </span><span class="nt">securityContext</span><span class="p">:</span><span class="w"></span>
<span class="w">              </span><span class="nt">runAsUser</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span><span class="w"></span>
<span class="w">            </span><span class="nt">command</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mpirun</span><span class="w"></span>
<span class="w">            </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">--bind-to</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">core</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">--map-by</span><span class="w"></span>
<span class="w">            </span><span class="c1"># Update the number (4) to match the number of GPUs per node</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ppr:4:node</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-np</span><span class="w"></span>
<span class="w">            </span><span class="c1"># Update the number (8) to match the total number of GPUs in the cluster, this example has 2 nodes * 4 GPUs per node</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="s">&quot;8&quot;</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">--report-bindings</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-q</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvbandwidth</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-t</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">multinode_device_to_device_memcpy_read_ce</span><span class="w"></span>
<span class="w">    </span><span class="nt">Worker</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="c1"># Update replicas to match the number of worker nodes</span><span class="w"></span>
<span class="w">      </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"></span>
<span class="w">      </span><span class="nt">template</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="nt">labels</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="nt">nvbandwidth-test-replica</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mpi-worker</span><span class="w"></span>
<span class="w">        </span><span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="nt">affinity</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="nt">podAffinity</span><span class="p">:</span><span class="w"></span>
<span class="w">              </span><span class="nt">requiredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span><span class="w"></span>
<span class="w">              </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">labelSelector</span><span class="p">:</span><span class="w"></span>
<span class="w">                  </span><span class="nt">matchExpressions</span><span class="p">:</span><span class="w"></span>
<span class="w">                  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvbandwidth-test-replica</span><span class="w"></span>
<span class="w">                    </span><span class="nt">operator</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">In</span><span class="w"></span>
<span class="w">                    </span><span class="nt">values</span><span class="p">:</span><span class="w"></span>
<span class="w">                    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mpi-worker</span><span class="w"></span>
<span class="w">                </span><span class="nt">topologyKey</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/gpu.clique</span><span class="w"></span>
<span class="w">          </span><span class="nt">containers</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ghcr.io/nvidia/k8s-samples:nvbandwidth-v0.7-8d103163</span><span class="w"></span>
<span class="w">            </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mpi-worker</span><span class="w"></span>
<span class="w">            </span><span class="nt">securityContext</span><span class="p">:</span><span class="w"></span>
<span class="w">              </span><span class="nt">runAsUser</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span><span class="w"></span>
<span class="w">            </span><span class="nt">env</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="nt">command</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/usr/sbin/sshd</span><span class="w"></span>
<span class="w">            </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-De</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-f</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/home/mpiuser/.sshd_config</span><span class="w"></span>
<span class="w">            </span><span class="nt">resources</span><span class="p">:</span><span class="w"></span>
<span class="w">              </span><span class="nt">limits</span><span class="p">:</span><span class="w"></span>
<span class="w">                </span><span class="nt">nvidia.com/gpu</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">4</span><span class="w"></span>
<span class="w">              </span><span class="nt">claims</span><span class="p">:</span><span class="w"></span>
<span class="w">              </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">compute-domain-channel</span><span class="w"></span>
<span class="w">          </span><span class="nt">resourceClaims</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">compute-domain-channel</span><span class="w"></span>
<span class="w">            </span><span class="nt">resourceClaimTemplateName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvbandwidth-test-compute-domain-channel</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Apply the manifest.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl apply -f nvbandwidth-test-job.yaml
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">computedomain.resource.nvidia.com/nvbandwidth-test-compute-domain configured</span>
<span class="go">mpijob.kubeflow.org/nvbandwidth-test configured</span>
</pre></div>
</div>
</li>
<li><p>Verify that the nvbandwidth pods were created.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pods
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                              READY   STATUS    RESTARTS   AGE</span>
<span class="go">nvbandwidth-test-launcher-lzv84   1/1     Running   0          8s</span>
<span class="go">nvbandwidth-test-worker-0         1/1     Running   0          15s</span>
<span class="go">nvbandwidth-test-worker-1         1/1     Running   0          15s</span>
</pre></div>
</div>
</li>
<li><p>Verify that the ComputeDomain pods were created for each node.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl get pods -n nvidia-dra-driver-gpu -l resource.nvidia.com/computeDomain
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">NAME                                          READY   STATUS    RESTARTS   AGE</span>
<span class="go">nvbandwidth-test-compute-domain-ht24d-9jhmj   1/1     Running   0          20s</span>
<span class="go">nvbandwidth-test-compute-domain-ht24d-rcn2c   1/1     Running   0          20s</span>
</pre></div>
</div>
</li>
<li><p>Verify the nvbandwidth test results.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl logs --tail<span class="o">=</span>-1 -l job-name<span class="o">=</span>nvbandwidth-test-launcher
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">Warning: Permanently added &#39;[nvbandwidth-test-worker-0.nvbandwidth-test.default.svc]:2222&#39; (ECDSA) to the list of known hosts.</span>
<span class="go">Warning: Permanently added &#39;[nvbandwidth-test-worker-1.nvbandwidth-test.default.svc]:2222&#39; (ECDSA) to the list of known hosts.</span>
<span class="go">[nvbandwidth-test-worker-0:00025] MCW rank 0 bound to socket 0[core 0[hwt 0]]: [B/././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.][./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.]</span>
<span class="go">[nvbandwidth-test-worker-0:00025] MCW rank 1 bound to socket 0[core 1[hwt 0]]: [./B/./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.][./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.]</span>
<span class="go">[nvbandwidth-test-worker-0:00025] MCW rank 2 bound to socket 0[core 2[hwt 0]]: [././B/././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.][./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.]</span>
<span class="go">[nvbandwidth-test-worker-0:00025] MCW rank 3 bound to socket 0[core 3[hwt 0]]: [./././B/./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.][./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.]</span>
<span class="go">[nvbandwidth-test-worker-1:00025] MCW rank 4 bound to socket 0[core 0[hwt 0]]: [B/././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.][./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.]</span>
<span class="go">[nvbandwidth-test-worker-1:00025] MCW rank 5 bound to socket 0[core 1[hwt 0]]: [./B/./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.][./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.]</span>
<span class="go">[nvbandwidth-test-worker-1:00025] MCW rank 6 bound to socket 0[core 2[hwt 0]]: [././B/././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.][./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.]</span>
<span class="go">[nvbandwidth-test-worker-1:00025] MCW rank 7 bound to socket 0[core 3[hwt 0]]: [./././B/./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.][./././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././././.]</span>
<span class="go">nvbandwidth Version: v0.7</span>
<span class="go">Built from Git version: v0.7</span>

<span class="go">MPI version: Open MPI v4.1.4, package: Debian OpenMPI, ident: 4.1.4, repo rev: v4.1.4, May 26, 2022</span>
<span class="go">CUDA Runtime Version: 12080</span>
<span class="go">CUDA Driver Version: 12080</span>
<span class="go">Driver Version: 570.124.06</span>

<span class="go">Process 0 (nvbandwidth-test-worker-0): device 0: HGX GB200 (00000008:01:00)</span>
<span class="go">Process 1 (nvbandwidth-test-worker-0): device 1: HGX GB200 (00000009:01:00)</span>
<span class="go">Process 2 (nvbandwidth-test-worker-0): device 2: HGX GB200 (00000018:01:00)</span>
<span class="go">Process 3 (nvbandwidth-test-worker-0): device 3: HGX GB200 (00000019:01:00)</span>
<span class="go">Process 4 (nvbandwidth-test-worker-1): device 0: HGX GB200 (00000008:01:00)</span>
<span class="go">Process 5 (nvbandwidth-test-worker-1): device 1: HGX GB200 (00000009:01:00)</span>
<span class="go">Process 6 (nvbandwidth-test-worker-1): device 2: HGX GB200 (00000018:01:00)</span>
<span class="go">Process 7 (nvbandwidth-test-worker-1): device 3: HGX GB200 (00000019:01:00)</span>

<span class="go">Running multinode_device_to_device_memcpy_read_ce.</span>
<span class="go">memcpy CE GPU(row) -&gt; GPU(column) bandwidth (GB/s)</span>
<span class="go">          0         1         2         3         4         5         6         7</span>
<span class="go">0       N/A    798.02    798.25    798.02    798.02    797.88    797.73    797.95</span>
<span class="go">1    798.10       N/A    797.80    798.02    798.02    798.25    797.88    798.02</span>
<span class="go">2    797.95    797.95       N/A    797.73    797.80    797.95    797.95    797.65</span>
<span class="go">3    798.10    798.02    797.95       N/A    798.02    798.10    797.88    797.73</span>
<span class="go">4    797.80    798.02    798.02    798.02       N/A    797.95    797.80    798.02</span>
<span class="go">5    797.80    797.95    798.10    798.10    797.95       N/A    797.95    797.88</span>
<span class="go">6    797.73    797.95    798.10    798.02    797.95    797.88       N/A    797.80</span>
<span class="go">7    797.88    798.02    797.95    798.02    797.88    797.95    798.02       N/A</span>

<span class="go">SUM multinode_device_to_device_memcpy_read_ce 44685.29</span>

<span class="go">NOTE: The reported results may not reflect the full capabilities of the platform.</span>
</pre></div>
</div>
</li>
<li><p>Delete test.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl delete -f nvbandwidth-test-job.yaml
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">computedomain.resource.nvidia.com &quot;nvbandwidth-test-compute-domain&quot; deleted</span>
<span class="go">mpijob.kubeflow.org &quot;nvbandwidth-test&quot; deleted</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="cdi.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Container Device Interface Support in the GPU Operator</p>
      </div>
    </a>
    <a class="right-next"
       href="gpu-operator-kubevirt.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">GPU Operator with KubeVirt</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-use-cases">Supported Use Cases</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-node-nvlink-support-for-nvidia-gb200">Multi-Node NVLink Support for NVIDIA GB200</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-resource-allocation-technology-preview">GPU resource allocation (Technology Preview)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installation">Installation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#install-the-nvidia-dra-driver-for-gpus-with-helm">Install the NVIDIA DRA Driver for GPUs with Helm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-chart-customization-options">Common Chart Customization Options</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#verify-installation">Verify Installation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#about-the-computedomain-custom-resource">About the ComputeDomain Custom Resource</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#configuration-options-for-computedomain">Configuration Options for ComputeDomain</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#node-and-pod-affinity-strategies">Node and Pod Affinity Strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-create-a-computedomain-and-run-a-workload">Example: Create a ComputeDomain and Run a Workload</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#run-a-multi-node-nvbandwidth-test-requiring-imex-channels-with-mpi">Run a Multi-node nvbandwidth Test Requiring IMEX Channels with MPI</a></li>
</ul>
  </nav></div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">

<div class="footer-links">
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Manage My Privacy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/preferences/start/">Do Not Sell or Share My Data</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/">Terms of Service</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/">Accessibility</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/">Corporate Policies</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/product-security/">Product Security</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/contact/">Contact</a>
  
  
  
</div>
</div>
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright © 2020-2025, NVIDIA Corporation.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
</div>

  </footer>

  <script type="text/javascript">if (typeof _satellite !== "undefined") {_satellite.pageBottom();}</script>
  


  </body>
</html>