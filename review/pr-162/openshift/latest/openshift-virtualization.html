


<!DOCTYPE html>


<html data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>NVIDIA GPU Operator with OpenShift Virtualization &#8212; NVIDIA GPU Operator on Red Hat OpenShift Container Platform</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/nvidia-sphinx-theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/mermaid-init.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/design-tabs.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'openshift-virtualization';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = '../versions1.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '25.3.1';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>
    <script src="_static/version.js"></script>
    <script src="_static/social-media.js"></script>
    <link rel="shortcut icon" href="_static/favicon.ico"/>
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Precompiled Drivers for the NVIDIA GPU Operator for RHCOS" href="gpu-operator-with-precompiled-drivers.html" />
    <link rel="prev" title="Time-slicing NVIDIA GPUs in OpenShift" href="time-slicing-gpus-in-openshift.html" />


  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  <meta name="docsearch:version" content="" />
    <meta name="docbuild:last-update" content="Jul 15, 2025"/>



  <script src="https://assets.adobedtm.com/5d4962a43b79/c1061d2c5e7b/launch-191c2462b890.min.js" ></script>
  


  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA GPU Operator on Red Hat OpenShift Container Platform - Home"/>
    <img src="_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="NVIDIA GPU Operator on Red Hat OpenShift Container Platform - Home"/>
  
  
    <p class="title logo__title">NVIDIA GPU Operator on Red Hat OpenShift Container Platform</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        



  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/nvidia-logo-horiz-rgb-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA GPU Operator on Red Hat OpenShift Container Platform - Home"/>
    <img src="_static/nvidia-logo-horiz-rgb-wht-for-screen.svg" class="logo__image only-dark pst-js-only" alt="NVIDIA GPU Operator on Red Hat OpenShift Container Platform - Home"/>
  
  
    <p class="title logo__title">NVIDIA GPU Operator on Red Hat OpenShift Container Platform</p>
  
</a>


  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">


<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">



<nav class="bd-docs-nav bd-links"
     aria-label="Table of Contents">
  <p class="bd-links__title" role="heading" aria-level="1">Table of Contents</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="prerequisites.html">Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="steps-overview.html">Installation and Upgrade Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-nfd.html">NFD Operator Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="install-gpu-ocp.html">GPU Operator Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="nvaie-with-ocp.html">NVIDIA AI Enterprise with OpenShift</a></li>
<li class="toctree-l1"><a class="reference internal" href="mig-ocp.html">MIG Support in OpenShift Container Platform</a></li>
<li class="toctree-l1"><a class="reference internal" href="clean-up.html">Cleanup</a></li>
<li class="toctree-l1"><a class="reference internal" href="mirror-gpu-ocp-disconnected.html">Deploy GPU Operators in a disconnected or airgapped environment</a></li>
<li class="toctree-l1"><a class="reference internal" href="enable-gpu-monitoring-dashboard.html">Enabling the GPU Monitoring Dashboard</a></li>

<li class="toctree-l1"><a class="reference internal" href="time-slicing-gpus-in-openshift.html">Time-slicing NVIDIA GPUs in OpenShift</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">NVIDIA GPU Operator with OpenShift Virtualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu-operator-with-precompiled-drivers.html">Precompiled Drivers for the NVIDIA GPU Operator for RHCOS</a></li>
<li class="toctree-l1"><a class="reference internal" href="troubleshooting-gpu-ocp.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-ocp.html">Appendix</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>



      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">
<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    <li class="breadcrumb-item breadcrumb-home">
      <a href="https://docs.nvidia.com">NVIDIA Docs Hub</a>
    </li>
    <li class="breadcrumb-item">
      <a href="https://docs.nvidia.com/datacenter/cloud-native">Cloud Native Technologies</a>
    </li>
    <li class="breadcrumb-item">
      <a href="index.html">NVIDIA GPU Operator on Red Hat OpenShift Container Platform</a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">NVIDIA GPU Operator with OpenShift Virtualization</li>
  </ul>
</nav></div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="nvidia-gpu-operator-with-openshift-virtualization">
<span id="nvidia-gpu-operator-openshift-virtualization-vgpu-enablement"></span><h1>NVIDIA GPU Operator with OpenShift Virtualization<a class="headerlink" href="#nvidia-gpu-operator-with-openshift-virtualization" title="Permalink to this headline">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>There is a growing demand among Red Hat customers to use virtual GPUs (NVIDIA vGPU)
with Red Hat OpenShift Virtualization.
Red Hat OpenShift Virtualization is based on KubeVirt, a virtual machine (VM) management add-on to Kubernetes that allows you to run and manage VMs in a Kubernetes cluster.
It eliminates the need to manage separate clusters for VM and container workloads, as both can now coexist in a single Kubernetes cluster.
Red Hat OpenShift Virtualization is an OpenShift feature to run virtual machines (VMs) orchestrated by OpenShift (Kubernetes).</p>
<p>In addition to the GPU Operator being able to provision worker nodes for running GPU-accelerated containers, the GPU Operator can also be used to provision worker nodes for running GPU-accelerated virtual machines.</p>
<p>There are some different prerequisites required virtual machines with GPU(s) than running containers with GPU(s).
The primary difference is the drivers required.
For example, the datacenter driver is needed for containers, the vfio-pci driver is needed for GPU passthrough, and the <a class="reference external" href="https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#installing-configuring-grid-vgpu">NVIDIA vGPU Manager</a> is needed for creating vGPU devices.</p>
<section id="configure-worker-nodes-for-gpu-operator-components">
<span id="id1"></span><h3>Configure Worker Nodes for GPU Operator components<a class="headerlink" href="#configure-worker-nodes-for-gpu-operator-components" title="Permalink to this headline">#</a></h3>
<p>The GPU Operator can now be configured to deploy different software components on worker nodes depending on what GPU workload is configured to run on those nodes.
This is configured by adding a <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config</span></code> label to the worker node with the value of <code class="docutils literal notranslate"><span class="pre">container</span></code>, <code class="docutils literal notranslate"><span class="pre">vm-passthrough</span></code>, or <code class="docutils literal notranslate"><span class="pre">vm-vgpu</span></code> depending on if you are planning to use vGPU or not.
The GPU Operator will use the label to determine which software components to deploy on the worker nodes.</p>
<p>Given the following node configuration:</p>
<ul class="simple">
<li><p>Node A is configured to run containers.</p></li>
<li><p>Node B is configured to run VMs with Passthrough GPU.</p></li>
<li><p>Node C is configured to run VMs with vGPU.</p></li>
</ul>
<p>Node A receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Datacenter</span> <span class="pre">Driver</span></code> - To install the driver.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Container</span> <span class="pre">Toolkit</span></code> - To ensure containers can properly access GPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">Kubernetes</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - To discover and advertise GPU resources to the kubelet.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">DCGM</span> <span class="pre">and</span> <span class="pre">DCGM</span> <span class="pre">Exporter</span></code> - To monitor the GPU(s).</p></li>
</ul>
<p>Node B receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">VFIO</span> <span class="pre">Manager</span></code> - Optional. To load vfio-pci and bind it to all GPUs on the node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> - Optional. To discover and advertise the passthrough GPUs to the kubelet.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Validator</span></code> -Optional. Validates that Sandbox Device Plugin is working.</p></li>
</ul>
<p>Node C receives the following software components:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Manager</span></code> - To install the driver.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA</span> <span class="pre">vGPU</span> <span class="pre">Device</span> <span class="pre">Manager</span></code> - To create vGPU devices on the node.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Device</span> <span class="pre">Plugin</span></code> -Optional. To discover and advertise the vGPU devices to kubelet.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Sandbox</span> <span class="pre">Validator</span></code> -Optional. Validates that Sandbox Device Plugin is working.</p></li>
</ul>
</section>
<section id="assumptions-constraints-and-dependencies">
<h3>Assumptions, constraints, and dependencies<a class="headerlink" href="#assumptions-constraints-and-dependencies" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>A worker node can run GPU-accelerated containers, or GPU accelerated VMs with GPU passthrough, or GPU accelerated-VMs with vGPU, but not a combination of any of them.</p></li>
<li><p>The cluster admin or developer has knowledge about their cluster ahead of time, and can properly label nodes to indicate what types of GPU workloads they will run.</p></li>
<li><p>Worker nodes running GPU accelerated VMs (with pGPU or vGPU) are assumed to be bare metal.</p></li>
<li><p>MIG-backed vGPUs are not supported.</p></li>
<li><p>The GPU Operator will not automate the installation of the vGPU guest driver inside KubeVirt VMs with vGPUs attached.</p></li>
<li><p>There are two separate device plugins – the NVIDIA k8s-device-plugin and the NVIDIA kubevirt-gpu-device-plugin.</p></li>
<li><p>KubeVirt/Openshift virtualization provides built-in device plugins. These are the default tested device plugins.</p></li>
</ul>
</section>
</section>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">#</a></h2>
<ul>
<li><p><a class="reference external" href="https://docs.redhat.com/en/documentation/openshift_container_platform/latest/html/virtualization/installing">Install the OpenShift Virtualization Operator</a>.</p></li>
<li><p><a class="reference external" href="https://docs.redhat.com/en/documentation/openshift_container_platform/latest/html/virtualization/getting-started#installing-virtctl_virt-using-the-cli-tools">Install the virtctl client</a>.</p></li>
<li><p>Starting with OpenShift Virtualization 4.12.3 and 4.13.0, set the <code class="docutils literal notranslate"><span class="pre">disableMDevConfiguration</span></code> feature gate:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>kubectl patch hyperconverged -n openshift-cnv  kubevirt-hyperconverged --type<span class="o">=</span><span class="s1">&#39;json&#39;</span> -p<span class="o">=</span><span class="s1">&#39;[{&quot;op&quot;: &quot;add&quot;, &quot;path&quot;: &quot;/spec/featureGates/disableMDevConfiguration&quot;, &quot;value&quot;: true}]&#39;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">hyperconverged.hco.kubevirt.io/kubevirt-hyperconverged patched</span>
</pre></div>
</div>
</li>
<li><p>If planning to use NVIDIA vGPU, SR-IOV must be enabled in the BIOS if your GPUs are based on the NVIDIA Ampere architecture or later. Refer to the <a class="reference external" href="https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#prereqs-vgpu">NVIDIA vGPU Documentation</a> to ensure you have met all of the prerequisites for using NVIDIA vGPU.</p></li>
</ul>
</section>
<section id="configure-nvidia-gpu-operator-with-openshift-virtualization">
<h2>Configure NVIDIA GPU Operator with OpenShift Virtualization<a class="headerlink" href="#configure-nvidia-gpu-operator-with-openshift-virtualization" title="Permalink to this headline">#</a></h2>
<p>After configuring the <a class="reference internal" href="#id7"><span class="std std-ref">prerequisites</span></a>, the high level workflow for using the NVIDIA GPU Operator with OpenShift Virtualization is as follows:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#enable-iommu-driver"><span class="std std-ref">Enable the IOMMU driver</span></a>.</p></li>
<li><p><a class="reference internal" href="#label-worker-nodes"><span class="std std-ref">Label worker nodes</span></a> based on the GPU workloads they will run.</p></li>
<li><p><a class="reference internal" href="#install-the-gpu-operator"><span class="std std-ref">Install the GPU Operator</span></a> and set <code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.enabled=true</span></code>.</p></li>
</ul>
<dl class="simple">
<dt>If you are planning to deploy VMs with vGPU, the workflow is as follows:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#build-vgpu-manager-image"><span class="std std-ref">Build the NVIDIA vGPU Manager image</span></a>, before installing the GPU Operator.</p></li>
<li><p><a class="reference internal" href="#vgpu-device-configuration"><span class="std std-ref">Label the node for the vGPU configuration</span></a></p></li>
<li><p><a class="reference internal" href="#add-vgpu-resources-to-the-hyperconverged-custom-resource"><span class="std std-ref">Add vGPU resources to the HyperConverged Custom Resource</span></a></p></li>
<li><p><a class="reference internal" href="#create-a-virtual-machine-with-gpu"><span class="std std-ref">Create a virtual machine with vGPU</span></a></p></li>
</ul>
</dd>
<dt>If you are planning to deploy VMs with GPU passthrough, the workflow is as follows:</dt><dd><ul class="simple">
<li><p><a class="reference internal" href="#id3"><span class="std std-ref">Add GPU resources to the HyperConverged Custom Resource</span></a>.</p></li>
<li><p><a class="reference internal" href="#create-a-virtual-machine-with-gpu"><span class="std std-ref">Create a virtual machine with GPU passthrough</span></a></p></li>
</ul>
</dd>
</dl>
</section>
<section id="enabling-the-iommu-driver-on-hosts">
<span id="enable-iommu-driver"></span><h2>Enabling the IOMMU driver on hosts<a class="headerlink" href="#enabling-the-iommu-driver-on-hosts" title="Permalink to this headline">#</a></h2>
<p>To enable the IOMMU (Input-Output Memory Management Unit) driver in the kernel, create the <code class="docutils literal notranslate"><span class="pre">MachineConfig</span></code> object and add the kernel arguments.</p>
<section id="id2">
<h3>Prerequisites<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Administrative privilege to a working OpenShift Container Platform cluster.</p></li>
<li><p>Intel or AMD CPU hardware.</p></li>
<li><p>Intel Virtualization Technology for Directed I/O extensions or AMD IOMMU in the BIOS (Basic Input/Output System) is enabled.</p></li>
</ul>
</section>
<section id="procedure">
<h3>Procedure<a class="headerlink" href="#procedure" title="Permalink to this headline">#</a></h3>
<ol class="arabic">
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">MachineConfig</span></code> object that identifies the kernel argument.
The following example shows a kernel argument for an Intel CPU:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">machineconfiguration.openshift.io/v1</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">MachineConfig</span><span class="w"></span>
<span class="nt">metadata</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">labels</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">machineconfiguration.openshift.io/role</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">worker</span><span class="w"></span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">100-worker-iommu</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">config</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">ignition</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3.2.0</span><span class="w"></span>
<span class="w">  </span><span class="nt">kernelArguments</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">intel_iommu=on</span><span class="w"></span>
<span class="w">      </span><span class="c1"># If you are using AMD CPU, include the following argument:</span><span class="w"></span>
<span class="w">      </span><span class="c1"># - amd_iommu=on</span><span class="w"></span>
</pre></div>
</div>
</li>
<li><p>Create the new <code class="docutils literal notranslate"><span class="pre">MachineConfig</span></code> object:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc create -f <span class="m">100</span>-worker-kernel-arg-iommu.yaml
</pre></div>
</div>
</li>
<li><p>Verify that the new <code class="docutils literal notranslate"><span class="pre">MachineConfig</span></code> object was added:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get machineconfig
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="labeling-worker-nodes">
<span id="label-worker-nodes"></span><h2>Labeling worker nodes<a class="headerlink" href="#labeling-worker-nodes" title="Permalink to this headline">#</a></h2>
<p>Use the following command to add a label to a worker node:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc label node &lt;node-name&gt; --overwrite nvidia.com/gpu.workload.config<span class="o">=</span>vm-vgpu
</pre></div>
</div>
<p>You can assign the following values to the label: <code class="docutils literal notranslate"><span class="pre">container</span></code>, <code class="docutils literal notranslate"><span class="pre">vm-passthrough</span></code>, and <code class="docutils literal notranslate"><span class="pre">vm-vgpu</span></code>.
The GPU Operator uses the value of this label to determine which operands to deploy.</p>
<p>If the node label <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config</span></code> does not exist on the node, the GPU Operator assumes the default GPU workload configuration, <code class="docutils literal notranslate"><span class="pre">container</span></code>, and deploys the software components needed to support this workload type.
To change the default GPU workload configuration, set the following value in <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code>: <code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.defaultWorkload=&lt;config&gt;</span></code>.</p>
</section>
<section id="building-the-vgpu-manager-image">
<span id="build-vgpu-manager-image"></span><h2>Building the vGPU Manager image<a class="headerlink" href="#building-the-vgpu-manager-image" title="Permalink to this headline">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Building a vGPU Manager image is only required for NVIDIA vGPU.
If you plan to use GPU Passthrough only, skip this section.</p>
</div>
<p>Use the following steps to build the vGPU Manager container and push it to a private registry.</p>
<ol class="arabic">
<li><p>Download the vGPU Software from the <a class="reference external" href="https://nvid.nvidia.com/dashboard/#/dashboard">NVIDIA Licensing Portal</a>.</p>
<ul class="simple">
<li><p>Login to the NVIDIA Licensing Portal and navigate to the <strong>Software Downloads</strong> section.</p></li>
<li><p>The NVIDIA vGPU Software is located on the <strong>Driver downloads</strong> tab of the <strong>Software Downloads</strong> page.</p></li>
<li><p>Click the <strong>Download</strong> link for the Linux KVM complete vGPU package.
Confirm that the <strong>Product Version</strong> column shows the vGPU version to install.
Unzip the bundle to obtain the NVIDIA vGPU Manager for Linux file, <code class="docutils literal notranslate"><span class="pre">NVIDIA-Linux-x86_64-&lt;version&gt;-vgpu-kvm.run</span></code>.</p></li>
</ul>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>NVIDIA AI Enterprise customers must use the <code class="docutils literal notranslate"><span class="pre">aie</span></code> .run file for building the NVIDIA vGPU Manager image.
Download the <code class="docutils literal notranslate"><span class="pre">NVIDIA-Linux-x86_64-&lt;version&gt;-vgpu-kvm-aie.run</span></code> file instead, and rename it to
<code class="docutils literal notranslate"><span class="pre">NVIDIA-Linux-x86_64-&lt;version&gt;-vgpu-kvm.run</span></code> before proceeding with the rest of the procedure.
Refer to the <code class="docutils literal notranslate"><span class="pre">Infrastructure</span> <span class="pre">Support</span> <span class="pre">Matrix</span></code> under section under the <a class="reference external" href="https://docs.nvidia.com/ai-enterprise/index.html#infrastructure-software">NVIDIA AI Enterprise Infra Release Branches</a> for details on supported version number to use.</p>
</div>
</div></blockquote>
<p>Use the following steps to clone the driver container repository and build the driver image.</p>
</li>
<li><p>Open a terminal and clone the driver container image repository:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>git clone https://gitlab.com/nvidia/container-images/driver
<span class="gp">$ </span><span class="nb">cd</span> driver
</pre></div>
</div>
</li>
<li><p>Change to the <code class="docutils literal notranslate"><span class="pre">vgpu-manager</span></code> directory for your OS:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">cd</span> vgpu-manager/rhel8
</pre></div>
</div>
</li>
<li><p>Copy the NVIDIA vGPU Manager from your extracted zip file:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cp &lt;local-driver-download-directory&gt;/*-vgpu-kvm.run ./
</pre></div>
</div>
</li>
<li><p>Set the following environment variables:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">PRIVATE_REGISTRY</span></code> - Name of the private registry used to store the driver image.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VERSION</span></code> - The NVIDIA vGPU Manager version downloaded from the NVIDIA Software Portal.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OS_TAG</span></code> - This must match the Guest OS version.
For RedHat OpenShift, specify <code class="docutils literal notranslate"><span class="pre">rhcos4.x</span></code> where _x_ is the supported minor OCP version.</p></li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span><span class="nb">export</span> <span class="nv">PRIVATE_REGISTRY</span><span class="o">=</span>my/private/registry <span class="nv">VERSION</span><span class="o">=</span><span class="m">510</span>.73.06 <span class="nv">OS_TAG</span><span class="o">=</span>rhcos4.11
</pre></div>
</div>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The recommended registry to use is the Integrated OpenShift Container Platform registry.
For more information about the registry, see <a class="reference external" href="https://docs.openshift.com/container-platform/latest/registry/accessing-the-registry.html">Accessing the registry</a>.</p>
</div>
<ol class="arabic">
<li><p>Build the NVIDIA vGPU Manager image:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker build <span class="se">\</span>
    --build-arg <span class="nv">DRIVER_VERSION</span><span class="o">=</span><span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span> <span class="se">\</span>
    -t <span class="si">${</span><span class="nv">PRIVATE_REGISTRY</span><span class="si">}</span>/vgpu-manager:<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>-<span class="si">${</span><span class="nv">OS_TAG</span><span class="si">}</span> .
</pre></div>
</div>
</li>
<li><p>Push the NVIDIA vGPU Manager image to your private registry:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>docker push <span class="si">${</span><span class="nv">PRIVATE_REGISTRY</span><span class="si">}</span>/vgpu-manager:<span class="si">${</span><span class="nv">VERSION</span><span class="si">}</span>-<span class="si">${</span><span class="nv">OS_TAG</span><span class="si">}</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="installing-the-nvidia-gpu-operator">
<span id="install-the-gpu-operator"></span><h2>Installing the NVIDIA GPU Operator<a class="headerlink" href="#installing-the-nvidia-gpu-operator" title="Permalink to this headline">#</a></h2>
<p>Install the NVIDIA GPU Operator using the guidance at <a class="reference internal" href="install-gpu-ocp.html#install-nvidiagpu"><span class="std std-ref">Installing the NVIDIA GPU Operator</span></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When prompted to create a cluster policy follow the guidance <a class="reference internal" href="#install-cluster-policy-vgpu"><span class="std std-ref">Creating a ClusterPolicy for the GPU Operator</span></a>.</p>
</div>
<section id="create-the-secret">
<h3>Create the secret<a class="headerlink" href="#create-the-secret" title="Permalink to this headline">#</a></h3>
<p>OpenShift has a secret object type which provides a mechanism for holding sensitive information such as passwords and private source repository credentials. Next you will create a secret object for storing your registry API key (the mechanism used to authenticate your access to the
private container registry).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Before you begin you will need to generate or use an existing API key for your private registry.</p>
</div>
<ol class="arabic">
<li><p>Navigate to <strong>Home</strong> &gt; <strong>Projects</strong> and ensure the <code class="docutils literal notranslate"><span class="pre">nvidia-gpu-operator</span></code> is selected.</p></li>
<li><p>In the OpenShift Container Platform web console, click <strong>Secrets</strong> from the Workloads drop down.</p></li>
<li><p>Click the <strong>Create</strong> Drop down.</p></li>
<li><p>Select Image Pull Secret.</p>
<img alt="_images/secrets.png" src="_images/secrets.png" />
</li>
<li><p>Enter the following into each field:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Secret name</strong>: private-registry-secret</p></li>
<li><p><strong>Authentication type</strong>: Image registry credentials</p></li>
<li><p><strong>Registry server address</strong>: &lt;private-registry_address&gt;</p></li>
<li><p><strong>Username</strong>: $oauthtoken</p></li>
<li><p><strong>Password</strong>: &lt;API-KEY&gt;</p></li>
<li><p><strong>Email</strong>: &lt;YOUR-EMAIL&gt;</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Click <strong>Create</strong>.</p>
<p>A pull secret is created.</p>
</li>
</ol>
</section>
<section id="creating-a-clusterpolicy-for-the-gpu-operator-using-the-openshift-container-platform-cli">
<span id="install-cluster-policy-vgpu"></span><h3>Creating a ClusterPolicy for the GPU Operator using the OpenShift Container Platform CLI<a class="headerlink" href="#creating-a-clusterpolicy-for-the-gpu-operator-using-the-openshift-container-platform-cli" title="Permalink to this headline">#</a></h3>
<p>As a cluster administrator, you can create a ClusterPolicy using the OpenShift Container Platform CLI.
Create the cluster policy using the CLI:</p>
<ol class="arabic">
<li><p>Create the ClusterPolicy:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get csv -n nvidia-gpu-operator gpu-operator-certified.v22.9.0 -ojsonpath<span class="o">={</span>.metadata.annotations.alm-examples<span class="o">}</span> <span class="p">|</span> jq .<span class="o">[</span><span class="m">0</span><span class="o">]</span> &gt; clusterpolicy.json
</pre></div>
</div>
</li>
<li><p>Modify the <code class="docutils literal notranslate"><span class="pre">clusterpolicy.json</span></code> file as follows:</p>
<ul class="simple">
<li><p>sandboxWorloads.enabled=true</p></li>
<li><p>vgpuManager.enabled=true</p></li>
<li><p>vgpuManager.repository=&lt;path to private repository&gt;</p></li>
<li><p>vgpuManager.image=vgpu-manager</p></li>
<li><p>vgpuManager.version=&lt;driver version&gt;</p></li>
<li><p>vgpuManager.imagePullSecrets={&lt;name of image pull secret&gt;}</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">vgpuManager</span></code> options are only required if you want to use the NVIDIA vGPU. If you are only using GPU passthrough, these options should not be set.</p>
<p>In general, the flag <code class="docutils literal notranslate"><span class="pre">sandboxWorkloads.enabled</span></code> in <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code> controls whether the GPU Operator can provision GPU worker nodes for virtual machine workloads, in addition to container workloads. This flag is disabled by default, meaning all nodes get provisioned with the same software which enables container workloads, and the <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config</span></code> node label is not used.</p>
<p>The term <code class="docutils literal notranslate"><span class="pre">sandboxing</span></code> refers to running software in a separate isolated environment, typically for added security (i.e. a virtual machine). We use the term <code class="docutils literal notranslate"><span class="pre">sandbox</span> <span class="pre">workloads</span></code> to signify workloads that run in a virtual machine, irrespective of the virtualization technology used.</p>
</li>
<li><p>Apply the changes:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc apply -f clusterpolicy.json
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">clusterpolicy.nvidia.com/gpu-cluster-policy created</span>
</pre></div>
</div>
</li>
</ol>
<p>The vGPU Device Manager, deployed by the GPU Operator, automatically creates vGPU devices which can be assigned to KubeVirt VMs.
Without additional configuration, the GPU Operator creates a default set of devices on all GPUs.
To learn more about how the vGPU Device Manager and configure which types of vGPU devices get created in your cluster, refer to <a class="reference internal" href="#vgpu-device-configuration"><span class="std std-ref">vGPU Device Configuration</span></a>.</p>
</section>
<section id="creating-a-clusterpolicy-for-the-gpu-operator-using-the-openshift-container-platform-web-console">
<h3>Creating a ClusterPolicy for the GPU Operator using the OpenShift Container Platform Web Console<a class="headerlink" href="#creating-a-clusterpolicy-for-the-gpu-operator-using-the-openshift-container-platform-web-console" title="Permalink to this headline">#</a></h3>
<p>As a cluster administrator, you can create a ClusterPolicy using the OpenShift Container Platform web console.</p>
<ol class="arabic">
<li><p>Navigate to <strong>Operators</strong> &gt; <strong>Installed Operators</strong> and find your installed NVIDIA GPU Operator.</p></li>
<li><p>Under <em>Provided APIs</em>, click <strong>ClusterPolicy</strong>.</p>
<img alt="_images/navigate_to_cluster_policy.png" src="_images/navigate_to_cluster_policy.png" />
</li>
<li><p>Click <strong>Create ClusterPolicy</strong>.</p>
<img alt="_images/create_cluster_policy.png" src="_images/create_cluster_policy.png" />
</li>
<li><p>Expand the <strong>NVIDIA GPU/vGPU Driver config</strong> section.</p></li>
<li><p>Expand the <strong>Sandbox Workloads config</strong> section and select the checkbox to enable sandbox workloads.</p>
<p>In general, when sandbox workloads are enabled, <code class="docutils literal notranslate"><span class="pre">ClusterPolicy</span></code> controls whether the GPU Operator can provision GPU worker nodes for virtual machine workloads, in addition to container workloads. This flag is disabled by default, meaning all nodes get provisioned with the same software which enables container workloads, and the <code class="docutils literal notranslate"><span class="pre">nvidia.com/gpu.workload.config</span></code> node label is not used.</p>
<p>The term <code class="docutils literal notranslate"><span class="pre">sandboxing</span></code> refers to running software in a separate isolated environment, typically for added security (i.e. a virtual machine). We use the term <code class="docutils literal notranslate"><span class="pre">sandbox</span> <span class="pre">workloads</span></code> to signify workloads that run in a virtual machine, irrespective of the virtualization technology used.
* Click <strong>Create</strong> to create the ClusterPolicy.</p>
<img alt="_images/cluster_policy_enable_sandbox_workloads.png" src="_images/cluster_policy_enable_sandbox_workloads.png" />
</li>
<li><p>If you are planning to use NVIDIA vGPU, expand the <strong>NVIDIA vGPU Manager config</strong> section and fill in your desired configuration settings, including:</p>
<ul class="simple">
<li><p>Select the <strong>enabled</strong> checkbox to enable the NVIDIA vGPU Manager.</p></li>
<li><p>Add your <strong>imagePullSecrets</strong>.</p></li>
<li><p>Under <em>driverManager</em>, fill in <strong>repository</strong> with the path to your private repository.</p></li>
<li><p>Under <em>env</em>, fill in <strong>image</strong> with <code class="docutils literal notranslate"><span class="pre">vgpu-manager</span></code> and the <strong>version</strong> with your driver version.</p></li>
</ul>
<p>If you are only using GPU passthrough, you dont need to fill this section out.</p>
<img alt="_images/cluster_policy_configure_vgpu.png" src="_images/cluster_policy_configure_vgpu.png" />
</li>
<li><p>Click <strong>Create</strong> to create the ClusterPolicy.</p>
<p>The vGPU Device Manager, deployed by the GPU Operator, automatically creates vGPU devices which can be assigned to KubeVirt VMs.
Without additional configuration, the GPU Operator creates a default set of devices on all GPUs.
To learn more about the vGPU Device Manager and how to configure which types of vGPU devices get created in your cluster, refer to <a class="reference internal" href="#vgpu-device-configuration"><span class="std std-ref">vGPU Device Configuration</span></a>.</p>
</li>
</ol>
</section>
</section>
<section id="add-gpu-resources-to-the-hyperconverged-custom-resource">
<h2>Add GPU Resources to the HyperConverged Custom Resource<a class="headerlink" href="#add-gpu-resources-to-the-hyperconverged-custom-resource" title="Permalink to this headline">#</a></h2>
<p>Update the <code class="docutils literal notranslate"><span class="pre">HyperConverged</span></code> custom resource so that all GPU and vGPU devices in your cluster are permitted and can be assigned to virtual machines.</p>
<section id="add-gpu-passthrough-resources-to-the-hyperconverged-custom-resource">
<span id="id3"></span><h3>Add GPU passthrough resources to the HyperConverged Custom Resource<a class="headerlink" href="#add-gpu-passthrough-resources-to-the-hyperconverged-custom-resource" title="Permalink to this headline">#</a></h3>
<p>The following example permits the A10 GPU device, the device names for the GPUs on your cluster will likely be different.</p>
<ol class="arabic">
<li><p>Determine the resource names for the GPU devices:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get node cnt-server-2 -o json <span class="p">|</span> jq <span class="s1">&#39;.status.allocatable | with_entries(select(.key | startswith(&quot;nvidia.com/&quot;))) | with_entries(select(.value != &quot;0&quot;))&#39;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &quot;nvidia.com/GA102GL_A10&quot;: &quot;1&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</li>
<li><p>Determine the PCI device IDs for the GPUs.</p>
<ul>
<li><p>You can search by device name in the <a class="reference external" href="https://pci-ids.ucw.cz/v2.2/pci.ids">PCI IDs database</a>.</p></li>
<li><p>If you have host access to the node, you can list the NVIDIA GPU devices with a command like the following example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lspci -nnk -d 10de:
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="hll"><span class="go">65:00.0 3D controller [0302]: NVIDIA Corporation GA102GL [A10] [10de:2236] (rev a1)</span>
</span><span class="go">        Subsystem: NVIDIA Corporation GA102GL [A10] [10de:1482]</span>
<span class="go">        Kernel modules: nvidiafb, nouveau</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Modify the <code class="docutils literal notranslate"><span class="pre">HyperConverged</span></code> custom resource like the following partial examples.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">...</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">   </span><span class="nt">featureGates</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">disableMDevConfiguration</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">   </span><span class="nt">permittedHostDevices</span><span class="p">:</span><span class="w"> </span><span class="c1"># Defines VM devices to import.</span><span class="w"></span>
<span class="w">      </span><span class="nt">pciHostDevices</span><span class="p">:</span><span class="w"> </span><span class="c1"># Include for GPU passthrough</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">externalResourceProvider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">        </span><span class="nt">pciDeviceSelector</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10DE:2236</span><span class="w"></span>
<span class="w">        </span><span class="nt">resourceName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/GA102GL_A10</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
</pre></div>
</div>
<p>Replace the values in the YAML as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pciDeviceSelector</span></code> and <code class="docutils literal notranslate"><span class="pre">resourceName</span></code> under <code class="docutils literal notranslate"><span class="pre">pciHostDevices</span></code> to correspond to your GPU type.</p></li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">externalResourceProvider=true</span></code> to indicate that this resource is provided by an external device plugin, in this case the <code class="docutils literal notranslate"><span class="pre">sandbox-device-plugin</span></code> that is deployed by the GPU Operator.</p></li>
</ul>
</li>
</ol>
<p>Refer to the <a class="reference external" href="https://kubevirt.io/user-guide/virtual_machines/host-devices/#listing-permitted-devices">KubeVirt user guide</a> for more information on the configuration options.</p>
</section>
<section id="add-vgpu-resources-to-the-hyperconverged-custom-resource">
<span id="id4"></span><h3>Add vGPU resources to the HyperConverged Custom Resource<a class="headerlink" href="#add-vgpu-resources-to-the-hyperconverged-custom-resource" title="Permalink to this headline">#</a></h3>
<p>The following example permits the A10-12Q vGPU device, the device names for the GPUs on your cluster will likely be different.</p>
<ol class="arabic">
<li><p>Determine the resource names for the GPU devices:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get node cnt-server-2 -o json <span class="p">|</span> jq <span class="s1">&#39;.status.allocatable | with_entries(select(.key | startswith(&quot;nvidia.com/&quot;))) | with_entries(select(.value != &quot;0&quot;))&#39;</span>
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="go">{</span>
<span class="go">  &quot;nvidia.com/NVIDIA_A10-12Q&quot;: &quot;4&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</li>
<li><p>Determine the PCI device IDs for the GPUs.</p>
<ul>
<li><p>You can search by device name in the <a class="reference external" href="https://pci-ids.ucw.cz/v2.2/pci.ids">PCI IDs database</a>.</p></li>
<li><p>If you have host access to the node, you can list the NVIDIA GPU devices with a command like the following example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>lspci -nnk -d 10de:
</pre></div>
</div>
<p><em>Example Output</em></p>
<div class="highlight-output notranslate"><div class="highlight"><pre><span></span><span class="hll"><span class="go">65:00.0 3D controller [0302]: NVIDIA Corporation GA102GL [A10] [10de:2236] (rev a1)</span>
</span><span class="go">        Subsystem: NVIDIA Corporation GA102GL [A10] [10de:1482]</span>
<span class="go">        Kernel modules: nvidiafb, nouveau</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Modify the <code class="docutils literal notranslate"><span class="pre">HyperConverged</span></code> custom resource like the following partial examples.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">...</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">featureGates</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">disableMDevConfiguration</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">  </span><span class="nt">permittedHostDevices</span><span class="p">:</span><span class="w"> </span><span class="c1"># Defines VM devices to import.</span><span class="w"></span>
<span class="w">    </span><span class="nt">mediatedDevices</span><span class="p">:</span><span class="w"> </span><span class="c1"># Include for vGPU</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">externalResourceProvider</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span><span class="w"></span>
<span class="w">      </span><span class="nt">mdevNameSelector</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">NVIDIA A10-12Q</span><span class="w"></span>
<span class="w">      </span><span class="nt">resourceName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/NVIDIA_A10-12Q</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
</pre></div>
</div>
<p>Replace the values in the YAML as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mdevNameSelector</span></code> and <code class="docutils literal notranslate"><span class="pre">resourceName</span></code> under <code class="docutils literal notranslate"><span class="pre">mediatedDevices</span></code> to correspond to your vGPU type.</p></li>
<li><p>Set <code class="docutils literal notranslate"><span class="pre">externalResourceProvider=true</span></code> to indicate that this resource is provided by an external device plugin, in this case the <code class="docutils literal notranslate"><span class="pre">sandbox-device-plugin</span></code> that is deployed by the GPU Operator.</p></li>
</ul>
</li>
</ol>
<p>Refer to the <a class="reference external" href="https://kubevirt.io/user-guide/virtual_machines/host-devices/#listing-permitted-devices">KubeVirt user guide</a> for more information on the configuration options.</p>
</section>
<section id="about-mediated-devices">
<h3>About Mediated Devices<a class="headerlink" href="#about-mediated-devices" title="Permalink to this headline">#</a></h3>
<p>A physical device that is divided into one or more virtual devices. A vGPU is a type of mediated device
(mdev); the performance of the physical GPU is divided among the virtual devices. You can assign mediated
devices to one or more virtual machines (VMs), but the number of guests must be compatible with your GPU.
Some GPUs do not support multiple guests.</p>
</section>
</section>
<section id="creating-a-virtual-machine-with-gpu">
<span id="create-a-virtual-machine-with-gpu"></span><h2>Creating a virtual machine with GPU<a class="headerlink" href="#creating-a-virtual-machine-with-gpu" title="Permalink to this headline">#</a></h2>
<p>Assign GPU devices, either passthrough or vGPU, to virtual machines.</p>
<section id="id7">
<h3>Prerequisites<a class="headerlink" href="#id7" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>The GPU devices are configured in the <code class="docutils literal notranslate"><span class="pre">HyperConverged</span></code> custom resource (CR).</p></li>
</ul>
</section>
<section id="id8">
<h3>Procedure<a class="headerlink" href="#id8" title="Permalink to this headline">#</a></h3>
<ol class="arabic">
<li><p>Assign the GPU devices to a virtual machine (VM) by editing the <code class="docutils literal notranslate"><span class="pre">spec.domain.devices.gpus</span></code> field of the <code class="docutils literal notranslate"><span class="pre">VirtualMachine</span></code> manifest:</p>
<p>Example for GPU passthrough:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kubevirt.io/v1alpha3</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">VirtualMachineInstance</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="nt">domain</span><span class="p">:</span><span class="w"></span>
<span class="w">   </span><span class="nt">devices</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">gpus</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">deviceName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/GA102GL_A10</span><span class="w"></span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu1</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
</pre></div>
</div>
<p>Example for vGPU:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">kubevirt.io/v1alpha3</span><span class="w"></span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">VirtualMachineInstance</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
<span class="nt">spec</span><span class="p">:</span><span class="w"></span>
<span class="nt">domain</span><span class="p">:</span><span class="w"></span>
<span class="w">   </span><span class="nt">devices</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="nt">gpus</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">deviceName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">nvidia.com/NVIDIA_A10-12Q</span><span class="w"></span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gpu1</span><span class="w"></span>
<span class="nn">...</span><span class="w"></span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">deviceName</span></code> The resource name associated with the GPU.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">name</span></code> A name to identify the device on the VM.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<section id="vgpu-device-configuration">
<span id="id9"></span><h2>vGPU Device Configuration<a class="headerlink" href="#vgpu-device-configuration" title="Permalink to this headline">#</a></h2>
<p>The vGPU Device Manager assists in creating vGPU devices on GPU worker nodes.</p>
<p>The vGPU Device Manager allows administrators to declaratively define a set of possible vGPU device configurations they would like applied to GPUs on a node.
At runtime, they then point the vGPU Device Manager at one of these configurations, and vGPU Device Manager takes care of applying it.</p>
<p>The configuration file is created as a ConfigMap, and is shared across all worker nodes.
At runtime, a node label, <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code>, can be used to decide which of these configurations to actually apply to a node at any given time.
If the node is not labeled, then the <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration will be used.</p>
<p>For more information on this component and how it is configured, refer to the project <a class="reference external" href="https://github.com/NVIDIA/vgpu-device-manager">README</a>.</p>
<p>By default, the GPU Operator deploys a ConfigMap for the vGPU Device Manager, containing named configurations for all <a class="reference external" href="https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#supported-gpus-grid-vgpu">vGPU types</a> supported by NVIDIA vGPU.
Users can select a specific configuration for a worker node by applying the <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code> node label.</p>
<p>For example, labeling a node with <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config=A10-8Q</span></code> would create 3 vGPU devices of type <strong>A10-8Q</strong> on all <strong>A10</strong> GPUs on the node (note: 3 is the maximum number of <strong>A10-8Q</strong> devices that can be created per GPU).
If the node is not labeled, the <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration will be applied.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration will create Q-series vGPU devices on all GPUs, where the amount of framebuffer memory per vGPU device is half the total GPU memory.
For example, the <code class="docutils literal notranslate"><span class="pre">default</span></code> configuration will create two <strong>A10-12Q</strong> devices on all <strong>A10</strong> GPUs, two <strong>V100-8Q</strong> devices  on all <strong>V100</strong> GPUs, and two <strong>T4-8Q</strong> devices on all <strong>T4</strong> GPUs.</p>
<p>If custom vGPU device configuration is desired, more than the default ConfigMap provides, you can create your own ConfigMap:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc create configmap custom-vgpu-config -n gpu-operator --from-file<span class="o">=</span>config.yaml<span class="o">=</span>/path/to/file
</pre></div>
</div>
</div></blockquote>
<p>And then configure the GPU Operator to use it by setting <code class="docutils literal notranslate"><span class="pre">vgpuDeviceManager.config.name=custom-vgpu-config</span></code>.</p>
<section id="apply-a-new-vgpu-device-configuration">
<h3>Apply a New vGPU Device Configuration<a class="headerlink" href="#apply-a-new-vgpu-device-configuration" title="Permalink to this headline">#</a></h3>
<p>Apply a specific vGPU device configuration on a per-node basis by setting the <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code> node label. It is recommended to set this node label prior to installing the GPU Operator if you do not want the default configuration applied.</p>
<p>Switching vGPU device configuration after one has been successfully applied assumes that no VMs with vGPU are currently running on the node. Any existing VMs will have to be shutdown/migrated first.</p>
<p>To apply a new configuration after GPU Operator install, simply update the <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code> node label.</p>
<p>Let’s run through an example on a system with two <strong>A10</strong> GPUs.</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>nvidia-smi -L
<span class="go">GPU 0: NVIDIA A10 (UUID: GPU-ebd34bdf-1083-eaac-2aff-4b71a022f9bd)</span>
<span class="go">GPU 1: NVIDIA A10 (UUID: GPU-1795e88b-3395-b27b-dad8-0488474eec0c)</span>
</pre></div>
</div>
</div></blockquote>
<p>After installing the GPU Operator as detailed in the previous sections and without labeling the node with <code class="docutils literal notranslate"><span class="pre">nvidia.com/vgpu.config</span></code>, the <code class="docutils literal notranslate"><span class="pre">default</span></code> vGPU config get applied – four <strong>A10-12Q</strong> devices get created (two per GPU):</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get node cnt-server-2 -o json <span class="p">|</span> jq <span class="s1">&#39;.status.allocatable | with_entries(select(.key | startswith(&quot;nvidia.com/&quot;))) | with_entries(select(.value != &quot;0&quot;))&#39;</span>
<span class="go">{</span>
<span class="go">  &quot;nvidia.com/NVIDIA_A10-12Q&quot;: &quot;4&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</div></blockquote>
<p>If instead you want to create <strong>A10-4Q</strong> devices, we can label the node like such:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc label node &lt;node-name&gt; --overwrite nvidia.com/vgpu.config<span class="o">=</span>A10-4Q
</pre></div>
</div>
</div></blockquote>
<p>After the vGPU Device Manager finishes applying the new configuration, all GPU Operator pods should return to the Running state.</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get pods -n gpu-operator
<span class="go">NAME                                                          READY   STATUS    RESTARTS   AGE</span>
<span class="go">...</span>
<span class="go">nvidia-sandbox-device-plugin-daemonset-brtb6                  1/1     Running   0          10s</span>
<span class="go">nvidia-sandbox-validator-ljnwg                                1/1     Running   0          10s</span>
<span class="go">nvidia-vgpu-device-manager-8mgg8                              1/1     Running   0          30m</span>
<span class="go">nvidia-vgpu-manager-daemonset-fpplc                           1/1     Running   0          31m</span>
</pre></div>
</div>
</div></blockquote>
<p>You should now see 12 <strong>A10-4Q</strong> devices on the node, as 6 <strong>A10-4Q</strong> devices can be created per <strong>A10</strong> GPU.</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>oc get node cnt-server-2 -o json <span class="p">|</span> jq <span class="s1">&#39;.status.allocatable | with_entries(select(.key | startswith(&quot;nvidia.com/&quot;))) | with_entries(select(.value != &quot;0&quot;))&#39;</span>
<span class="go">{</span>
<span class="go">  &quot;nvidia.com/NVIDIA_A10-4Q&quot;: &quot;12&quot;</span>
<span class="go">}</span>
</pre></div>
</div>
</div></blockquote>
</section>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="time-slicing-gpus-in-openshift.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Time-slicing NVIDIA GPUs in OpenShift</p>
      </div>
    </a>
    <a class="right-next"
       href="gpu-operator-with-precompiled-drivers.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Precompiled Drivers for the NVIDIA GPU Operator for RHCOS</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            


              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-worker-nodes-for-gpu-operator-components">Configure Worker Nodes for GPU Operator components</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-constraints-and-dependencies">Assumptions, constraints, and dependencies</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-nvidia-gpu-operator-with-openshift-virtualization">Configure NVIDIA GPU Operator with OpenShift Virtualization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#enabling-the-iommu-driver-on-hosts">Enabling the IOMMU driver on hosts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Prerequisites</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#procedure">Procedure</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#labeling-worker-nodes">Labeling worker nodes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-vgpu-manager-image">Building the vGPU Manager image</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-the-nvidia-gpu-operator">Installing the NVIDIA GPU Operator</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#create-the-secret">Create the secret</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-clusterpolicy-for-the-gpu-operator-using-the-openshift-container-platform-cli">Creating a ClusterPolicy for the GPU Operator using the OpenShift Container Platform CLI</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-clusterpolicy-for-the-gpu-operator-using-the-openshift-container-platform-web-console">Creating a ClusterPolicy for the GPU Operator using the OpenShift Container Platform Web Console</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#add-gpu-resources-to-the-hyperconverged-custom-resource">Add GPU Resources to the HyperConverged Custom Resource</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#add-gpu-passthrough-resources-to-the-hyperconverged-custom-resource">Add GPU passthrough resources to the HyperConverged Custom Resource</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#add-vgpu-resources-to-the-hyperconverged-custom-resource">Add vGPU resources to the HyperConverged Custom Resource</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#about-mediated-devices">About Mediated Devices</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-virtual-machine-with-gpu">Creating a virtual machine with GPU</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Prerequisites</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Procedure</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#vgpu-device-configuration">vGPU Device Configuration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#apply-a-new-vgpu-device-configuration">Apply a New vGPU Device Configuration</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            

          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>


  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
<a class="footer-brand logo" href="https://www.nvidia.com">
  <img src="_static/nvidia-logo-horiz-rgb-1c-blk-for-screen.svg" class="logo__image only-light" alt="NVIDIA"/>
  <img src="_static/nvidia-logo-horiz-rgb-1c-wht-for-screen.svg" class="logo__image only-dark" alt="NVIDIA"/>
</a></div>
      
        <div class="footer-item">

<div class="footer-links">
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/">Privacy Policy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/">Manage My Privacy</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/preferences/start/">Do Not Sell or Share My Data</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/">Terms of Service</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/">Accessibility</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/">Corporate Policies</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/product-security/">Product Security</a>
   | 
  
  
  
  <a class="external" href="https://www.nvidia.com/en-us/contact/">Contact</a>
  
  
  
</div>
</div>
      
        <div class="footer-item">




  <p class="copyright">
    
      Copyright © 2020-2025, NVIDIA Corporation.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
</div>

  </footer>

  <script type="text/javascript">if (typeof _satellite !== "undefined") {_satellite.pageBottom();}</script>
  


  </body>
</html>